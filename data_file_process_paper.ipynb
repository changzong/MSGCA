{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a5f0c17",
   "metadata": {},
   "source": [
    "# Raw Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f379e345",
   "metadata": {},
   "source": [
    "## DBLP paper ciation data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5865fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/zongchang/Desktop/实验数据集/DBLP-Paper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "861a6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path + '/DBLPOnlyCitationOct19.txt', 'r') as f:\n",
    "    paper_list = []\n",
    "    paper = {}\n",
    "    paper_raw = []\n",
    "    for line in f.readlines():\n",
    "        if line == '\\n':\n",
    "            ref_index = []\n",
    "            for item in paper_raw:\n",
    "                if item.startswith('#*'):\n",
    "                    paper['title'] = item.replace('#*', '')\n",
    "                elif item.startswith('#@'):\n",
    "                    paper['author'] = item.replace('#@', '')\n",
    "                elif item.startswith('#t'):\n",
    "                    paper['year'] = item.replace('#t', '')\n",
    "                elif item.startswith('#c'):\n",
    "                    paper['venue'] = item.replace('#c', '')\n",
    "                elif item.startswith('#index'):\n",
    "                    paper['index'] = item.replace('#index', '')\n",
    "                elif item.startswith('#%'):\n",
    "                    ref_index.append(item.replace('#%', ''))\n",
    "                elif item.startswith('#!'):\n",
    "                    paper['abstract'] = item.replace('#!', '')\n",
    "                else:\n",
    "                    continue\n",
    "            paper['ref_index'] = ref_index\n",
    "            paper_list.append(paper)\n",
    "            paper = {}\n",
    "            paper_raw = []\n",
    "        else:\n",
    "            paper_raw.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "72d9e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter paper with low quality: absence of properties, earlier papers, small number of references\n",
    "paper_list_filter = []\n",
    "for paper in paper_list:\n",
    "    if ('abstract' not in paper) or ('title' not in paper) or ('author' not in paper) or (int(paper['year']) < 2000) or len(paper['ref_index'])<10:\n",
    "        continue\n",
    "    else:\n",
    "        paper_list_filter.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4682810d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66501"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_list_filter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fda474ac",
   "metadata": {},
   "source": [
    "## DBLP paper citation count (indicator_input.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "18428cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index yearly citation dict for papers at year 2000\n",
    "index_cite_count = {}\n",
    "for paper in paper_list_filter:\n",
    "    if paper['year'] == '2000':\n",
    "        index_cite_count[paper['index']] = {}\n",
    "\n",
    "# count citation\n",
    "for paper in paper_list_filter:\n",
    "    for ref in paper['ref_index']:\n",
    "        if ref in index_cite_count:\n",
    "            if paper['year'] not in index_cite_count[ref]:\n",
    "                index_cite_count[ref][paper['year']] = 1\n",
    "            else:\n",
    "                index_cite_count[ref][paper['year']] += 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "360f076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep papers have citations\n",
    "index_cite_count_nonzero = {}\n",
    "for k,v in index_cite_count.items():\n",
    "    if v:\n",
    "        index_cite_count_nonzero[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bd725331",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010']\n",
    "citation_count_list = [] # index, year, citation_count\n",
    "for k,v in index_cite_count_nonzero.items():\n",
    "    for year in years:\n",
    "        if year not in v:\n",
    "            tmp = [k, year, 0]\n",
    "        else:\n",
    "            tmp = [k, year, v[year]]\n",
    "        citation_count_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0fde8b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5652', '2000', 0],\n",
       " ['5652', '2001', 0],\n",
       " ['5652', '2002', 0],\n",
       " ['5652', '2003', 1],\n",
       " ['5652', '2004', 4],\n",
       " ['5652', '2005', 0],\n",
       " ['5652', '2006', 1],\n",
       " ['5652', '2007', 2],\n",
       " ['5652', '2008', 1],\n",
       " ['5652', '2009', 4],\n",
       " ['5652', '2010', 0],\n",
       " ['5693', '2000', 0],\n",
       " ['5693', '2001', 0],\n",
       " ['5693', '2002', 2],\n",
       " ['5693', '2003', 0],\n",
       " ['5693', '2004', 1],\n",
       " ['5693', '2005', 0],\n",
       " ['5693', '2006', 1],\n",
       " ['5693', '2007', 2],\n",
       " ['5693', '2008', 0],\n",
       " ['5693', '2009', 1],\n",
       " ['5693', '2010', 0],\n",
       " ['6516', '2000', 1],\n",
       " ['6516', '2001', 0],\n",
       " ['6516', '2002', 0],\n",
       " ['6516', '2003', 0],\n",
       " ['6516', '2004', 0],\n",
       " ['6516', '2005', 0],\n",
       " ['6516', '2006', 0],\n",
       " ['6516', '2007', 1],\n",
       " ['6516', '2008', 0],\n",
       " ['6516', '2009', 0],\n",
       " ['6516', '2010', 0],\n",
       " ['9722', '2000', 0],\n",
       " ['9722', '2001', 0],\n",
       " ['9722', '2002', 2],\n",
       " ['9722', '2003', 0],\n",
       " ['9722', '2004', 0],\n",
       " ['9722', '2005', 0],\n",
       " ['9722', '2006', 1],\n",
       " ['9722', '2007', 1],\n",
       " ['9722', '2008', 0],\n",
       " ['9722', '2009', 1],\n",
       " ['9722', '2010', 0],\n",
       " ['13378', '2000', 0],\n",
       " ['13378', '2001', 0],\n",
       " ['13378', '2002', 0],\n",
       " ['13378', '2003', 1],\n",
       " ['13378', '2004', 2],\n",
       " ['13378', '2005', 4],\n",
       " ['13378', '2006', 0],\n",
       " ['13378', '2007', 1],\n",
       " ['13378', '2008', 0],\n",
       " ['13378', '2009', 0],\n",
       " ['13378', '2010', 0],\n",
       " ['17568', '2000', 0],\n",
       " ['17568', '2001', 0],\n",
       " ['17568', '2002', 0],\n",
       " ['17568', '2003', 0],\n",
       " ['17568', '2004', 1],\n",
       " ['17568', '2005', 1],\n",
       " ['17568', '2006', 0],\n",
       " ['17568', '2007', 0],\n",
       " ['17568', '2008', 0],\n",
       " ['17568', '2009', 1],\n",
       " ['17568', '2010', 0],\n",
       " ['17575', '2000', 0],\n",
       " ['17575', '2001', 0],\n",
       " ['17575', '2002', 0],\n",
       " ['17575', '2003', 0],\n",
       " ['17575', '2004', 0],\n",
       " ['17575', '2005', 0],\n",
       " ['17575', '2006', 1],\n",
       " ['17575', '2007', 0],\n",
       " ['17575', '2008', 0],\n",
       " ['17575', '2009', 0],\n",
       " ['17575', '2010', 0],\n",
       " ['17759', '2000', 0],\n",
       " ['17759', '2001', 0],\n",
       " ['17759', '2002', 0],\n",
       " ['17759', '2003', 0],\n",
       " ['17759', '2004', 0],\n",
       " ['17759', '2005', 0],\n",
       " ['17759', '2006', 0],\n",
       " ['17759', '2007', 1],\n",
       " ['17759', '2008', 0],\n",
       " ['17759', '2009', 0],\n",
       " ['17759', '2010', 0],\n",
       " ['19853', '2000', 0],\n",
       " ['19853', '2001', 1],\n",
       " ['19853', '2002', 1],\n",
       " ['19853', '2003', 1],\n",
       " ['19853', '2004', 0],\n",
       " ['19853', '2005', 2],\n",
       " ['19853', '2006', 0],\n",
       " ['19853', '2007', 3],\n",
       " ['19853', '2008', 1],\n",
       " ['19853', '2009', 1],\n",
       " ['19853', '2010', 1],\n",
       " ['20105', '2000', 1],\n",
       " ['20105', '2001', 2],\n",
       " ['20105', '2002', 3],\n",
       " ['20105', '2003', 2],\n",
       " ['20105', '2004', 0],\n",
       " ['20105', '2005', 1],\n",
       " ['20105', '2006', 0],\n",
       " ['20105', '2007', 1],\n",
       " ['20105', '2008', 0],\n",
       " ['20105', '2009', 2],\n",
       " ['20105', '2010', 0],\n",
       " ['20181', '2000', 0],\n",
       " ['20181', '2001', 1],\n",
       " ['20181', '2002', 0],\n",
       " ['20181', '2003', 0],\n",
       " ['20181', '2004', 0],\n",
       " ['20181', '2005', 0],\n",
       " ['20181', '2006', 0],\n",
       " ['20181', '2007', 0],\n",
       " ['20181', '2008', 0],\n",
       " ['20181', '2009', 0],\n",
       " ['20181', '2010', 0],\n",
       " ['20383', '2000', 0],\n",
       " ['20383', '2001', 0],\n",
       " ['20383', '2002', 0],\n",
       " ['20383', '2003', 3],\n",
       " ['20383', '2004', 4],\n",
       " ['20383', '2005', 7],\n",
       " ['20383', '2006', 15],\n",
       " ['20383', '2007', 5],\n",
       " ['20383', '2008', 14],\n",
       " ['20383', '2009', 13],\n",
       " ['20383', '2010', 2],\n",
       " ['20494', '2000', 0],\n",
       " ['20494', '2001', 2],\n",
       " ['20494', '2002', 0],\n",
       " ['20494', '2003', 0],\n",
       " ['20494', '2004', 0],\n",
       " ['20494', '2005', 0],\n",
       " ['20494', '2006', 0],\n",
       " ['20494', '2007', 1],\n",
       " ['20494', '2008', 0],\n",
       " ['20494', '2009', 1],\n",
       " ['20494', '2010', 0],\n",
       " ['20534', '2000', 0],\n",
       " ['20534', '2001', 0],\n",
       " ['20534', '2002', 0],\n",
       " ['20534', '2003', 1],\n",
       " ['20534', '2004', 0],\n",
       " ['20534', '2005', 0],\n",
       " ['20534', '2006', 0],\n",
       " ['20534', '2007', 0],\n",
       " ['20534', '2008', 0],\n",
       " ['20534', '2009', 0],\n",
       " ['20534', '2010', 0],\n",
       " ['20634', '2000', 0],\n",
       " ['20634', '2001', 0],\n",
       " ['20634', '2002', 2],\n",
       " ['20634', '2003', 0],\n",
       " ['20634', '2004', 0],\n",
       " ['20634', '2005', 0],\n",
       " ['20634', '2006', 0],\n",
       " ['20634', '2007', 0],\n",
       " ['20634', '2008', 0],\n",
       " ['20634', '2009', 0],\n",
       " ['20634', '2010', 0],\n",
       " ['20952', '2000', 0],\n",
       " ['20952', '2001', 1],\n",
       " ['20952', '2002', 1],\n",
       " ['20952', '2003', 1],\n",
       " ['20952', '2004', 1],\n",
       " ['20952', '2005', 0],\n",
       " ['20952', '2006', 0],\n",
       " ['20952', '2007', 0],\n",
       " ['20952', '2008', 6],\n",
       " ['20952', '2009', 3],\n",
       " ['20952', '2010', 1],\n",
       " ['21087', '2000', 0],\n",
       " ['21087', '2001', 0],\n",
       " ['21087', '2002', 1],\n",
       " ['21087', '2003', 0],\n",
       " ['21087', '2004', 1],\n",
       " ['21087', '2005', 4],\n",
       " ['21087', '2006', 1],\n",
       " ['21087', '2007', 0],\n",
       " ['21087', '2008', 0],\n",
       " ['21087', '2009', 0],\n",
       " ['21087', '2010', 1],\n",
       " ['21116', '2000', 0],\n",
       " ['21116', '2001', 0],\n",
       " ['21116', '2002', 0],\n",
       " ['21116', '2003', 0],\n",
       " ['21116', '2004', 0],\n",
       " ['21116', '2005', 2],\n",
       " ['21116', '2006', 0],\n",
       " ['21116', '2007', 1],\n",
       " ['21116', '2008', 0],\n",
       " ['21116', '2009', 0],\n",
       " ['21116', '2010', 0],\n",
       " ['23291', '2000', 0],\n",
       " ['23291', '2001', 0],\n",
       " ['23291', '2002', 0],\n",
       " ['23291', '2003', 0],\n",
       " ['23291', '2004', 0],\n",
       " ['23291', '2005', 3],\n",
       " ['23291', '2006', 1],\n",
       " ['23291', '2007', 3],\n",
       " ['23291', '2008', 2],\n",
       " ['23291', '2009', 2],\n",
       " ['23291', '2010', 0],\n",
       " ['23292', '2000', 0],\n",
       " ['23292', '2001', 0],\n",
       " ['23292', '2002', 0],\n",
       " ['23292', '2003', 0],\n",
       " ['23292', '2004', 0],\n",
       " ['23292', '2005', 0],\n",
       " ['23292', '2006', 2],\n",
       " ['23292', '2007', 0],\n",
       " ['23292', '2008', 0],\n",
       " ['23292', '2009', 1],\n",
       " ['23292', '2010', 0],\n",
       " ['23293', '2000', 0],\n",
       " ['23293', '2001', 0],\n",
       " ['23293', '2002', 1],\n",
       " ['23293', '2003', 0],\n",
       " ['23293', '2004', 0],\n",
       " ['23293', '2005', 0],\n",
       " ['23293', '2006', 1],\n",
       " ['23293', '2007', 0],\n",
       " ['23293', '2008', 1],\n",
       " ['23293', '2009', 0],\n",
       " ['23293', '2010', 0],\n",
       " ['39252', '2000', 0],\n",
       " ['39252', '2001', 0],\n",
       " ['39252', '2002', 0],\n",
       " ['39252', '2003', 1],\n",
       " ['39252', '2004', 0],\n",
       " ['39252', '2005', 0],\n",
       " ['39252', '2006', 0],\n",
       " ['39252', '2007', 0],\n",
       " ['39252', '2008', 3],\n",
       " ['39252', '2009', 0],\n",
       " ['39252', '2010', 0],\n",
       " ['39260', '2000', 0],\n",
       " ['39260', '2001', 0],\n",
       " ['39260', '2002', 1],\n",
       " ['39260', '2003', 0],\n",
       " ['39260', '2004', 0],\n",
       " ['39260', '2005', 0],\n",
       " ['39260', '2006', 0],\n",
       " ['39260', '2007', 0],\n",
       " ['39260', '2008', 0],\n",
       " ['39260', '2009', 0],\n",
       " ['39260', '2010', 0],\n",
       " ['39305', '2000', 1],\n",
       " ['39305', '2001', 0],\n",
       " ['39305', '2002', 0],\n",
       " ['39305', '2003', 1],\n",
       " ['39305', '2004', 2],\n",
       " ['39305', '2005', 0],\n",
       " ['39305', '2006', 1],\n",
       " ['39305', '2007', 0],\n",
       " ['39305', '2008', 0],\n",
       " ['39305', '2009', 1],\n",
       " ['39305', '2010', 0],\n",
       " ['39518', '2000', 0],\n",
       " ['39518', '2001', 0],\n",
       " ['39518', '2002', 0],\n",
       " ['39518', '2003', 0],\n",
       " ['39518', '2004', 0],\n",
       " ['39518', '2005', 0],\n",
       " ['39518', '2006', 1],\n",
       " ['39518', '2007', 0],\n",
       " ['39518', '2008', 1],\n",
       " ['39518', '2009', 0],\n",
       " ['39518', '2010', 0],\n",
       " ['39789', '2000', 0],\n",
       " ['39789', '2001', 0],\n",
       " ['39789', '2002', 0],\n",
       " ['39789', '2003', 0],\n",
       " ['39789', '2004', 0],\n",
       " ['39789', '2005', 0],\n",
       " ['39789', '2006', 0],\n",
       " ['39789', '2007', 0],\n",
       " ['39789', '2008', 0],\n",
       " ['39789', '2009', 1],\n",
       " ['39789', '2010', 0],\n",
       " ['39861', '2000', 0],\n",
       " ['39861', '2001', 0],\n",
       " ['39861', '2002', 0],\n",
       " ['39861', '2003', 0],\n",
       " ['39861', '2004', 0],\n",
       " ['39861', '2005', 2],\n",
       " ['39861', '2006', 4],\n",
       " ['39861', '2007', 2],\n",
       " ['39861', '2008', 0],\n",
       " ['39861', '2009', 2],\n",
       " ['39861', '2010', 0],\n",
       " ['41420', '2000', 0],\n",
       " ['41420', '2001', 1],\n",
       " ['41420', '2002', 5],\n",
       " ['41420', '2003', 2],\n",
       " ['41420', '2004', 2],\n",
       " ['41420', '2005', 3],\n",
       " ['41420', '2006', 6],\n",
       " ['41420', '2007', 5],\n",
       " ['41420', '2008', 7],\n",
       " ['41420', '2009', 5],\n",
       " ['41420', '2010', 0],\n",
       " ['41428', '2000', 0],\n",
       " ['41428', '2001', 1],\n",
       " ['41428', '2002', 1],\n",
       " ['41428', '2003', 0],\n",
       " ['41428', '2004', 2],\n",
       " ['41428', '2005', 0],\n",
       " ['41428', '2006', 0],\n",
       " ['41428', '2007', 0],\n",
       " ['41428', '2008', 0],\n",
       " ['41428', '2009', 0],\n",
       " ['41428', '2010', 0],\n",
       " ['41440', '2000', 0],\n",
       " ['41440', '2001', 0],\n",
       " ['41440', '2002', 0],\n",
       " ['41440', '2003', 0],\n",
       " ['41440', '2004', 0],\n",
       " ['41440', '2005', 0],\n",
       " ['41440', '2006', 0],\n",
       " ['41440', '2007', 1],\n",
       " ['41440', '2008', 0],\n",
       " ['41440', '2009', 0],\n",
       " ['41440', '2010', 0],\n",
       " ['41447', '2000', 0],\n",
       " ['41447', '2001', 0],\n",
       " ['41447', '2002', 1],\n",
       " ['41447', '2003', 0],\n",
       " ['41447', '2004', 0],\n",
       " ['41447', '2005', 2],\n",
       " ['41447', '2006', 0],\n",
       " ['41447', '2007', 0],\n",
       " ['41447', '2008', 1],\n",
       " ['41447', '2009', 0],\n",
       " ['41447', '2010', 0],\n",
       " ['41473', '2000', 0],\n",
       " ['41473', '2001', 0],\n",
       " ['41473', '2002', 0],\n",
       " ['41473', '2003', 0],\n",
       " ['41473', '2004', 1],\n",
       " ['41473', '2005', 0],\n",
       " ['41473', '2006', 0],\n",
       " ['41473', '2007', 0],\n",
       " ['41473', '2008', 0],\n",
       " ['41473', '2009', 0],\n",
       " ['41473', '2010', 0],\n",
       " ['41523', '2000', 0],\n",
       " ['41523', '2001', 0],\n",
       " ['41523', '2002', 0],\n",
       " ['41523', '2003', 0],\n",
       " ['41523', '2004', 0],\n",
       " ['41523', '2005', 0],\n",
       " ['41523', '2006', 0],\n",
       " ['41523', '2007', 0],\n",
       " ['41523', '2008', 0],\n",
       " ['41523', '2009', 1],\n",
       " ['41523', '2010', 0],\n",
       " ['41546', '2000', 0],\n",
       " ['41546', '2001', 1],\n",
       " ['41546', '2002', 0],\n",
       " ['41546', '2003', 0],\n",
       " ['41546', '2004', 0],\n",
       " ['41546', '2005', 0],\n",
       " ['41546', '2006', 0],\n",
       " ['41546', '2007', 1],\n",
       " ['41546', '2008', 2],\n",
       " ['41546', '2009', 0],\n",
       " ['41546', '2010', 0],\n",
       " ['41588', '2000', 0],\n",
       " ['41588', '2001', 1],\n",
       " ['41588', '2002', 0],\n",
       " ['41588', '2003', 0],\n",
       " ['41588', '2004', 0],\n",
       " ['41588', '2005', 0],\n",
       " ['41588', '2006', 0],\n",
       " ['41588', '2007', 0],\n",
       " ['41588', '2008', 0],\n",
       " ['41588', '2009', 0],\n",
       " ['41588', '2010', 0],\n",
       " ['41600', '2000', 1],\n",
       " ['41600', '2001', 0],\n",
       " ['41600', '2002', 1],\n",
       " ['41600', '2003', 0],\n",
       " ['41600', '2004', 0],\n",
       " ['41600', '2005', 0],\n",
       " ['41600', '2006', 1],\n",
       " ['41600', '2007', 0],\n",
       " ['41600', '2008', 0],\n",
       " ['41600', '2009', 0],\n",
       " ['41600', '2010', 0],\n",
       " ['41604', '2000', 2],\n",
       " ['41604', '2001', 2],\n",
       " ['41604', '2002', 4],\n",
       " ['41604', '2003', 3],\n",
       " ['41604', '2004', 1],\n",
       " ['41604', '2005', 0],\n",
       " ['41604', '2006', 0],\n",
       " ['41604', '2007', 0],\n",
       " ['41604', '2008', 1],\n",
       " ['41604', '2009', 0],\n",
       " ['41604', '2010', 1],\n",
       " ['41671', '2000', 0],\n",
       " ['41671', '2001', 0],\n",
       " ['41671', '2002', 0],\n",
       " ['41671', '2003', 1],\n",
       " ['41671', '2004', 1],\n",
       " ['41671', '2005', 0],\n",
       " ['41671', '2006', 1],\n",
       " ['41671', '2007', 2],\n",
       " ['41671', '2008', 0],\n",
       " ['41671', '2009', 0],\n",
       " ['41671', '2010', 0],\n",
       " ['41672', '2000', 1],\n",
       " ['41672', '2001', 1],\n",
       " ['41672', '2002', 1],\n",
       " ['41672', '2003', 0],\n",
       " ['41672', '2004', 0],\n",
       " ['41672', '2005', 0],\n",
       " ['41672', '2006', 0],\n",
       " ['41672', '2007', 0],\n",
       " ['41672', '2008', 0],\n",
       " ['41672', '2009', 0],\n",
       " ['41672', '2010', 0],\n",
       " ['44834', '2000', 0],\n",
       " ['44834', '2001', 0],\n",
       " ['44834', '2002', 0],\n",
       " ['44834', '2003', 0],\n",
       " ['44834', '2004', 1],\n",
       " ['44834', '2005', 0],\n",
       " ['44834', '2006', 0],\n",
       " ['44834', '2007', 0],\n",
       " ['44834', '2008', 0],\n",
       " ['44834', '2009', 0],\n",
       " ['44834', '2010', 0],\n",
       " ['50066', '2000', 0],\n",
       " ['50066', '2001', 0],\n",
       " ['50066', '2002', 0],\n",
       " ['50066', '2003', 1],\n",
       " ['50066', '2004', 0],\n",
       " ['50066', '2005', 0],\n",
       " ['50066', '2006', 0],\n",
       " ['50066', '2007', 0],\n",
       " ['50066', '2008', 0],\n",
       " ['50066', '2009', 0],\n",
       " ['50066', '2010', 0],\n",
       " ['53584', '2000', 1],\n",
       " ['53584', '2001', 0],\n",
       " ['53584', '2002', 4],\n",
       " ['53584', '2003', 5],\n",
       " ['53584', '2004', 5],\n",
       " ['53584', '2005', 5],\n",
       " ['53584', '2006', 2],\n",
       " ['53584', '2007', 1],\n",
       " ['53584', '2008', 1],\n",
       " ['53584', '2009', 0],\n",
       " ['53584', '2010', 1],\n",
       " ['53600', '2000', 0],\n",
       " ['53600', '2001', 1],\n",
       " ['53600', '2002', 0],\n",
       " ['53600', '2003', 0],\n",
       " ['53600', '2004', 1],\n",
       " ['53600', '2005', 0],\n",
       " ['53600', '2006', 0],\n",
       " ['53600', '2007', 0],\n",
       " ['53600', '2008', 0],\n",
       " ['53600', '2009', 0],\n",
       " ['53600', '2010', 0],\n",
       " ['53610', '2000', 0],\n",
       " ['53610', '2001', 2],\n",
       " ['53610', '2002', 2],\n",
       " ['53610', '2003', 2],\n",
       " ['53610', '2004', 1],\n",
       " ['53610', '2005', 4],\n",
       " ['53610', '2006', 6],\n",
       " ['53610', '2007', 6],\n",
       " ['53610', '2008', 3],\n",
       " ['53610', '2009', 1],\n",
       " ['53610', '2010', 1],\n",
       " ['53625', '2000', 0],\n",
       " ['53625', '2001', 2],\n",
       " ['53625', '2002', 1],\n",
       " ['53625', '2003', 2],\n",
       " ['53625', '2004', 0],\n",
       " ['53625', '2005', 1],\n",
       " ['53625', '2006', 0],\n",
       " ['53625', '2007', 0],\n",
       " ['53625', '2008', 2],\n",
       " ['53625', '2009', 0],\n",
       " ['53625', '2010', 0],\n",
       " ['53665', '2000', 0],\n",
       " ['53665', '2001', 2],\n",
       " ['53665', '2002', 2],\n",
       " ['53665', '2003', 1],\n",
       " ['53665', '2004', 1],\n",
       " ['53665', '2005', 0],\n",
       " ['53665', '2006', 1],\n",
       " ['53665', '2007', 0],\n",
       " ['53665', '2008', 0],\n",
       " ['53665', '2009', 0],\n",
       " ['53665', '2010', 0],\n",
       " ['53701', '2000', 0],\n",
       " ['53701', '2001', 0],\n",
       " ['53701', '2002', 0],\n",
       " ['53701', '2003', 0],\n",
       " ['53701', '2004', 0],\n",
       " ['53701', '2005', 1],\n",
       " ['53701', '2006', 0],\n",
       " ['53701', '2007', 3],\n",
       " ['53701', '2008', 0],\n",
       " ['53701', '2009', 3],\n",
       " ['53701', '2010', 1],\n",
       " ['53713', '2000', 0],\n",
       " ['53713', '2001', 5],\n",
       " ['53713', '2002', 8],\n",
       " ['53713', '2003', 6],\n",
       " ['53713', '2004', 8],\n",
       " ['53713', '2005', 10],\n",
       " ['53713', '2006', 5],\n",
       " ['53713', '2007', 11],\n",
       " ['53713', '2008', 6],\n",
       " ['53713', '2009', 9],\n",
       " ['53713', '2010', 1],\n",
       " ['53732', '2000', 1],\n",
       " ['53732', '2001', 3],\n",
       " ['53732', '2002', 3],\n",
       " ['53732', '2003', 2],\n",
       " ['53732', '2004', 8],\n",
       " ['53732', '2005', 6],\n",
       " ['53732', '2006', 9],\n",
       " ['53732', '2007', 6],\n",
       " ['53732', '2008', 11],\n",
       " ['53732', '2009', 13],\n",
       " ['53732', '2010', 2],\n",
       " ['53755', '2000', 0],\n",
       " ['53755', '2001', 0],\n",
       " ['53755', '2002', 1],\n",
       " ['53755', '2003', 1],\n",
       " ['53755', '2004', 1],\n",
       " ['53755', '2005', 1],\n",
       " ['53755', '2006', 1],\n",
       " ['53755', '2007', 0],\n",
       " ['53755', '2008', 1],\n",
       " ['53755', '2009', 2],\n",
       " ['53755', '2010', 0],\n",
       " ['53758', '2000', 0],\n",
       " ['53758', '2001', 7],\n",
       " ['53758', '2002', 4],\n",
       " ['53758', '2003', 1],\n",
       " ['53758', '2004', 5],\n",
       " ['53758', '2005', 6],\n",
       " ['53758', '2006', 7],\n",
       " ['53758', '2007', 5],\n",
       " ['53758', '2008', 7],\n",
       " ['53758', '2009', 5],\n",
       " ['53758', '2010', 0],\n",
       " ['53761', '2000', 0],\n",
       " ['53761', '2001', 0],\n",
       " ['53761', '2002', 1],\n",
       " ['53761', '2003', 1],\n",
       " ['53761', '2004', 1],\n",
       " ['53761', '2005', 0],\n",
       " ['53761', '2006', 2],\n",
       " ['53761', '2007', 3],\n",
       " ['53761', '2008', 1],\n",
       " ['53761', '2009', 0],\n",
       " ['53761', '2010', 0],\n",
       " ['53791', '2000', 1],\n",
       " ['53791', '2001', 0],\n",
       " ['53791', '2002', 0],\n",
       " ['53791', '2003', 0],\n",
       " ['53791', '2004', 0],\n",
       " ['53791', '2005', 1],\n",
       " ['53791', '2006', 1],\n",
       " ['53791', '2007', 0],\n",
       " ['53791', '2008', 1],\n",
       " ['53791', '2009', 1],\n",
       " ['53791', '2010', 0],\n",
       " ['53831', '2000', 0],\n",
       " ['53831', '2001', 1],\n",
       " ['53831', '2002', 1],\n",
       " ['53831', '2003', 4],\n",
       " ['53831', '2004', 2],\n",
       " ['53831', '2005', 2],\n",
       " ['53831', '2006', 1],\n",
       " ['53831', '2007', 1],\n",
       " ['53831', '2008', 3],\n",
       " ['53831', '2009', 0],\n",
       " ['53831', '2010', 0],\n",
       " ['53835', '2000', 0],\n",
       " ['53835', '2001', 0],\n",
       " ['53835', '2002', 1],\n",
       " ['53835', '2003', 1],\n",
       " ['53835', '2004', 1],\n",
       " ['53835', '2005', 1],\n",
       " ['53835', '2006', 1],\n",
       " ['53835', '2007', 0],\n",
       " ['53835', '2008', 0],\n",
       " ['53835', '2009', 0],\n",
       " ['53835', '2010', 0],\n",
       " ['60916', '2000', 1],\n",
       " ['60916', '2001', 0],\n",
       " ['60916', '2002', 0],\n",
       " ['60916', '2003', 0],\n",
       " ['60916', '2004', 1],\n",
       " ['60916', '2005', 0],\n",
       " ['60916', '2006', 1],\n",
       " ['60916', '2007', 1],\n",
       " ['60916', '2008', 0],\n",
       " ['60916', '2009', 0],\n",
       " ['60916', '2010', 0],\n",
       " ['60939', '2000', 1],\n",
       " ['60939', '2001', 0],\n",
       " ['60939', '2002', 4],\n",
       " ['60939', '2003', 2],\n",
       " ['60939', '2004', 2],\n",
       " ['60939', '2005', 1],\n",
       " ['60939', '2006', 5],\n",
       " ['60939', '2007', 8],\n",
       " ['60939', '2008', 4],\n",
       " ['60939', '2009', 3],\n",
       " ['60939', '2010', 0],\n",
       " ['60955', '2000', 1],\n",
       " ['60955', '2001', 0],\n",
       " ['60955', '2002', 1],\n",
       " ['60955', '2003', 0],\n",
       " ['60955', '2004', 1],\n",
       " ['60955', '2005', 1],\n",
       " ['60955', '2006', 1],\n",
       " ['60955', '2007', 0],\n",
       " ['60955', '2008', 4],\n",
       " ['60955', '2009', 2],\n",
       " ['60955', '2010', 0],\n",
       " ['61021', '2000', 0],\n",
       " ['61021', '2001', 0],\n",
       " ['61021', '2002', 0],\n",
       " ['61021', '2003', 1],\n",
       " ['61021', '2004', 0],\n",
       " ['61021', '2005', 1],\n",
       " ['61021', '2006', 1],\n",
       " ['61021', '2007', 1],\n",
       " ['61021', '2008', 0],\n",
       " ['61021', '2009', 0],\n",
       " ['61021', '2010', 0],\n",
       " ['61048', '2000', 1],\n",
       " ['61048', '2001', 1],\n",
       " ['61048', '2002', 0],\n",
       " ['61048', '2003', 1],\n",
       " ['61048', '2004', 0],\n",
       " ['61048', '2005', 0],\n",
       " ['61048', '2006', 1],\n",
       " ['61048', '2007', 1],\n",
       " ['61048', '2008', 0],\n",
       " ['61048', '2009', 0],\n",
       " ['61048', '2010', 0],\n",
       " ['61053', '2000', 0],\n",
       " ['61053', '2001', 0],\n",
       " ['61053', '2002', 1],\n",
       " ['61053', '2003', 0],\n",
       " ['61053', '2004', 0],\n",
       " ['61053', '2005', 1],\n",
       " ['61053', '2006', 2],\n",
       " ['61053', '2007', 2],\n",
       " ['61053', '2008', 0],\n",
       " ['61053', '2009', 0],\n",
       " ['61053', '2010', 0],\n",
       " ['61078', '2000', 0],\n",
       " ['61078', '2001', 1],\n",
       " ['61078', '2002', 0],\n",
       " ['61078', '2003', 0],\n",
       " ['61078', '2004', 0],\n",
       " ['61078', '2005', 1],\n",
       " ['61078', '2006', 0],\n",
       " ['61078', '2007', 0],\n",
       " ['61078', '2008', 0],\n",
       " ['61078', '2009', 0],\n",
       " ['61078', '2010', 0],\n",
       " ['61125', '2000', 1],\n",
       " ['61125', '2001', 0],\n",
       " ['61125', '2002', 0],\n",
       " ['61125', '2003', 0],\n",
       " ['61125', '2004', 2],\n",
       " ['61125', '2005', 0],\n",
       " ['61125', '2006', 4],\n",
       " ['61125', '2007', 2],\n",
       " ['61125', '2008', 4],\n",
       " ['61125', '2009', 1],\n",
       " ['61125', '2010', 1],\n",
       " ['62742', '2000', 0],\n",
       " ['62742', '2001', 0],\n",
       " ['62742', '2002', 0],\n",
       " ['62742', '2003', 0],\n",
       " ['62742', '2004', 1],\n",
       " ['62742', '2005', 1],\n",
       " ['62742', '2006', 1],\n",
       " ['62742', '2007', 2],\n",
       " ['62742', '2008', 3],\n",
       " ['62742', '2009', 6],\n",
       " ['62742', '2010', 0],\n",
       " ['71315', '2000', 0],\n",
       " ['71315', '2001', 1],\n",
       " ['71315', '2002', 2],\n",
       " ['71315', '2003', 1],\n",
       " ['71315', '2004', 1],\n",
       " ['71315', '2005', 1],\n",
       " ['71315', '2006', 2],\n",
       " ['71315', '2007', 1],\n",
       " ['71315', '2008', 0],\n",
       " ['71315', '2009', 1],\n",
       " ['71315', '2010', 1],\n",
       " ['71927', '2000', 0],\n",
       " ['71927', '2001', 0],\n",
       " ['71927', '2002', 0],\n",
       " ['71927', '2003', 0],\n",
       " ['71927', '2004', 0],\n",
       " ['71927', '2005', 0],\n",
       " ['71927', '2006', 0],\n",
       " ['71927', '2007', 1],\n",
       " ['71927', '2008', 0],\n",
       " ['71927', '2009', 0],\n",
       " ['71927', '2010', 0],\n",
       " ['77045', '2000', 0],\n",
       " ['77045', '2001', 1],\n",
       " ['77045', '2002', 0],\n",
       " ['77045', '2003', 0],\n",
       " ['77045', '2004', 0],\n",
       " ['77045', '2005', 0],\n",
       " ['77045', '2006', 1],\n",
       " ['77045', '2007', 1],\n",
       " ['77045', '2008', 1],\n",
       " ['77045', '2009', 0],\n",
       " ['77045', '2010', 1],\n",
       " ['77100', '2000', 0],\n",
       " ['77100', '2001', 0],\n",
       " ['77100', '2002', 2],\n",
       " ['77100', '2003', 3],\n",
       " ['77100', '2004', 1],\n",
       " ['77100', '2005', 1],\n",
       " ['77100', '2006', 1],\n",
       " ['77100', '2007', 0],\n",
       " ['77100', '2008', 2],\n",
       " ['77100', '2009', 0],\n",
       " ['77100', '2010', 0],\n",
       " ['77108', '2000', 0],\n",
       " ['77108', '2001', 0],\n",
       " ['77108', '2002', 1],\n",
       " ['77108', '2003', 0],\n",
       " ['77108', '2004', 2],\n",
       " ['77108', '2005', 2],\n",
       " ['77108', '2006', 0],\n",
       " ['77108', '2007', 4],\n",
       " ['77108', '2008', 1],\n",
       " ['77108', '2009', 1],\n",
       " ['77108', '2010', 0],\n",
       " ['77195', '2000', 0],\n",
       " ['77195', '2001', 0],\n",
       " ['77195', '2002', 0],\n",
       " ['77195', '2003', 0],\n",
       " ['77195', '2004', 1],\n",
       " ['77195', '2005', 0],\n",
       " ['77195', '2006', 0],\n",
       " ['77195', '2007', 0],\n",
       " ['77195', '2008', 1],\n",
       " ['77195', '2009', 0],\n",
       " ['77195', '2010', 0],\n",
       " ['77277', '2000', 1],\n",
       " ['77277', '2001', 0],\n",
       " ['77277', '2002', 2],\n",
       " ['77277', '2003', 0],\n",
       " ['77277', '2004', 1],\n",
       " ['77277', '2005', 1],\n",
       " ['77277', '2006', 1],\n",
       " ['77277', '2007', 0],\n",
       " ['77277', '2008', 0],\n",
       " ['77277', '2009', 0],\n",
       " ['77277', '2010', 0],\n",
       " ['77314', '2000', 0],\n",
       " ['77314', '2001', 0],\n",
       " ['77314', '2002', 0],\n",
       " ['77314', '2003', 0],\n",
       " ['77314', '2004', 1],\n",
       " ['77314', '2005', 0],\n",
       " ['77314', '2006', 1],\n",
       " ['77314', '2007', 0],\n",
       " ['77314', '2008', 0],\n",
       " ['77314', '2009', 0],\n",
       " ['77314', '2010', 0],\n",
       " ['79285', '2000', 0],\n",
       " ['79285', '2001', 1],\n",
       " ['79285', '2002', 2],\n",
       " ['79285', '2003', 3],\n",
       " ['79285', '2004', 4],\n",
       " ['79285', '2005', 0],\n",
       " ['79285', '2006', 3],\n",
       " ['79285', '2007', 0],\n",
       " ['79285', '2008', 0],\n",
       " ['79285', '2009', 1],\n",
       " ['79285', '2010', 0],\n",
       " ['87013', '2000', 0],\n",
       " ['87013', '2001', 1],\n",
       " ['87013', '2002', 0],\n",
       " ['87013', '2003', 1],\n",
       " ['87013', '2004', 1],\n",
       " ['87013', '2005', 1],\n",
       " ['87013', '2006', 0],\n",
       " ['87013', '2007', 1],\n",
       " ['87013', '2008', 0],\n",
       " ['87013', '2009', 0],\n",
       " ['87013', '2010', 0],\n",
       " ['87466', '2000', 0],\n",
       " ['87466', '2001', 0],\n",
       " ['87466', '2002', 0],\n",
       " ['87466', '2003', 0],\n",
       " ['87466', '2004', 0],\n",
       " ['87466', '2005', 2],\n",
       " ['87466', '2006', 4],\n",
       " ['87466', '2007', 3],\n",
       " ['87466', '2008', 3],\n",
       " ['87466', '2009', 3],\n",
       " ['87466', '2010', 0],\n",
       " ['87467', '2000', 3],\n",
       " ['87467', '2001', 0],\n",
       " ['87467', '2002', 1],\n",
       " ['87467', '2003', 2],\n",
       " ['87467', '2004', 4],\n",
       " ['87467', '2005', 4],\n",
       " ['87467', '2006', 4],\n",
       " ['87467', '2007', 4],\n",
       " ['87467', '2008', 5],\n",
       " ['87467', '2009', 6],\n",
       " ['87467', '2010', 1],\n",
       " ['87575', '2000', 0],\n",
       " ['87575', '2001', 2],\n",
       " ['87575', '2002', 0],\n",
       " ['87575', '2003', 2],\n",
       " ['87575', '2004', 3],\n",
       " ['87575', '2005', 0],\n",
       " ['87575', '2006', 2],\n",
       " ['87575', '2007', 3],\n",
       " ['87575', '2008', 4],\n",
       " ['87575', '2009', 2],\n",
       " ['87575', '2010', 2],\n",
       " ['87792', '2000', 0],\n",
       " ['87792', '2001', 2],\n",
       " ['87792', '2002', 1],\n",
       " ['87792', '2003', 1],\n",
       " ['87792', '2004', 6],\n",
       " ['87792', '2005', 5],\n",
       " ['87792', '2006', 9],\n",
       " ['87792', '2007', 10],\n",
       " ['87792', '2008', 6],\n",
       " ['87792', '2009', 2],\n",
       " ['87792', '2010', 0],\n",
       " ['87888', '2000', 1],\n",
       " ['87888', '2001', 0],\n",
       " ['87888', '2002', 4],\n",
       " ['87888', '2003', 1],\n",
       " ['87888', '2004', 4],\n",
       " ['87888', '2005', 7],\n",
       " ['87888', '2006', 3],\n",
       " ['87888', '2007', 7],\n",
       " ['87888', '2008', 8],\n",
       " ['87888', '2009', 6],\n",
       " ['87888', '2010', 1],\n",
       " ['88032', '2000', 3],\n",
       " ['88032', '2001', 1],\n",
       " ['88032', '2002', 0],\n",
       " ['88032', '2003', 0],\n",
       " ['88032', '2004', 1],\n",
       " ['88032', '2005', 1],\n",
       " ['88032', '2006', 0],\n",
       " ['88032', '2007', 6],\n",
       " ['88032', '2008', 1],\n",
       " ['88032', '2009', 1],\n",
       " ['88032', '2010', 0],\n",
       " ['88596', '2000', 1],\n",
       " ['88596', '2001', 2],\n",
       " ['88596', '2002', 0],\n",
       " ['88596', '2003', 0],\n",
       " ['88596', '2004', 0],\n",
       " ['88596', '2005', 1],\n",
       " ['88596', '2006', 0],\n",
       " ['88596', '2007', 0],\n",
       " ['88596', '2008', 0],\n",
       " ['88596', '2009', 0],\n",
       " ['88596', '2010', 0],\n",
       " ['89100', '2000', 0],\n",
       " ['89100', '2001', 0],\n",
       " ['89100', '2002', 1],\n",
       " ['89100', '2003', 5],\n",
       " ['89100', '2004', 1],\n",
       " ['89100', '2005', 0],\n",
       " ['89100', '2006', 1],\n",
       " ['89100', '2007', 0],\n",
       " ['89100', '2008', 1],\n",
       " ['89100', '2009', 3],\n",
       " ['89100', '2010', 0],\n",
       " ['89206', '2000', 1],\n",
       " ['89206', '2001', 0],\n",
       " ['89206', '2002', 1],\n",
       " ['89206', '2003', 1],\n",
       " ['89206', '2004', 2],\n",
       " ['89206', '2005', 0],\n",
       " ['89206', '2006', 2],\n",
       " ['89206', '2007', 1],\n",
       " ['89206', '2008', 2],\n",
       " ['89206', '2009', 3],\n",
       " ['89206', '2010', 0],\n",
       " ['89222', '2000', 1],\n",
       " ['89222', '2001', 3],\n",
       " ['89222', '2002', 1],\n",
       " ['89222', '2003', 4],\n",
       " ['89222', '2004', 4],\n",
       " ['89222', '2005', 3],\n",
       " ['89222', '2006', 3],\n",
       " ['89222', '2007', 6],\n",
       " ['89222', '2008', 7],\n",
       " ['89222', '2009', 4],\n",
       " ['89222', '2010', 0],\n",
       " ['89253', '2000', 0],\n",
       " ['89253', '2001', 0],\n",
       " ['89253', '2002', 0],\n",
       " ['89253', '2003', 2],\n",
       " ['89253', '2004', 1],\n",
       " ['89253', '2005', 1],\n",
       " ['89253', '2006', 0],\n",
       " ['89253', '2007', 2],\n",
       " ['89253', '2008', 1],\n",
       " ['89253', '2009', 3],\n",
       " ['89253', '2010', 1],\n",
       " ['89344', '2000', 3],\n",
       " ['89344', '2001', 1],\n",
       " ['89344', '2002', 0],\n",
       " ['89344', '2003', 1],\n",
       " ['89344', '2004', 4],\n",
       " ['89344', '2005', 1],\n",
       " ['89344', '2006', 1],\n",
       " ['89344', '2007', 3],\n",
       " ['89344', '2008', 0],\n",
       " ['89344', '2009', 4],\n",
       " ['89344', '2010', 0],\n",
       " ['89910', '2000', 0],\n",
       " ['89910', '2001', 0],\n",
       " ['89910', '2002', 0],\n",
       " ['89910', '2003', 0],\n",
       " ['89910', '2004', 2],\n",
       " ['89910', '2005', 0],\n",
       " ['89910', '2006', 1],\n",
       " ['89910', '2007', 1],\n",
       " ['89910', '2008', 0],\n",
       " ['89910', '2009', 1],\n",
       " ['89910', '2010', 0],\n",
       " ['90042', '2000', 0],\n",
       " ['90042', '2001', 1],\n",
       " ['90042', '2002', 1],\n",
       " ['90042', '2003', 3],\n",
       " ['90042', '2004', 6],\n",
       " ['90042', '2005', 5],\n",
       " ['90042', '2006', 7],\n",
       " ['90042', '2007', 4],\n",
       " ['90042', '2008', 2],\n",
       " ['90042', '2009', 5],\n",
       " ['90042', '2010', 0],\n",
       " ['90055', '2000', 0],\n",
       " ['90055', '2001', 0],\n",
       " ['90055', '2002', 0],\n",
       " ['90055', '2003', 2],\n",
       " ['90055', '2004', 0],\n",
       " ['90055', '2005', 2],\n",
       " ['90055', '2006', 2],\n",
       " ['90055', '2007', 0],\n",
       " ['90055', '2008', 0],\n",
       " ['90055', '2009', 0],\n",
       " ['90055', '2010', 0],\n",
       " ['90496', '2000', 0],\n",
       " ['90496', '2001', 1],\n",
       " ['90496', '2002', 0],\n",
       " ['90496', '2003', 0],\n",
       " ['90496', '2004', 1],\n",
       " ['90496', '2005', 1],\n",
       " ['90496', '2006', 0],\n",
       " ['90496', '2007', 2],\n",
       " ['90496', '2008', 2],\n",
       " ['90496', '2009', 0],\n",
       " ['90496', '2010', 1],\n",
       " ['90722', '2000', 0],\n",
       " ['90722', '2001', 1],\n",
       " ['90722', '2002', 0],\n",
       " ['90722', '2003', 1],\n",
       " ['90722', '2004', 2],\n",
       " ['90722', '2005', 1],\n",
       " ['90722', '2006', 0],\n",
       " ['90722', '2007', 0],\n",
       " ['90722', '2008', 1],\n",
       " ['90722', '2009', 0],\n",
       " ...]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citation_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "244479e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/dblp_paper/paper_1749_citation.pkl', 'wb') as f:\n",
    "    pickle.dump(citation_count_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "805f9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_2_paper_index = {}\n",
    "idx = 0\n",
    "for paper_index in list(index_cite_count_nonzero.keys()):\n",
    "    idx_2_paper_index[idx] = paper_index\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "aa435c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_citation = []\n",
    "for idx in range(len(list(idx_2_paper_index.keys()))):\n",
    "    tmp = []\n",
    "    for indicator in citation_count_list:\n",
    "        if indicator[0] == idx_2_paper_index[idx]:\n",
    "            tmp.append([[indicator[2]], indicator[1]])\n",
    "    \n",
    "    indicator_citation.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9aaa0bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[15], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[14], '2008'],\n",
       "  [[13], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[6], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[8], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[8], '2004'],\n",
       "  [[10], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[11], '2007'],\n",
       "  [[6], '2008'],\n",
       "  [[9], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[8], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[11], '2008'],\n",
       "  [[13], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[7], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[7], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[3], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[10], '2007'],\n",
       "  [[6], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[3], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[3], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[7], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[9], '2007'],\n",
       "  [[6], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[9], '2007'],\n",
       "  [[9], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[9], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[13], '2004'],\n",
       "  [[13], '2005'],\n",
       "  [[14], '2006'],\n",
       "  [[10], '2007'],\n",
       "  [[11], '2008'],\n",
       "  [[9], '2009'],\n",
       "  [[3], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[12], '2003'],\n",
       "  [[22], '2004'],\n",
       "  [[22], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[10], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[9], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[8], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[7], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[15], '2006'],\n",
       "  [[12], '2007'],\n",
       "  [[16], '2008'],\n",
       "  [[14], '2009'],\n",
       "  [[3], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[13], '2007'],\n",
       "  [[14], '2008'],\n",
       "  [[11], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[8], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[10], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[6], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[10], '2002'],\n",
       "  [[11], '2003'],\n",
       "  [[7], '2004'],\n",
       "  [[12], '2005'],\n",
       "  [[8], '2006'],\n",
       "  [[9], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[10], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[9], '2007'],\n",
       "  [[14], '2008'],\n",
       "  [[13], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[8], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[10], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[9], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[14], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[8], '2006'],\n",
       "  [[9], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[7], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[8], '2003'],\n",
       "  [[8], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[12], '2008'],\n",
       "  [[8], '2009'],\n",
       "  [[3], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[8], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[7], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[9], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[3], '2000'],\n",
       "  [[11], '2001'],\n",
       "  [[18], '2002'],\n",
       "  [[19], '2003'],\n",
       "  [[25], '2004'],\n",
       "  [[20], '2005'],\n",
       "  [[25], '2006'],\n",
       "  [[29], '2007'],\n",
       "  [[22], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[7], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[7], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[9], '2001'],\n",
       "  [[10], '2002'],\n",
       "  [[11], '2003'],\n",
       "  [[14], '2004'],\n",
       "  [[13], '2005'],\n",
       "  [[13], '2006'],\n",
       "  [[10], '2007'],\n",
       "  [[9], '2008'],\n",
       "  [[12], '2009'],\n",
       "  [[3], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[10], '2003'],\n",
       "  [[14], '2004'],\n",
       "  [[17], '2005'],\n",
       "  [[14], '2006'],\n",
       "  [[12], '2007'],\n",
       "  [[11], '2008'],\n",
       "  [[9], '2009'],\n",
       "  [[4], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[8], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[22], '2001'],\n",
       "  [[21], '2002'],\n",
       "  [[38], '2003'],\n",
       "  [[48], '2004'],\n",
       "  [[42], '2005'],\n",
       "  [[54], '2006'],\n",
       "  [[41], '2007'],\n",
       "  [[55], '2008'],\n",
       "  [[35], '2009'],\n",
       "  [[14], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[13], '2004'],\n",
       "  [[8], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[8], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[9], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[13], '2006'],\n",
       "  [[9], '2007'],\n",
       "  [[6], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[12], '2008'],\n",
       "  [[10], '2009'],\n",
       "  [[3], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[8], '2003'],\n",
       "  [[7], '2004'],\n",
       "  [[13], '2005'],\n",
       "  [[13], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[9], '2008'],\n",
       "  [[9], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[13], '2001'],\n",
       "  [[9], '2002'],\n",
       "  [[9], '2003'],\n",
       "  [[11], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[10], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[7], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[9], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[7], '2001'],\n",
       "  [[11], '2002'],\n",
       "  [[47], '2003'],\n",
       "  [[36], '2004'],\n",
       "  [[49], '2005'],\n",
       "  [[65], '2006'],\n",
       "  [[61], '2007'],\n",
       "  [[59], '2008'],\n",
       "  [[48], '2009'],\n",
       "  [[11], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[8], '2001'],\n",
       "  [[10], '2002'],\n",
       "  [[25], '2003'],\n",
       "  [[32], '2004'],\n",
       "  [[50], '2005'],\n",
       "  [[55], '2006'],\n",
       "  [[71], '2007'],\n",
       "  [[52], '2008'],\n",
       "  [[67], '2009'],\n",
       "  [[22], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[7], '2001'],\n",
       "  [[7], '2002'],\n",
       "  [[11], '2003'],\n",
       "  [[13], '2004'],\n",
       "  [[19], '2005'],\n",
       "  [[18], '2006'],\n",
       "  [[16], '2007'],\n",
       "  [[20], '2008'],\n",
       "  [[16], '2009'],\n",
       "  [[3], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[7], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[10], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[15], '2003'],\n",
       "  [[12], '2004'],\n",
       "  [[21], '2005'],\n",
       "  [[15], '2006'],\n",
       "  [[28], '2007'],\n",
       "  [[22], '2008'],\n",
       "  [[17], '2009'],\n",
       "  [[10], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[9], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[8], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[7], '2002'],\n",
       "  [[9], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[3], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[9], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[3], '2000'],\n",
       "  [[10], '2001'],\n",
       "  [[9], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[9], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[10], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[17], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[3], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[3], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[11], '2001'],\n",
       "  [[8], '2002'],\n",
       "  [[12], '2003'],\n",
       "  [[12], '2004'],\n",
       "  [[12], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[10], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[7], '2001'],\n",
       "  [[8], '2002'],\n",
       "  [[11], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[11], '2006'],\n",
       "  [[9], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[7], '2001'],\n",
       "  [[13], '2002'],\n",
       "  [[16], '2003'],\n",
       "  [[16], '2004'],\n",
       "  [[28], '2005'],\n",
       "  [[22], '2006'],\n",
       "  [[20], '2007'],\n",
       "  [[9], '2008'],\n",
       "  [[16], '2009'],\n",
       "  [[5], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[8], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[9], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[8], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[3], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[9], '2001'],\n",
       "  [[8], '2002'],\n",
       "  [[9], '2003'],\n",
       "  [[9], '2004'],\n",
       "  [[13], '2005'],\n",
       "  [[11], '2006'],\n",
       "  [[9], '2007'],\n",
       "  [[11], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[5], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[8], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[10], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[7], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[10], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[9], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[7], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[12], '2003'],\n",
       "  [[12], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[7], '2006'],\n",
       "  [[11], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[8], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[3], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[3], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[11], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[7], '2001'],\n",
       "  [[7], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[10], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[10], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[8], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[11], '2005'],\n",
       "  [[7], '2006'],\n",
       "  [[9], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[13], '2002'],\n",
       "  [[9], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[12], '2005'],\n",
       "  [[14], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[9], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[10], '2001'],\n",
       "  [[13], '2002'],\n",
       "  [[12], '2003'],\n",
       "  [[13], '2004'],\n",
       "  [[9], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[13], '2007'],\n",
       "  [[12], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[7], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[8], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[13], '2002'],\n",
       "  [[14], '2003'],\n",
       "  [[15], '2004'],\n",
       "  [[8], '2005'],\n",
       "  [[7], '2006'],\n",
       "  [[11], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[13], '2002'],\n",
       "  [[14], '2003'],\n",
       "  [[9], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[12], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[10], '2006'],\n",
       "  [[17], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[10], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[9], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[10], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[3], '2000'],\n",
       "  [[7], '2001'],\n",
       "  [[16], '2002'],\n",
       "  [[16], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[11], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[8], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[8], '2002'],\n",
       "  [[13], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[9], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[7], '2006'],\n",
       "  [[16], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[13], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[20], '2001'],\n",
       "  [[14], '2002'],\n",
       "  [[24], '2003'],\n",
       "  [[19], '2004'],\n",
       "  [[11], '2005'],\n",
       "  [[8], '2006'],\n",
       "  [[19], '2007'],\n",
       "  [[18], '2008'],\n",
       "  [[11], '2009'],\n",
       "  [[3], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[12], '2002'],\n",
       "  [[9], '2003'],\n",
       "  [[8], '2004'],\n",
       "  [[10], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[19], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[11], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[11], '2001'],\n",
       "  [[23], '2002'],\n",
       "  [[18], '2003'],\n",
       "  [[23], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[9], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[12], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[14], '2001'],\n",
       "  [[24], '2002'],\n",
       "  [[23], '2003'],\n",
       "  [[28], '2004'],\n",
       "  [[12], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[10], '2007'],\n",
       "  [[6], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[8], '2001'],\n",
       "  [[20], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[8], '2004'],\n",
       "  [[8], '2005'],\n",
       "  [[12], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[12], '2002'],\n",
       "  [[10], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[13], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[8], '2005'],\n",
       "  [[8], '2006'],\n",
       "  [[13], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[7], '2001'],\n",
       "  [[14], '2002'],\n",
       "  [[12], '2003'],\n",
       "  [[14], '2004'],\n",
       "  [[16], '2005'],\n",
       "  [[11], '2006'],\n",
       "  [[22], '2007'],\n",
       "  [[11], '2008'],\n",
       "  [[16], '2009'],\n",
       "  [[3], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[13], '2002'],\n",
       "  [[8], '2003'],\n",
       "  [[7], '2004'],\n",
       "  [[9], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[11], '2006'],\n",
       "  [[10], '2007'],\n",
       "  [[6], '2008'],\n",
       "  [[8], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[9], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[10], '2002'],\n",
       "  [[12], '2003'],\n",
       "  [[8], '2004'],\n",
       "  [[12], '2005'],\n",
       "  [[12], '2006'],\n",
       "  [[12], '2007'],\n",
       "  [[14], '2008'],\n",
       "  [[13], '2009'],\n",
       "  [[3], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[8], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[6], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[15], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[10], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[6], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[8], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[7], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[7], '2002'],\n",
       "  [[9], '2003'],\n",
       "  [[12], '2004'],\n",
       "  [[15], '2005'],\n",
       "  [[22], '2006'],\n",
       "  [[48], '2007'],\n",
       "  [[35], '2008'],\n",
       "  [[38], '2009'],\n",
       "  [[9], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[8], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[11], '2004'],\n",
       "  [[10], '2005'],\n",
       "  [[7], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[9], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[8], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[15], '2006'],\n",
       "  [[14], '2007'],\n",
       "  [[11], '2008'],\n",
       "  [[16], '2009'],\n",
       "  [[4], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[20], '2002'],\n",
       "  [[21], '2003'],\n",
       "  [[23], '2004'],\n",
       "  [[18], '2005'],\n",
       "  [[26], '2006'],\n",
       "  [[16], '2007'],\n",
       "  [[23], '2008'],\n",
       "  [[18], '2009'],\n",
       "  [[6], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[8], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[3], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[9], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[10], '2003'],\n",
       "  [[10], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[9], '2007'],\n",
       "  [[6], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[21], '2002'],\n",
       "  [[37], '2003'],\n",
       "  [[35], '2004'],\n",
       "  [[37], '2005'],\n",
       "  [[40], '2006'],\n",
       "  [[51], '2007'],\n",
       "  [[45], '2008'],\n",
       "  [[49], '2009'],\n",
       "  [[8], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[12], '2002'],\n",
       "  [[14], '2003'],\n",
       "  [[11], '2004'],\n",
       "  [[12], '2005'],\n",
       "  [[11], '2006'],\n",
       "  [[10], '2007'],\n",
       "  [[13], '2008'],\n",
       "  [[18], '2009'],\n",
       "  [[6], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[10], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[8], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[12], '2007'],\n",
       "  [[10], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[7], '2005'],\n",
       "  [[11], '2006'],\n",
       "  [[18], '2007'],\n",
       "  [[10], '2008'],\n",
       "  [[13], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[10], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[16], '2003'],\n",
       "  [[19], '2004'],\n",
       "  [[25], '2005'],\n",
       "  [[19], '2006'],\n",
       "  [[20], '2007'],\n",
       "  [[18], '2008'],\n",
       "  [[15], '2009'],\n",
       "  [[5], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[7], '2003'],\n",
       "  [[7], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[7], '2006'],\n",
       "  [[12], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[10], '2009'],\n",
       "  [[5], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[9], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[9], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[4], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[12], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[9], '2003'],\n",
       "  [[14], '2004'],\n",
       "  [[14], '2005'],\n",
       "  [[16], '2006'],\n",
       "  [[15], '2007'],\n",
       "  [[24], '2008'],\n",
       "  [[16], '2009'],\n",
       "  [[5], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[4], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[7], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[6], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[14], '2003'],\n",
       "  [[7], '2004'],\n",
       "  [[12], '2005'],\n",
       "  [[8], '2006'],\n",
       "  [[11], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[8], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[7], '2001'],\n",
       "  [[14], '2002'],\n",
       "  [[14], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[15], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[5], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[6], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[3], '2001'],\n",
       "  [[16], '2002'],\n",
       "  [[10], '2003'],\n",
       "  [[8], '2004'],\n",
       "  [[10], '2005'],\n",
       "  [[10], '2006'],\n",
       "  [[11], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[11], '2009'],\n",
       "  [[6], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[4], '2008'],\n",
       "  [[7], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[5], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[5], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[7], '2002'],\n",
       "  [[5], '2003'],\n",
       "  [[4], '2004'],\n",
       "  [[5], '2005'],\n",
       "  [[7], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[6], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[3], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[5], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[4], '2003'],\n",
       "  [[9], '2004'],\n",
       "  [[6], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[8], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[5], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[6], '2006'],\n",
       "  [[6], '2007'],\n",
       "  [[8], '2008'],\n",
       "  [[5], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[3], '2006'],\n",
       "  [[4], '2007'],\n",
       "  [[7], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[2], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[2], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[2], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[3], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[4], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[7], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[6], '2002'],\n",
       "  [[6], '2003'],\n",
       "  [[10], '2004'],\n",
       "  [[11], '2005'],\n",
       "  [[9], '2006'],\n",
       "  [[14], '2007'],\n",
       "  [[18], '2008'],\n",
       "  [[11], '2009'],\n",
       "  [[5], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[3], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[3], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[2], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[4], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[2], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[1], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[3], '2003'],\n",
       "  [[6], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[4], '2006'],\n",
       "  [[8], '2007'],\n",
       "  [[5], '2008'],\n",
       "  [[3], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[2], '2002'],\n",
       "  [[2], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[3], '2005'],\n",
       "  [[0], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[0], '2001'],\n",
       "  [[0], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[2], '2004'],\n",
       "  [[2], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[3], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[2], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[1], '2000'],\n",
       "  [[2], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[1], '2003'],\n",
       "  [[1], '2004'],\n",
       "  [[1], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[1], '2007'],\n",
       "  [[0], '2008'],\n",
       "  [[1], '2009'],\n",
       "  [[0], '2010']],\n",
       " [[[0], '2000'],\n",
       "  [[1], '2001'],\n",
       "  [[1], '2002'],\n",
       "  [[0], '2003'],\n",
       "  [[0], '2004'],\n",
       "  [[0], '2005'],\n",
       "  [[1], '2006'],\n",
       "  [[0], '2007'],\n",
       "  [[1], '2008'],\n",
       "  [[0], '2009'],\n",
       "  [[0], '2010']],\n",
       " ...]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicator_citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d39ce195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/dblp_paper/indicator_input.pkl', 'wb') as f:\n",
    "    pickle.dump(indicator_citation, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baf310be",
   "metadata": {},
   "source": [
    "## DBLP paper graph (graph_input.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "420349da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract author and citation relations from high quality papers\n",
    "paper_author = {}\n",
    "paper_citation = {}\n",
    "author_list = []\n",
    "paper_list = []\n",
    "years = ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010']\n",
    "\n",
    "for item in paper_list_filter:\n",
    "    author_list.extend(item['author'].split(','))\n",
    "    paper_list.append(item['index'])\n",
    "    paper_author[item['index']] = {'author': item['author'].split(','), 'year': item['year']}\n",
    "    paper_citation[item['index']] = {'paper': item['ref_index'], 'year': item['year']}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "50924da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = list(set(author_list))\n",
    "paper_list = list(set(paper_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0f735773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66501"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(paper_author.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c2c4e30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66501"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(paper_citation.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "df75d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue add paper index\n",
    "exist_paper_index = list(idx_2_paper_index.values())\n",
    "idx = max(list(idx_2_paper_index.keys())) + 1\n",
    "for paper_index in paper_list:\n",
    "    if paper_index in exist_paper_index:\n",
    "        continue\n",
    "    else:\n",
    "        idx_2_paper_index[idx] = paper_index\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "983f8ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66500"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(idx_2_paper_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1c8c932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_2_author_name = {}\n",
    "idx = 70000\n",
    "for author_name in author_list:\n",
    "    idx_2_author_name[idx] = author_name\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "16eeb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_name_2_idx = {}\n",
    "for k,v in idx_2_author_name.items():\n",
    "    author_name_2_idx[v] = k\n",
    "\n",
    "paper_index_2_idx = {}\n",
    "for k,v in idx_2_paper_index.items():\n",
    "    paper_index_2_idx[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6fb5dcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{70000: 'Kouichi Sakurai',\n",
       " 70001: 'Sathish Gopalakrishnan',\n",
       " 70002: 'Hemant Tagare',\n",
       " 70003: 'Jonathan de Halleux',\n",
       " 70004: 'Karina Hauser',\n",
       " 70005: 'Zhigang Peng',\n",
       " 70006: 'Edwin K. P. Chong',\n",
       " 70007: 'Michael Dittenbach',\n",
       " 70008: 'Yuan-Tse Yu',\n",
       " 70009: 'Paul Thompson',\n",
       " 70010: 'Thierry Mora',\n",
       " 70011: 'Natalie Lee-San Pang',\n",
       " 70012: 'Traci A. Carte',\n",
       " 70013: 'Z. M. Ma',\n",
       " 70014: 'Junyu Dong',\n",
       " 70015: 'Emmanuel Juin',\n",
       " 70016: 'Deng Cai',\n",
       " 70017: \"Andrea D'Ambrogio\",\n",
       " 70018: 'Janne Riihijärvi',\n",
       " 70019: 'James A. Storer',\n",
       " 70020: 'Jagoba Arias',\n",
       " 70021: 'Riko Jacob',\n",
       " 70022: 'Peter R. Wilson',\n",
       " 70023: 'Ravi Janardan',\n",
       " 70024: 'Darrell C. Anderson',\n",
       " 70025: 'Yingwei Cui',\n",
       " 70026: 'Alan Halverson',\n",
       " 70027: 'Matthew Suderman',\n",
       " 70028: 'Reetuparna Das',\n",
       " 70029: 'Christopher Schwarz',\n",
       " 70030: 'Ferdi A. Smit',\n",
       " 70031: 'Su-Ping Chen',\n",
       " 70032: 'Daniela Giorgetti',\n",
       " 70033: 'Zhongnan Zhang',\n",
       " 70034: 'Vishal J. Mehta',\n",
       " 70035: 'Rami Beidas',\n",
       " 70036: 'Robert C. Goldstein',\n",
       " 70037: 'Farshad Fotouhi',\n",
       " 70038: 'Pao-Hwa Yang',\n",
       " 70039: 'Tobias Dyrks',\n",
       " 70040: 'Desh Ranjan',\n",
       " 70041: 'Kaifeng Xu',\n",
       " 70042: 'Walter Whiteley',\n",
       " 70043: 'Vikram Jandhyala',\n",
       " 70044: 'Jérôme Durand-Lose',\n",
       " 70045: 'Anupam Chattopadhyay',\n",
       " 70046: 'Craig Van Slyke',\n",
       " 70047: 'Lefteris M. Kirousis',\n",
       " 70048: 'Yasue Kishino',\n",
       " 70049: 'Deok-Hwan Kim',\n",
       " 70050: 'Eric Cronin',\n",
       " 70051: 'De-Jun Guan',\n",
       " 70052: 'Roland Kaschek',\n",
       " 70053: 'Dominic G. Lucchetti',\n",
       " 70054: 'Anh N. Le',\n",
       " 70055: 'Elisabetta Delponte',\n",
       " 70056: 'Mark Roberts',\n",
       " 70057: 'Yann Michel',\n",
       " 70058: 'Kang Li',\n",
       " 70059: 'D. Ll. L. Rees',\n",
       " 70060: 'Sebastian Montabone',\n",
       " 70061: 'Motoki Obata',\n",
       " 70062: 'Mark R. Shinwell',\n",
       " 70063: 'Der-Jiunn Deng',\n",
       " 70064: 'Narendran Sachindran',\n",
       " 70065: 'Xianglan Chen',\n",
       " 70066: 'Nevin Kirman',\n",
       " 70067: 'Mats Skoglund',\n",
       " 70068: 'Frank Bentley',\n",
       " 70069: 'Sitt Sen Chok',\n",
       " 70070: 'Nisse Husberg',\n",
       " 70071: 'Changxuan Wan',\n",
       " 70072: 'M. Y. Y. Leung',\n",
       " 70073: 'Elena García Barriocanal',\n",
       " 70074: 'Khaled M. Alzoubi',\n",
       " 70075: 'Isabelle Sivignon',\n",
       " 70076: 'Pawel Wawrzynski',\n",
       " 70077: 'Jeannie A. Stamberger',\n",
       " 70078: 'Guohong Fu',\n",
       " 70079: 'David A. Weitzman',\n",
       " 70080: 'Rodrigo de Oliveira',\n",
       " 70081: 'Héctor Corzo',\n",
       " 70082: 'Roberto Furnari',\n",
       " 70083: 'Julien Pettré',\n",
       " 70084: 'Mary Jean Harrold',\n",
       " 70085: 'Simon Ormholt Schrøder',\n",
       " 70086: 'Cindy Xinmin Chen',\n",
       " 70087: 'Alexander Bachmann',\n",
       " 70088: 'Andreas Tolk',\n",
       " 70089: 'Scott T. Campbell',\n",
       " 70090: 'Thomas Neidhart',\n",
       " 70091: 'Stathes Hadjiefthymiades',\n",
       " 70092: 'Martina Angela Sasse',\n",
       " 70093: 'Thomas Wöhner',\n",
       " 70094: 'Daniel R. L. Brown',\n",
       " 70095: 'Jin Li',\n",
       " 70096: 'Ricky A. Kendall',\n",
       " 70097: 'William K. Josephson',\n",
       " 70098: 'Yi Fang',\n",
       " 70099: 'Agnès Front',\n",
       " 70100: 'Kin-Sum Mak',\n",
       " 70101: 'Kemafor Anyanwu',\n",
       " 70102: 'Jørgen Bang-Jensen',\n",
       " 70103: 'Qian You',\n",
       " 70104: 'Bernd Jähne',\n",
       " 70105: 'Jaewon Sung',\n",
       " 70106: 'Ikuko Shimizu Okatani',\n",
       " 70107: 'Elizabeth Nunge',\n",
       " 70108: 'James K. Lan',\n",
       " 70109: 'Carlos André Guimarães Ferraz',\n",
       " 70110: 'Philip W. Trinder',\n",
       " 70111: 'Andreas Leininger',\n",
       " 70112: 'David Simons',\n",
       " 70113: 'Florina Almenárez Mendoza',\n",
       " 70114: 'Alain Casali',\n",
       " 70115: 'Seungchul Ha',\n",
       " 70116: 'Patrick Schaumont',\n",
       " 70117: 'Gregorij Kurillo',\n",
       " 70118: 'James Hammerton',\n",
       " 70119: 'Jung-Yu Lai',\n",
       " 70120: 'Alfonso F. Cardenas',\n",
       " 70121: 'Brooke Cowan',\n",
       " 70122: 'Xun Cheng',\n",
       " 70123: 'Igor Guskov',\n",
       " 70124: 'Andreas Krause',\n",
       " 70125: 'Suneeta Ramaswami',\n",
       " 70126: 'David Friggens',\n",
       " 70127: 'Juan Guillen Scholten',\n",
       " 70128: 'J.-Y. Lee',\n",
       " 70129: 'Jennifer Ren',\n",
       " 70130: 'Lihong Ma',\n",
       " 70131: 'Zixiang Xiong',\n",
       " 70132: 'Tomohiro Yamasaki',\n",
       " 70133: 'Tore Risch',\n",
       " 70134: 'Tetsuo Sawaragi',\n",
       " 70135: 'Gabriela Peretti',\n",
       " 70136: 'Liwei Dai',\n",
       " 70137: 'Deniz Balkan',\n",
       " 70138: 'Jong Kuk Park',\n",
       " 70139: 'A. Persson',\n",
       " 70140: 'Felipe Bertrand',\n",
       " 70141: 'Sami Rollins',\n",
       " 70142: 'Robert C. Chalmers',\n",
       " 70143: 'Elisa Martínez',\n",
       " 70144: 'Matthew Garber',\n",
       " 70145: 'Francis Crimmins',\n",
       " 70146: 'Raymond Y. K. Lau',\n",
       " 70147: 'Zifeng Hou',\n",
       " 70148: 'Bärbel Mertsching',\n",
       " 70149: 'Yufan Zhu',\n",
       " 70150: 'Juan Du',\n",
       " 70151: 'Itay Bar Yosef',\n",
       " 70152: 'Donguk Kim',\n",
       " 70153: 'I. Elizabeth Shanthi',\n",
       " 70154: 'Howard H. Chen',\n",
       " 70155: 'Edda Happ',\n",
       " 70156: 'James Scott',\n",
       " 70157: 'Rex Min',\n",
       " 70158: 'Xiaojian Zhao',\n",
       " 70159: 'Thomas Polacsek',\n",
       " 70160: 'Kuei-Ping Shih',\n",
       " 70161: 'Lewis W. F. Li',\n",
       " 70162: 'Özgür Kabak',\n",
       " 70163: 'Olivier Lavialle',\n",
       " 70164: 'Anoop Gupta',\n",
       " 70165: 'Pasi Tyrväinen',\n",
       " 70166: 'Jesper G. Henriksen',\n",
       " 70167: 'Andrea Tomatis',\n",
       " 70168: 'Rémi Forax',\n",
       " 70169: 'Hayato Yamana',\n",
       " 70170: 'Sergiu M. Dascalu',\n",
       " 70171: 'Alexandre Z. Caldeira',\n",
       " 70172: 'Robert R. Kessler',\n",
       " 70173: 'Ye Diana Wang',\n",
       " 70174: 'Andrzej Sluzek',\n",
       " 70175: 'Norman Chang',\n",
       " 70176: 'Jeff White',\n",
       " 70177: 'Thomas A. Henzinger',\n",
       " 70178: 'Dong-Ho Lee',\n",
       " 70179: 'Rajiv Govindarajan',\n",
       " 70180: 'Nedyalko Borisov',\n",
       " 70181: 'Mustafa Mat Deris',\n",
       " 70182: 'In Hwan Doh',\n",
       " 70183: 'James Dinan',\n",
       " 70184: 'Hongyuan Shi',\n",
       " 70185: 'Pascal Fradet',\n",
       " 70186: 'Minh Hoai Nguyen',\n",
       " 70187: 'Manigandan Radhakrishnan',\n",
       " 70188: 'William J. Buchanan',\n",
       " 70189: 'Su-Cheng Haw',\n",
       " 70190: 'C. C. Lin',\n",
       " 70191: 'Sarah E. Crudge',\n",
       " 70192: 'Jochen Messner',\n",
       " 70193: 'Seongbeom Kim',\n",
       " 70194: 'Prafulla Joglekar',\n",
       " 70195: 'Alessandro Dal Palù',\n",
       " 70196: 'Carol de Benito',\n",
       " 70197: 'Ji-Han Jiang',\n",
       " 70198: 'R. Yugo Kartono Isal',\n",
       " 70199: 'Rattapon Limprasittiporn',\n",
       " 70200: 'Dejin Zhao',\n",
       " 70201: 'Volker Gruhn',\n",
       " 70202: 'Gang George Yin',\n",
       " 70203: 'José Miguel Cleva',\n",
       " 70204: 'S. Seshadri',\n",
       " 70205: 'Yingdong Lu',\n",
       " 70206: 'Andrew Trotman',\n",
       " 70207: 'Xiaoyun Li',\n",
       " 70208: 'Yu Tezuka',\n",
       " 70209: 'Mikko Viikari',\n",
       " 70210: 'Xiaofan Jiang',\n",
       " 70211: 'Devi Parikh',\n",
       " 70212: 'Michael Wachter',\n",
       " 70213: 'Theresa Dirndorfer Anderson',\n",
       " 70214: 'Shai Fine',\n",
       " 70215: 'Gernot Heiser',\n",
       " 70216: 'Thi Minh Chau Tran',\n",
       " 70217: 'Giorgio Vassallo',\n",
       " 70218: 'Imade Benelallam',\n",
       " 70219: 'Helmut Krcmar',\n",
       " 70220: 'Mehmet Artiklar',\n",
       " 70221: 'Melanie Hilario',\n",
       " 70222: 'Dianxun Shuai',\n",
       " 70223: 'Matthew S. Gibbs',\n",
       " 70224: 'Esa Rahtu',\n",
       " 70225: 'Kurt Rohloff',\n",
       " 70226: 'Peep Küngas',\n",
       " 70227: 'Adrian Carlos Ferreira',\n",
       " 70228: 'Joong Kyu Kim',\n",
       " 70229: 'Qian Zhou',\n",
       " 70230: 'Kostas Masselos',\n",
       " 70231: 'Achille Fokoue',\n",
       " 70232: 'Jehad Najjar',\n",
       " 70233: 'Jesse Fox',\n",
       " 70234: 'Youcef Bouchebaba',\n",
       " 70235: 'Clelia de Felice',\n",
       " 70236: 'Sachiko Yoshihama',\n",
       " 70237: 'Zhouyao Chen',\n",
       " 70238: 'Danlu Zhang',\n",
       " 70239: 'Adam Wierman',\n",
       " 70240: 'Hua Huang',\n",
       " 70241: 'Lawrence T. Pileggi',\n",
       " 70242: 'Zhibin Jiang',\n",
       " 70243: 'Dong Jin',\n",
       " 70244: 'Sukho Lee',\n",
       " 70245: 'Matthew E. Taylor',\n",
       " 70246: 'Peter I. Cowling',\n",
       " 70247: 'Ojelanki K. Ngwenyama',\n",
       " 70248: 'Hua Guo',\n",
       " 70249: 'Lixia Liu',\n",
       " 70250: 'Stephen Cox',\n",
       " 70251: 'Jay Anderson',\n",
       " 70252: 'Kallirroi Georgila',\n",
       " 70253: 'Austin T. Clements',\n",
       " 70254: 'Andreas P. J. Breu',\n",
       " 70255: 'Kael Rowan',\n",
       " 70256: 'Elliot Jaffe',\n",
       " 70257: 'Martha White',\n",
       " 70258: 'Thomas A. Maier',\n",
       " 70259: 'Sonia L. Rueda',\n",
       " 70260: 'Valentín Valero Ruiz',\n",
       " 70261: 'Yingpeng Sang',\n",
       " 70262: 'David Jensen',\n",
       " 70263: 'Patrice Boizumault',\n",
       " 70264: 'Flavia Sparacino',\n",
       " 70265: 'Greg Seidman',\n",
       " 70266: 'Sam Ellis',\n",
       " 70267: 'Roland T. Chin',\n",
       " 70268: 'Kim Potter Kihlstrom',\n",
       " 70269: 'Sholom M. Weiss',\n",
       " 70270: 'Erez Petrank',\n",
       " 70271: 'Stefan Näher',\n",
       " 70272: 'Naoya Maruyama',\n",
       " 70273: 'Atsushi Ohnishi',\n",
       " 70274: 'Hung-Chang Hsiao',\n",
       " 70275: 'Nils Hanssen',\n",
       " 70276: 'Yao-Hsien Hsu',\n",
       " 70277: 'Roger Wattenhofer',\n",
       " 70278: 'Alyssa B. Apsel',\n",
       " 70279: 'Flaviu Cristian',\n",
       " 70280: 'Jarmo Toivonen',\n",
       " 70281: 'Asia Slowinska',\n",
       " 70282: 'Jianwen Yin',\n",
       " 70283: 'Fabrice Theoleyre',\n",
       " 70284: 'David Brooks',\n",
       " 70285: 'Chris C. Kirkham',\n",
       " 70286: 'Spyros Blanas',\n",
       " 70287: 'V. Kumar',\n",
       " 70288: 'Yun Wang',\n",
       " 70289: 'Tomer Hertz',\n",
       " 70290: 'John G. Breslin',\n",
       " 70291: 'Vyas Sekar',\n",
       " 70292: 'Luciano Baresi',\n",
       " 70293: 'Izchak Sharfman',\n",
       " 70294: 'Fei Yin',\n",
       " 70295: 'Neeraj Kumar Singh',\n",
       " 70296: 'Éric Fusy',\n",
       " 70297: 'George T. Heineman',\n",
       " 70298: 'Jeffrey Scott Vitter',\n",
       " 70299: 'Yuh-Chuyn Luo',\n",
       " 70300: 'Catherine Letondal',\n",
       " 70301: 'Martin Trautmann',\n",
       " 70302: 'Ramesh Govindan',\n",
       " 70303: 'Luc Devroye',\n",
       " 70304: 'Alexander Bochman',\n",
       " 70305: 'W. F. Lu',\n",
       " 70306: 'Kehan Gao',\n",
       " 70307: 'Antonio Robles-Gómez',\n",
       " 70308: 'Ted Bapty',\n",
       " 70309: 'Andreas Geppert',\n",
       " 70310: 'Mel Slater',\n",
       " 70311: 'HyoJong Shin',\n",
       " 70312: 'David Gay',\n",
       " 70313: 'Xinyang Zhang',\n",
       " 70314: 'Antão Moura',\n",
       " 70315: 'Scott Kagan',\n",
       " 70316: 'Jeffrey Hammes',\n",
       " 70317: 'Zhongfei Zhang',\n",
       " 70318: 'Ahmed Al-Emran',\n",
       " 70319: 'Kazuyuki Samejima',\n",
       " 70320: 'Alec Woo',\n",
       " 70321: 'Dana S. Nau',\n",
       " 70322: 'Mizuho Iwaihara',\n",
       " 70323: 'Chase Freeman',\n",
       " 70324: 'James Clawson',\n",
       " 70325: 'Bernard Wong',\n",
       " 70326: 'Vittorio Ghini',\n",
       " 70327: 'Robert Bosch',\n",
       " 70328: 'Leonid Reyzin',\n",
       " 70329: 'Alf Zugenmaier',\n",
       " 70330: 'M. Benjelloun',\n",
       " 70331: 'Jouni Huotari',\n",
       " 70332: 'Juan Pablo López-Grao',\n",
       " 70333: 'Javier Tejel',\n",
       " 70334: 'ShaoJie Tang',\n",
       " 70335: 'Michael R. Lieberman',\n",
       " 70336: 'Mike P. Papazoglou',\n",
       " 70337: 'Dingxing Wang',\n",
       " 70338: 'Yuan-Ting Kao',\n",
       " 70339: 'Jiexin Lian',\n",
       " 70340: 'Li Qin',\n",
       " 70341: 'Michael Grafe',\n",
       " 70342: 'Daniel Cheng',\n",
       " 70343: 'Alia I. Abdelmoty',\n",
       " 70344: 'Carlos Cardenas',\n",
       " 70345: 'Alptekin Küpçü',\n",
       " 70346: 'Zvi Galil',\n",
       " 70347: 'Hua Shu',\n",
       " 70348: 'Ricardo Poley Martins Ferreira',\n",
       " 70349: 'Jennifer Jie Xu',\n",
       " 70350: 'Byung-Hyun Yu',\n",
       " 70351: 'Traian Pop',\n",
       " 70352: 'Berilhes Borges Garcia',\n",
       " 70353: 'Jan Arne Telle',\n",
       " 70354: 'Farookh Khadeer Hussain',\n",
       " 70355: 'Régis Vincent',\n",
       " 70356: 'Maozhen Li',\n",
       " 70357: 'Chris Develder',\n",
       " 70358: 'Ronald T. Fernández',\n",
       " 70359: 'Gareth Funka-Lea',\n",
       " 70360: 'S. Gordon',\n",
       " 70361: 'Anil Nerode',\n",
       " 70362: 'Zhisheng Li',\n",
       " 70363: 'Liang Lin',\n",
       " 70364: 'Wenlei Mao',\n",
       " 70365: 'Anupriya Ankolekar',\n",
       " 70366: 'Jun-Sung Kim',\n",
       " 70367: 'Cees Snoek',\n",
       " 70368: 'Daniela Rosca',\n",
       " 70369: 'Alexey Kupriyanov',\n",
       " 70370: 'Christopher D. Shaw',\n",
       " 70371: 'Amir Hossein Ghamarian',\n",
       " 70372: 'Chun-Yen Hsu',\n",
       " 70373: 'Wanlin Pang',\n",
       " 70374: 'Chris R. Johnson',\n",
       " 70375: 'I. Livenson',\n",
       " 70376: 'Hsin-Mu Tsai',\n",
       " 70377: 'Gilles Georges',\n",
       " 70378: 'Terri Hedgpeth',\n",
       " 70379: 'Gregory Marton',\n",
       " 70380: 'Ivan Deras',\n",
       " 70381: 'Kehuan Zhang',\n",
       " 70382: 'Petro Verkhogliad',\n",
       " 70383: 'Eric Debreuve',\n",
       " 70384: 'Minlong Lin',\n",
       " 70385: 'Tor Erlend Fægri',\n",
       " 70386: 'Fabio Paternò',\n",
       " 70387: 'Francisco José Alfaro',\n",
       " 70388: 'Shian-Shyong Tseng',\n",
       " 70389: 'Reshma Dixit',\n",
       " 70390: 'Luciano José Senger',\n",
       " 70391: 'Sarah Pennington',\n",
       " 70392: 'Ilias Michalarias',\n",
       " 70393: 'Steinar Kristoffersen',\n",
       " 70394: 'Sivan Toledo',\n",
       " 70395: 'Vijay K. Vaishnavi',\n",
       " 70396: 'Yan-Nei Law',\n",
       " 70397: 'Barbara Pernici',\n",
       " 70398: 'Fernando Díaz',\n",
       " 70399: 'Manuel M. T. Chakravarty',\n",
       " 70400: 'Yozo Hida',\n",
       " 70401: 'James H. Garrett Jr.',\n",
       " 70402: 'Ilmério Silva',\n",
       " 70403: 'Hoifung Poon',\n",
       " 70404: 'Daewook Kim',\n",
       " 70405: 'A. Vashisth',\n",
       " 70406: 'Micha Streppel',\n",
       " 70407: 'Marcelo Fiszman',\n",
       " 70408: 'Su-Wei Tan',\n",
       " 70409: 'Rakhi Chandrasekharan',\n",
       " 70410: 'Nicolai Petkov',\n",
       " 70411: 'Gábor Lugosi',\n",
       " 70412: 'Jon Hasselgren',\n",
       " 70413: 'Konstantin Kolchin',\n",
       " 70414: 'Aydan R. Yumerefendi',\n",
       " 70415: 'Christos Papadopoulos',\n",
       " 70416: 'Jonathan Chang',\n",
       " 70417: 'Andrej Dobnikar',\n",
       " 70418: 'Bernard Boigelot',\n",
       " 70419: 'Salvatore Venticinque',\n",
       " 70420: 'Michael W. Mislove',\n",
       " 70421: 'Takahiro Sakamoto',\n",
       " 70422: 'Nitin Navale',\n",
       " 70423: 'Dragan Tubic',\n",
       " 70424: 'Arie Matsliah',\n",
       " 70425: 'Gireesh Shrimali',\n",
       " 70426: 'René Caubet',\n",
       " 70427: 'Zhixun Su',\n",
       " 70428: 'Andrew Butterfield',\n",
       " 70429: 'Julien Vion',\n",
       " 70430: 'Roland Chapuis',\n",
       " 70431: 'Davide Ciucci',\n",
       " 70432: 'Yankui Feng',\n",
       " 70433: 'Suvranu De',\n",
       " 70434: 'Anders Yeo',\n",
       " 70435: 'Amol Nayate',\n",
       " 70436: 'Kevin Drummey',\n",
       " 70437: 'John H. Quick',\n",
       " 70438: 'Ieva Mitasiunaite',\n",
       " 70439: 'Vladimir Jakobac',\n",
       " 70440: 'Loïc Mazo',\n",
       " 70441: 'Cedric Ho',\n",
       " 70442: 'Michael Carbin',\n",
       " 70443: 'Maurício O. Tsugawa',\n",
       " 70444: 'Markus Neteler',\n",
       " 70445: 'Elaine Chew',\n",
       " 70446: 'Rosa Maria Vicari',\n",
       " 70447: 'Han-Bin Chang',\n",
       " 70448: 'Piyush Bansal',\n",
       " 70449: 'Vigyan Singhal',\n",
       " 70450: 'David W. Currie',\n",
       " 70451: 'Diptikalyan Saha',\n",
       " 70452: 'Alex Kondratyev',\n",
       " 70453: 'Milind Kulkarni',\n",
       " 70454: 'Thomas Schwarz',\n",
       " 70455: 'Evangelos Kalogerakis',\n",
       " 70456: 'Stuart N. K. Watt',\n",
       " 70457: 'Maria Chudnovsky',\n",
       " 70458: 'Stefan Römer',\n",
       " 70459: 'Luís Duarte',\n",
       " 70460: 'Zuren Feng',\n",
       " 70461: 'Frederick L. Crabbe',\n",
       " 70462: 'Chris Gray',\n",
       " 70463: 'Kenichi Hagihara',\n",
       " 70464: 'Michael M. Zharov',\n",
       " 70465: 'Wolfgang Glänzel',\n",
       " 70466: 'Axel Mönkeberg',\n",
       " 70467: 'Antonino Mazzeo',\n",
       " 70468: 'Reshef Meir',\n",
       " 70469: 'Nicolas Thériault',\n",
       " 70470: 'Erina Ferro',\n",
       " 70471: 'Ravi R. Iyer',\n",
       " 70472: 'Hassan Chafi',\n",
       " 70473: 'Mark Herbster',\n",
       " 70474: 'Sandra Sandri',\n",
       " 70475: 'P. Goddi',\n",
       " 70476: 'Jiang Li',\n",
       " 70477: 'Robert S. Laramee',\n",
       " 70478: 'Vassilis G. Kaburlasos',\n",
       " 70479: 'Mohammad Fathian',\n",
       " 70480: 'Ali El-Haj-Mahmoud',\n",
       " 70481: 'Lisa Higham',\n",
       " 70482: 'Boon-Ping Gan',\n",
       " 70483: 'Karl Beecher',\n",
       " 70484: 'Sumit Sarkar',\n",
       " 70485: 'Patricia Velázquez-Morales',\n",
       " 70486: 'Pascal Vicaire',\n",
       " 70487: 'Salwa K. Abd-El-Hafiz',\n",
       " 70488: 'Mingliang Zhu',\n",
       " 70489: 'John K. Ousterhout',\n",
       " 70490: 'Erik Anderson',\n",
       " 70491: 'R. Zölch',\n",
       " 70492: 'Prodromos D. Chatzoglou',\n",
       " 70493: 'Chenghua Lin',\n",
       " 70494: 'Jens Lufter',\n",
       " 70495: 'Paul D. Amer',\n",
       " 70496: 'Qingsheng Ren',\n",
       " 70497: 'Dimitrios D. Vergados',\n",
       " 70498: 'Tansu Alpcan',\n",
       " 70499: 'Yu-Chun Chang',\n",
       " 70500: 'Jürgen P. Schulze',\n",
       " 70501: 'Miriam Brandl',\n",
       " 70502: 'Tobias Gass',\n",
       " 70503: 'Ramon Nou',\n",
       " 70504: 'Weifa Liang',\n",
       " 70505: 'Ravi Kokku',\n",
       " 70506: 'Lichun Yang',\n",
       " 70507: 'David Mountain',\n",
       " 70508: 'Narasimha Bolloju',\n",
       " 70509: 'Akinori Kawachi',\n",
       " 70510: 'Sriram Gopal',\n",
       " 70511: 'Thomas King',\n",
       " 70512: 'Cristian Tejos',\n",
       " 70513: 'Ashvin Goel',\n",
       " 70514: 'Jie Li',\n",
       " 70515: 'Laura M. Grupp',\n",
       " 70516: 'Gao Cong',\n",
       " 70517: 'Qimin Sun',\n",
       " 70518: 'Heather Stoddart Barros',\n",
       " 70519: 'Subhankar Dhar',\n",
       " 70520: 'Yansheng Qu',\n",
       " 70521: 'Ya-Hui Chen',\n",
       " 70522: 'Ehab Abdelhamid',\n",
       " 70523: 'Lakshminarayanan Subramanian',\n",
       " 70524: 'Maksim Jenihhin',\n",
       " 70525: 'Ana Lucia Varbanescu',\n",
       " 70526: 'Thanassis Rikakis',\n",
       " 70527: 'Bo Li',\n",
       " 70528: 'Egidio Astesiano',\n",
       " 70529: 'Tetsuo Yamabe',\n",
       " 70530: 'I.-C. Park',\n",
       " 70531: 'Guohui Li',\n",
       " 70532: 'Ryan Eustice',\n",
       " 70533: 'Nathan Shnidman',\n",
       " 70534: 'Praveen Pathak',\n",
       " 70535: 'John Dunagan',\n",
       " 70536: 'Greg Horvath',\n",
       " 70537: 'Tiberiu Stratulat',\n",
       " 70538: 'Jun Zhang',\n",
       " 70539: 'Adam Morrison',\n",
       " 70540: 'Thanh-Duy Nguyen',\n",
       " 70541: 'Oscar Marbán',\n",
       " 70542: 'Judy Kay',\n",
       " 70543: 'Kaare Bøegh',\n",
       " 70544: 'Vince W. C. Chook',\n",
       " 70545: 'Sean Wheeler',\n",
       " 70546: 'Felix Sheng-Ho Chang',\n",
       " 70547: 'Khandoker Tarik-Ul Islam',\n",
       " 70548: 'Wentao Zhao',\n",
       " 70549: 'Christoph Helmberg',\n",
       " 70550: 'Martí Sánchez',\n",
       " 70551: 'Siva Nageswara Rao Borra',\n",
       " 70552: 'DeLiang L. Wang',\n",
       " 70553: 'Daniel Kristjansson',\n",
       " 70554: 'Youngmin Kim',\n",
       " 70555: 'Biao Yang',\n",
       " 70556: 'Barbara Jobstmann',\n",
       " 70557: 'Servane Gey',\n",
       " 70558: 'Ruijing Shen',\n",
       " 70559: 'Dimitris G. Kapopoulos',\n",
       " 70560: 'Efstratios T. Diamadis',\n",
       " 70561: 'Gregory Kulczycki',\n",
       " 70562: 'Amerson Lin',\n",
       " 70563: 'Ashwin Swaminathan',\n",
       " 70564: 'Kishore N. Menezes',\n",
       " 70565: 'William Y. Chen',\n",
       " 70566: 'Theodoros Salonidis',\n",
       " 70567: 'Barry Shackleford',\n",
       " 70568: 'Christopher D. Janneck',\n",
       " 70569: 'Andrew Blake',\n",
       " 70570: 'Kasper Bonne Rasmussen',\n",
       " 70571: 'Teng-Tiow Tay',\n",
       " 70572: 'Abdelaziz Bensrhair',\n",
       " 70573: 'Rina Dechter',\n",
       " 70574: 'Antonio Augusto de Aragão Rocha',\n",
       " 70575: 'Mark Looi',\n",
       " 70576: 'Amgad Zeitoun',\n",
       " 70577: 'Chun Chan',\n",
       " 70578: 'Alan L. Tharp',\n",
       " 70579: 'Nicolas Monmarché',\n",
       " 70580: 'Michael Steinbach',\n",
       " 70581: 'Ramanan Sankaran',\n",
       " 70582: 'Dimitrios I. Fotiadis',\n",
       " 70583: 'Dyi-Rong Duh',\n",
       " 70584: 'Muh-Chyun (Morris) Tang',\n",
       " 70585: 'Xiaoling Sun',\n",
       " 70586: 'Wolfgang Haid',\n",
       " 70587: 'Leen Stougie',\n",
       " 70588: 'L. Schröder',\n",
       " 70589: 'Kiem Hoang',\n",
       " 70590: 'Cristina N. González-Caro',\n",
       " 70591: 'Erika Gunadi',\n",
       " 70592: 'Long Fei',\n",
       " 70593: 'Camilla Schwind',\n",
       " 70594: 'Simon E. Overell',\n",
       " 70595: 'Lifeng Sang',\n",
       " 70596: 'Marco Colombetti',\n",
       " 70597: 'Biplav Srivastava',\n",
       " 70598: 'Erick Cantú-Paz',\n",
       " 70599: 'Peter J. Nürnberg',\n",
       " 70600: 'Ileana Ober',\n",
       " 70601: 'Vladimir Rychkov',\n",
       " 70602: 'Yuki Murakami',\n",
       " 70603: 'Jinsong Tan',\n",
       " 70604: 'Virgílio José Martins Ferreira Filho',\n",
       " 70605: 'R. Su',\n",
       " 70606: 'Francesco Santini',\n",
       " 70607: 'Shaul Markovitch',\n",
       " 70608: 'Xi Zhou',\n",
       " 70609: 'Hans Erik Sørensen',\n",
       " 70610: 'Olivier Barrière',\n",
       " 70611: 'Robin Abraham',\n",
       " 70612: 'Harald Röck',\n",
       " 70613: 'Martin Dick',\n",
       " 70614: 'Sanjay Jinturkar',\n",
       " 70615: 'William Donnelly',\n",
       " 70616: 'Yves Mathieu',\n",
       " 70617: 'JoAnne Holliday',\n",
       " 70618: 'Shmuel Safra',\n",
       " 70619: 'D. Manivannan',\n",
       " 70620: 'Mauricio Soto',\n",
       " 70621: 'Alessandro Giusti',\n",
       " 70622: 'Paul Theo Gonciari',\n",
       " 70623: 'Zhijun He',\n",
       " 70624: 'Nian-Shing Chen',\n",
       " 70625: 'Rong Zheng',\n",
       " 70626: 'William E. Warner',\n",
       " 70627: 'Edward A. Hirsch',\n",
       " 70628: 'Chun-Hsiung Wang',\n",
       " 70629: 'Shin-ichiro Kawano',\n",
       " 70630: 'Michael I. Miller',\n",
       " 70631: 'Sanjeev Arora',\n",
       " 70632: 'Annalu Waller',\n",
       " 70633: 'Yoav Goldberg',\n",
       " 70634: 'Bageshri Karkare',\n",
       " 70635: 'Yunfei Jiang',\n",
       " 70636: 'Qian Chen',\n",
       " 70637: 'Andrew Busch',\n",
       " 70638: 'June-Suh Cho',\n",
       " 70639: 'Ann Blandford',\n",
       " 70640: 'Tim Hendtlass',\n",
       " 70641: 'Bratislav Predic',\n",
       " 70642: 'Slimane Larabi',\n",
       " 70643: 'David M. Nichols',\n",
       " 70644: 'Karen Duca',\n",
       " 70645: 'Nicoló Carissimi',\n",
       " 70646: 'Vania Dimitrova',\n",
       " 70647: 'Shuang-Xian Liu',\n",
       " 70648: 'Liang Gou',\n",
       " 70649: 'Robert C. Moore',\n",
       " 70650: 'Ya-Jeng Lin',\n",
       " 70651: 'Nobel Khandaker',\n",
       " 70652: 'Leif Kornstaedt',\n",
       " 70653: 'Chenguang Zhu',\n",
       " 70654: 'Lingyun Qiu',\n",
       " 70655: 'Benjamin Scholbrock',\n",
       " 70656: 'Kwei-Jay Lin',\n",
       " 70657: 'Michael Fleischman',\n",
       " 70658: 'Michel Diaz',\n",
       " 70659: 'Christoph Donner',\n",
       " 70660: 'Kongwah Wan',\n",
       " 70661: 'Yong Chen',\n",
       " 70662: 'Gene Becker',\n",
       " 70663: 'Zhuoyao Zhang',\n",
       " 70664: 'Joachim Trescher',\n",
       " 70665: 'Emilio Parrado-Hernández',\n",
       " 70666: 'Zhiqiang Hou',\n",
       " 70667: 'Mahmut T. Kandemir',\n",
       " 70668: 'Sabri A. Mahmoud',\n",
       " 70669: 'Bo Hyoung Kim',\n",
       " 70670: 'Chao Zhang',\n",
       " 70671: 'Mehul Bhatt',\n",
       " 70672: 'Solomon Boulos',\n",
       " 70673: 'Jochen Schweflinghaus',\n",
       " 70674: 'V. Tenentes',\n",
       " 70675: 'Matthew J. Rattigan',\n",
       " 70676: 'Wuyi Yue',\n",
       " 70677: 'Liang-Hong Wu',\n",
       " 70678: 'Geoffrey Irving',\n",
       " 70679: 'Eliseo Marzal',\n",
       " 70680: 'Yingjun Zhang',\n",
       " 70681: 'Julie Porteous',\n",
       " 70682: 'Sonia López',\n",
       " 70683: 'Fotios Gioulekas',\n",
       " 70684: 'Phillip Musumeci',\n",
       " 70685: 'Matt Thomas',\n",
       " 70686: 'Kaizhong Zhang',\n",
       " 70687: 'Junseok Park',\n",
       " 70688: 'Vugranam C. Sreedhar',\n",
       " 70689: 'Bernard F. Buxton',\n",
       " 70690: 'Koji Noguchi',\n",
       " 70691: 'André Carlos Ponce de Leon Ferreira de Carvalho',\n",
       " 70692: 'Lijiang Guo',\n",
       " 70693: 'Hong Tian',\n",
       " 70694: 'Peter A. Boncz',\n",
       " 70695: 'Johan Nyström-Persson',\n",
       " 70696: 'Manesh Kokare',\n",
       " 70697: 'Han-I Su',\n",
       " 70698: 'Kurt Vanmechelen',\n",
       " 70699: 'Nigel Davies',\n",
       " 70700: 'Guido Zarrella',\n",
       " 70701: 'Kevin Thomas',\n",
       " 70702: 'Horacio González-Vélez',\n",
       " 70703: 'Gabriele Oligeri',\n",
       " 70704: 'John Corwin',\n",
       " 70705: 'Brian Cripe',\n",
       " 70706: 'Slavisa Garic',\n",
       " 70707: 'Keri Sarver',\n",
       " 70708: 'Yongxing Sun',\n",
       " 70709: 'Suyun Zhao',\n",
       " 70710: 'Lixin Li',\n",
       " 70711: 'Junsong Yuan',\n",
       " 70712: 'Seok Il Song',\n",
       " 70713: 'Ruth Yuee Zhang',\n",
       " 70714: 'Becky Reed Rosenberg',\n",
       " 70715: 'Frank Eliassen',\n",
       " 70716: 'Matthias Bossardt',\n",
       " 70717: 'M. H. Wong',\n",
       " 70718: 'Daniel Massey',\n",
       " 70719: 'Stéphane Lafortune',\n",
       " 70720: 'Won-Kyung Sung',\n",
       " 70721: 'Toby J. Teorey',\n",
       " 70722: 'Alejandro F. González',\n",
       " 70723: 'Liyuan Xing',\n",
       " 70724: 'Iok Ham Lam',\n",
       " 70725: 'Scott Owens',\n",
       " 70726: 'Michael Scheetz',\n",
       " 70727: 'Rajneesh Mahajan',\n",
       " 70728: 'Thorsten Meinl',\n",
       " 70729: 'Debapriyay Mukhopadhyay',\n",
       " 70730: 'Neil Henderson',\n",
       " 70731: 'Hong Guo',\n",
       " 70732: 'Bratin Saha',\n",
       " 70733: 'Jonghwan Lee',\n",
       " 70734: 'Fernanda Araujo Baião',\n",
       " 70735: 'Camélia Constantin',\n",
       " 70736: 'Umeshwar Dayal',\n",
       " 70737: 'Young-Hwan Park',\n",
       " 70738: 'Jianqiang Shen',\n",
       " 70739: 'Gregor Schaffrath',\n",
       " 70740: 'G. Bhatnagar',\n",
       " 70741: 'Jun He',\n",
       " 70742: 'Matthew Thorne',\n",
       " 70743: 'Ciarán Hughes',\n",
       " 70744: 'Kangning Wei',\n",
       " 70745: 'Yuh-Shyan Chen',\n",
       " 70746: 'Arnon Amir',\n",
       " 70747: 'Mirko Zadravec',\n",
       " 70748: 'Maguelonne Teisseire',\n",
       " 70749: 'Alessandro Bahgat Shehata',\n",
       " 70750: 'Xiao Lin',\n",
       " 70751: 'Hyunbo Cho',\n",
       " 70752: 'Anthony Stefanidis',\n",
       " 70753: 'Saad H. Alabbad',\n",
       " 70754: 'Ilker Demirkol',\n",
       " 70755: 'Chun-Hee Lee',\n",
       " 70756: 'Xu Lin',\n",
       " 70757: 'Nick Foster',\n",
       " 70758: 'Andrew M. Pitts',\n",
       " 70759: 'Serban I. Gavrila',\n",
       " 70760: 'Luiz Angelo Steffenel',\n",
       " 70761: 'Magnus Alexandersson',\n",
       " 70762: 'Oliver Bender',\n",
       " 70763: 'Sou King',\n",
       " 70764: 'Pinar Korkmaz',\n",
       " 70765: 'Tom Forsyth',\n",
       " 70766: 'Alexander Rasin',\n",
       " 70767: 'Anil L. Pereira',\n",
       " 70768: 'Shoumeng Yan',\n",
       " 70769: 'Michael D. Williams',\n",
       " 70770: 'Ruth Davis',\n",
       " 70771: 'Robert J. Cimikowski',\n",
       " 70772: 'Irit Goft',\n",
       " 70773: 'Jane Greenberg',\n",
       " 70774: 'Jinzhao Wu',\n",
       " 70775: 'Jack P. C. Kleijnen',\n",
       " 70776: 'Olga Brazhnik',\n",
       " 70777: 'Elizabeth S. Richards',\n",
       " 70778: 'Sara Matzner',\n",
       " 70779: 'Alexander Schliep',\n",
       " 70780: 'Daniel E. Riedel',\n",
       " 70781: 'Michel Parent',\n",
       " 70782: 'Fabien Dagnat',\n",
       " 70783: 'Miquel Pericàs',\n",
       " 70784: 'Loay Abusalah',\n",
       " 70785: 'Michael Grossniklaus',\n",
       " 70786: 'Henrik André-Jönsson',\n",
       " 70787: 'Iván García-Magariño',\n",
       " 70788: 'Ivan Marsá-Maestre',\n",
       " 70789: 'Bert Schiettecatte',\n",
       " 70790: 'Masayoshi Fuse',\n",
       " 70791: 'Mohamed G. Elfeky',\n",
       " 70792: 'Ambuj Tewari',\n",
       " 70793: 'Anne H. Anderson',\n",
       " 70794: 'Yuming Jiang',\n",
       " 70795: 'Domingos Dellamonica Jr.',\n",
       " 70796: 'Francisco Velasco',\n",
       " 70797: 'Andrés Monroy-Hernández',\n",
       " 70798: 'Aloys Mbala',\n",
       " 70799: 'Z. Maria Wang',\n",
       " 70800: 'Harald Schmidl',\n",
       " 70801: 'Lei Ding',\n",
       " 70802: 'Herman Tromp',\n",
       " 70803: 'Aroon Nataraj',\n",
       " 70804: 'Vladimir Rogojin',\n",
       " 70805: 'Yen-Chun Lin',\n",
       " 70806: 'Donghyung Kim',\n",
       " 70807: 'Danil Sokolov',\n",
       " 70808: 'Shuguang Wang',\n",
       " 70809: 'Ken Yocum',\n",
       " 70810: 'Gita Gopal',\n",
       " 70811: 'Samir Aknine',\n",
       " 70812: 'Kadri Hacioglu',\n",
       " 70813: 'Hyunsook Do',\n",
       " 70814: 'Elena Gabriela Barrantes',\n",
       " 70815: 'Silvio Micali',\n",
       " 70816: 'Sandeep Kumar Goel',\n",
       " 70817: 'Marc M. Van Hulle',\n",
       " 70818: 'Kyehyun Cho',\n",
       " 70819: 'Shanchieh Jay Yang',\n",
       " 70820: 'Umur Yilmaz',\n",
       " 70821: 'Chi-Hao Tsai',\n",
       " 70822: 'Hartmut Delong',\n",
       " 70823: 'João Rodrigues',\n",
       " 70824: 'Dimitar Denev',\n",
       " 70825: \"Shin'ichiro Okazaki\",\n",
       " 70826: 'Delphine Longuet',\n",
       " 70827: 'Oana Jurca',\n",
       " 70828: 'Helmut Jelinek',\n",
       " 70829: 'Ming L. Liou',\n",
       " 70830: 'Assim Sagahyroon',\n",
       " 70831: 'Simon Lehmann',\n",
       " 70832: 'Shibin Song',\n",
       " 70833: 'Tejaswini Herath',\n",
       " 70834: 'Kenton T. Unruh',\n",
       " 70835: 'Won Y. Kim',\n",
       " 70836: 'Jeffrey Dean',\n",
       " 70837: 'Naoki Fukuta',\n",
       " 70838: 'Christian Zimmer',\n",
       " 70839: 'S. Saqib Khursheed',\n",
       " 70840: 'Anand Raghunathan',\n",
       " 70841: 'Lluís Màrquez',\n",
       " 70842: 'Shou-Wen Chang',\n",
       " 70843: 'Young Jin Jung',\n",
       " 70844: 'Yiping Hong',\n",
       " 70845: 'Marco Laumanns',\n",
       " 70846: 'Malcolm Ware',\n",
       " 70847: 'David R. Wright',\n",
       " 70848: 'Gyula Simon',\n",
       " 70849: 'Mauro Olivieri',\n",
       " 70850: 'Danièle Beauquier',\n",
       " 70851: 'Mehmet Baysan',\n",
       " 70852: 'Stepán Urban',\n",
       " 70853: 'Tarek A. El-Ghazawi',\n",
       " 70854: 'Oliver Herbort',\n",
       " 70855: 'Evangelos Kotsakis',\n",
       " 70856: 'Chungki Lee',\n",
       " 70857: 'Christos E. Alexakos',\n",
       " 70858: 'Graeme E. Moss',\n",
       " 70859: 'Deborah Nichols',\n",
       " 70860: 'Jeong Seop Sim',\n",
       " 70861: 'Haewoon Kwak',\n",
       " 70862: 'Yi Zhang 0002',\n",
       " 70863: 'Frédéric Béal',\n",
       " 70864: 'Simon Malkowski',\n",
       " 70865: 'Wolfram Wöß',\n",
       " 70866: 'Xingwei Wang',\n",
       " 70867: 'Konstantinos G. Kakoulis',\n",
       " 70868: 'Yizhou Sun',\n",
       " 70869: 'Louis Bavoil',\n",
       " 70870: 'Calvin Kai Fan Tang',\n",
       " 70871: 'Michele Petracca',\n",
       " 70872: 'Damien Pous',\n",
       " 70873: 'Yoshinori Hirano',\n",
       " 70874: 'Sharon Small',\n",
       " 70875: 'Jinpyo Lee',\n",
       " 70876: 'Dmitri Roussinov',\n",
       " 70877: 'Doan B. Hoang',\n",
       " 70878: 'Brian Meeker',\n",
       " 70879: 'Daniel Mlynek',\n",
       " 70880: 'Satish Chand',\n",
       " 70881: 'Francisco B. Rodríguez',\n",
       " 70882: 'J. Alfredo Sánchez',\n",
       " 70883: 'Wan Chen',\n",
       " 70884: 'Paolo Ferraris',\n",
       " 70885: 'Kaustav Das',\n",
       " 70886: 'Yogesh S. Mahajan',\n",
       " 70887: 'Frans C. A. Groen',\n",
       " 70888: 'Aníbal R. Figueiras-Vidal',\n",
       " 70889: 'Vassil Roussev',\n",
       " 70890: 'Wouter Gelade',\n",
       " 70891: 'Mark-A. Krogel',\n",
       " 70892: 'Florence Dupin de Saint-Cyr',\n",
       " 70893: 'Harkirat Singh',\n",
       " 70894: 'Sergey Egorov',\n",
       " 70895: 'Sadia Sharif',\n",
       " 70896: 'Timothy P. McNamara',\n",
       " 70897: 'Uri N. Peled',\n",
       " 70898: 'Jianjiang Feng',\n",
       " 70899: 'Hendrik Lemelson',\n",
       " 70900: 'Zbigniew Palka',\n",
       " 70901: 'Yansheng Lu',\n",
       " 70902: 'Michael Zakharyaschev',\n",
       " 70903: 'Matthias Enzmann',\n",
       " 70904: 'Rodric M. Rabbah',\n",
       " 70905: 'Bernhard Preim',\n",
       " 70906: 'Vidhyacharan Bhaskar',\n",
       " 70907: 'Robert P. Brazile',\n",
       " 70908: 'Xi Wang',\n",
       " 70909: 'Ismael Rodríguez',\n",
       " 70910: 'Evan Barba',\n",
       " 70911: 'Shu Tao',\n",
       " 70912: 'Mihir Shah',\n",
       " 70913: 'George Kollios',\n",
       " 70914: 'Ahmed Amer',\n",
       " 70915: 'Lino A. Costa',\n",
       " 70916: 'Robert Strandh',\n",
       " 70917: 'Ahmet Akkas',\n",
       " 70918: 'Iván Dotú',\n",
       " 70919: 'Steven Noel',\n",
       " 70920: 'Matteo Casadei',\n",
       " 70921: 'Fei Hu',\n",
       " 70922: 'Eli Cortez C. Vilarinho',\n",
       " 70923: 'Casey Dugan',\n",
       " 70924: 'Vivian Cothey',\n",
       " 70925: 'David J. Abraham',\n",
       " 70926: 'Jeffrey Pang',\n",
       " 70927: 'Satoshi Niijima',\n",
       " 70928: 'Abhi Shelat',\n",
       " 70929: 'Kurt C. Wallnau',\n",
       " 70930: 'Johannes Rudolph',\n",
       " 70931: 'Hiroki Matsutani',\n",
       " 70932: 'John Good',\n",
       " 70933: 'Xianlong Hong',\n",
       " 70934: 'Claude Chrisment',\n",
       " 70935: 'Gabriel Antoniu',\n",
       " 70936: 'Emmanuelle Anceaume',\n",
       " 70937: 'Luis Unzueta',\n",
       " 70938: 'Ramesh Hariharan',\n",
       " 70939: 'Dan Page',\n",
       " 70940: 'Nancy R. Mead',\n",
       " 70941: 'Koichi Nakamura',\n",
       " 70942: 'Mika Siikarla',\n",
       " 70943: 'Claire Coverson',\n",
       " 70944: 'Sai Raghuram Durbha',\n",
       " 70945: 'Stefan Neumann',\n",
       " 70946: 'Daniel Dahlmeier',\n",
       " 70947: 'David Zhigang Pan',\n",
       " 70948: 'Johannes Grünbauer',\n",
       " 70949: 'Christopher M. Tongen',\n",
       " 70950: 'Ivan Djordjevic',\n",
       " 70951: 'Despina Polemi',\n",
       " 70952: 'Giuseppe Maruccia',\n",
       " 70953: 'Giovanni Rimassa',\n",
       " 70954: 'Anlei Dong',\n",
       " 70955: 'Wei Wu',\n",
       " 70956: 'Bill Yuan-chi Chiu',\n",
       " 70957: 'José A. Olivas',\n",
       " 70958: 'Hassan Mahmood',\n",
       " 70959: 'Jay Hoeflinger',\n",
       " 70960: 'Joahyoung Lee',\n",
       " 70961: 'Nan Wang 0002',\n",
       " 70962: 'Bengt Mueck',\n",
       " 70963: 'Kara K. W. Poon',\n",
       " 70964: 'Chuan Duan',\n",
       " 70965: 'Earl Oliver',\n",
       " 70966: 'Muthuramakrishnan Venkitasubramaniam',\n",
       " 70967: 'Daniel Ritchie',\n",
       " 70968: 'Thomas Dean',\n",
       " 70969: 'Melvyn Sim',\n",
       " 70970: 'Tido Röder',\n",
       " 70971: 'Ángela Blanco',\n",
       " 70972: 'Eljas Soisalon-Soininen',\n",
       " 70973: 'Changyeol Choi',\n",
       " 70974: 'Mohammad Maifi Hasan Khan',\n",
       " 70975: 'Jérôme Vouillon',\n",
       " 70976: 'Xiangru Chen',\n",
       " 70977: 'Frederick Butler',\n",
       " 70978: 'Jeff Crow',\n",
       " 70979: 'Yen-Wei Chen',\n",
       " 70980: 'Julia Handl',\n",
       " 70981: 'Changdon Kee',\n",
       " 70982: 'Ka-Cheong Leung',\n",
       " 70983: 'Martin Kutrib',\n",
       " 70984: 'J. Scott Miller',\n",
       " 70985: 'Edmund M. Yeh',\n",
       " 70986: 'Ke Liang',\n",
       " 70987: 'Javier de Lope Asiaín',\n",
       " 70988: 'E. Alessio',\n",
       " 70989: 'Bruno Kindarji',\n",
       " 70990: 'David Wiley',\n",
       " 70991: 'Suneuy Kim',\n",
       " 70992: 'Fabio Campi',\n",
       " 70993: 'Pingqiang Zhou',\n",
       " 70994: 'Alexander Vaynberg',\n",
       " 70995: 'John Hallam',\n",
       " 70996: 'Pragash Rajeswaran',\n",
       " 70997: 'Marta Kristín Lárusdóttir',\n",
       " 70998: 'Cheng Zhong',\n",
       " 70999: 'YoungSik Choi',\n",
       " ...}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_2_author_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4ce5b79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74395"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(idx_2_author_name.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "570d70eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '5652',\n",
       " 1: '5693',\n",
       " 2: '6516',\n",
       " 3: '9722',\n",
       " 4: '13378',\n",
       " 5: '17568',\n",
       " 6: '17575',\n",
       " 7: '17759',\n",
       " 8: '19853',\n",
       " 9: '20105',\n",
       " 10: '20181',\n",
       " 11: '20383',\n",
       " 12: '20494',\n",
       " 13: '20534',\n",
       " 14: '20634',\n",
       " 15: '20952',\n",
       " 16: '21087',\n",
       " 17: '21116',\n",
       " 18: '23291',\n",
       " 19: '23292',\n",
       " 20: '23293',\n",
       " 21: '39252',\n",
       " 22: '39260',\n",
       " 23: '39305',\n",
       " 24: '39518',\n",
       " 25: '39789',\n",
       " 26: '39861',\n",
       " 27: '41420',\n",
       " 28: '41428',\n",
       " 29: '41440',\n",
       " 30: '41447',\n",
       " 31: '41473',\n",
       " 32: '41523',\n",
       " 33: '41546',\n",
       " 34: '41588',\n",
       " 35: '41600',\n",
       " 36: '41604',\n",
       " 37: '41671',\n",
       " 38: '41672',\n",
       " 39: '44834',\n",
       " 40: '50066',\n",
       " 41: '53584',\n",
       " 42: '53600',\n",
       " 43: '53610',\n",
       " 44: '53625',\n",
       " 45: '53665',\n",
       " 46: '53701',\n",
       " 47: '53713',\n",
       " 48: '53732',\n",
       " 49: '53755',\n",
       " 50: '53758',\n",
       " 51: '53761',\n",
       " 52: '53791',\n",
       " 53: '53831',\n",
       " 54: '53835',\n",
       " 55: '60916',\n",
       " 56: '60939',\n",
       " 57: '60955',\n",
       " 58: '61021',\n",
       " 59: '61048',\n",
       " 60: '61053',\n",
       " 61: '61078',\n",
       " 62: '61125',\n",
       " 63: '62742',\n",
       " 64: '71315',\n",
       " 65: '71927',\n",
       " 66: '77045',\n",
       " 67: '77100',\n",
       " 68: '77108',\n",
       " 69: '77195',\n",
       " 70: '77277',\n",
       " 71: '77314',\n",
       " 72: '79285',\n",
       " 73: '87013',\n",
       " 74: '87466',\n",
       " 75: '87467',\n",
       " 76: '87575',\n",
       " 77: '87792',\n",
       " 78: '87888',\n",
       " 79: '88032',\n",
       " 80: '88596',\n",
       " 81: '89100',\n",
       " 82: '89206',\n",
       " 83: '89222',\n",
       " 84: '89253',\n",
       " 85: '89344',\n",
       " 86: '89910',\n",
       " 87: '90042',\n",
       " 88: '90055',\n",
       " 89: '90496',\n",
       " 90: '90722',\n",
       " 91: '90881',\n",
       " 92: '95611',\n",
       " 93: '95681',\n",
       " 94: '98796',\n",
       " 95: '104516',\n",
       " 96: '106110',\n",
       " 97: '106253',\n",
       " 98: '106261',\n",
       " 99: '106649',\n",
       " 100: '106755',\n",
       " 101: '106770',\n",
       " 102: '106928',\n",
       " 103: '107377',\n",
       " 104: '107446',\n",
       " 105: '107915',\n",
       " 106: '108044',\n",
       " 107: '108068',\n",
       " 108: '108073',\n",
       " 109: '108312',\n",
       " 110: '108370',\n",
       " 111: '108486',\n",
       " 112: '108658',\n",
       " 113: '108837',\n",
       " 114: '108996',\n",
       " 115: '109270',\n",
       " 116: '111122',\n",
       " 117: '114195',\n",
       " 118: '117315',\n",
       " 119: '117333',\n",
       " 120: '118606',\n",
       " 121: '121624',\n",
       " 122: '121651',\n",
       " 123: '121662',\n",
       " 124: '121687',\n",
       " 125: '121688',\n",
       " 126: '121714',\n",
       " 127: '121716',\n",
       " 128: '121768',\n",
       " 129: '121788',\n",
       " 130: '121795',\n",
       " 131: '121801',\n",
       " 132: '121830',\n",
       " 133: '121844',\n",
       " 134: '121883',\n",
       " 135: '121961',\n",
       " 136: '121970',\n",
       " 137: '121998',\n",
       " 138: '122011',\n",
       " 139: '127387',\n",
       " 140: '127990',\n",
       " 141: '128021',\n",
       " 142: '128084',\n",
       " 143: '129230',\n",
       " 144: '131206',\n",
       " 145: '131485',\n",
       " 146: '131585',\n",
       " 147: '131660',\n",
       " 148: '131669',\n",
       " 149: '131858',\n",
       " 150: '131993',\n",
       " 151: '132313',\n",
       " 152: '132366',\n",
       " 153: '132513',\n",
       " 154: '132700',\n",
       " 155: '132714',\n",
       " 156: '132842',\n",
       " 157: '132912',\n",
       " 158: '133226',\n",
       " 159: '133895',\n",
       " 160: '133913',\n",
       " 161: '134154',\n",
       " 162: '134384',\n",
       " 163: '134447',\n",
       " 164: '136830',\n",
       " 165: '137189',\n",
       " 166: '137222',\n",
       " 167: '137306',\n",
       " 168: '137891',\n",
       " 169: '137936',\n",
       " 170: '138361',\n",
       " 171: '138427',\n",
       " 172: '138504',\n",
       " 173: '140083',\n",
       " 174: '140087',\n",
       " 175: '140096',\n",
       " 176: '140099',\n",
       " 177: '140100',\n",
       " 178: '141738',\n",
       " 179: '142157',\n",
       " 180: '142193',\n",
       " 181: '142245',\n",
       " 182: '142380',\n",
       " 183: '142421',\n",
       " 184: '142787',\n",
       " 185: '143206',\n",
       " 186: '155379',\n",
       " 187: '155407',\n",
       " 188: '155443',\n",
       " 189: '156656',\n",
       " 190: '157345',\n",
       " 191: '157404',\n",
       " 192: '157448',\n",
       " 193: '157538',\n",
       " 194: '157550',\n",
       " 195: '161475',\n",
       " 196: '161583',\n",
       " 197: '162794',\n",
       " 198: '162917',\n",
       " 199: '162923',\n",
       " 200: '162927',\n",
       " 201: '162928',\n",
       " 202: '165005',\n",
       " 203: '169349',\n",
       " 204: '174211',\n",
       " 205: '174212',\n",
       " 206: '174213',\n",
       " 207: '174215',\n",
       " 208: '174223',\n",
       " 209: '174224',\n",
       " 210: '174226',\n",
       " 211: '178493',\n",
       " 212: '178497',\n",
       " 213: '178502',\n",
       " 214: '178507',\n",
       " 215: '178515',\n",
       " 216: '178518',\n",
       " 217: '183235',\n",
       " 218: '183410',\n",
       " 219: '184019',\n",
       " 220: '186200',\n",
       " 221: '189011',\n",
       " 222: '189755',\n",
       " 223: '189756',\n",
       " 224: '189757',\n",
       " 225: '189758',\n",
       " 226: '189760',\n",
       " 227: '189761',\n",
       " 228: '189762',\n",
       " 229: '190375',\n",
       " 230: '192663',\n",
       " 231: '192763',\n",
       " 232: '192770',\n",
       " 233: '192784',\n",
       " 234: '192876',\n",
       " 235: '192877',\n",
       " 236: '192896',\n",
       " 237: '192928',\n",
       " 238: '192959',\n",
       " 239: '193453',\n",
       " 240: '200309',\n",
       " 241: '206660',\n",
       " 242: '209153',\n",
       " 243: '209712',\n",
       " 244: '209713',\n",
       " 245: '209718',\n",
       " 246: '209734',\n",
       " 247: '210316',\n",
       " 248: '210802',\n",
       " 249: '210993',\n",
       " 250: '211933',\n",
       " 251: '213685',\n",
       " 252: '214463',\n",
       " 253: '219382',\n",
       " 254: '219516',\n",
       " 255: '224896',\n",
       " 256: '232784',\n",
       " 257: '232911',\n",
       " 258: '232963',\n",
       " 259: '232993',\n",
       " 260: '244003',\n",
       " 261: '248151',\n",
       " 262: '257306',\n",
       " 263: '267907',\n",
       " 264: '268219',\n",
       " 265: '268329',\n",
       " 266: '268595',\n",
       " 267: '268807',\n",
       " 268: '281823',\n",
       " 269: '281843',\n",
       " 270: '281845',\n",
       " 271: '282004',\n",
       " 272: '282063',\n",
       " 273: '282196',\n",
       " 274: '282261',\n",
       " 275: '282279',\n",
       " 276: '282282',\n",
       " 277: '282365',\n",
       " 278: '282399',\n",
       " 279: '282470',\n",
       " 280: '282563',\n",
       " 281: '282718',\n",
       " 282: '282958',\n",
       " 283: '283307',\n",
       " 284: '283371',\n",
       " 285: '283513',\n",
       " 286: '286985',\n",
       " 287: '299550',\n",
       " 288: '299751',\n",
       " 289: '299942',\n",
       " 290: '309866',\n",
       " 291: '309869',\n",
       " 292: '309879',\n",
       " 293: '309907',\n",
       " 294: '309914',\n",
       " 295: '309927',\n",
       " 296: '309940',\n",
       " 297: '309953',\n",
       " 298: '309959',\n",
       " 299: '309979',\n",
       " 300: '309984',\n",
       " 301: '310085',\n",
       " 302: '310106',\n",
       " 303: '310112',\n",
       " 304: '310139',\n",
       " 305: '310157',\n",
       " 306: '310161',\n",
       " 307: '334466',\n",
       " 308: '334806',\n",
       " 309: '343233',\n",
       " 310: '344781',\n",
       " 311: '347842',\n",
       " 312: '348182',\n",
       " 313: '348492',\n",
       " 314: '359941',\n",
       " 315: '360000',\n",
       " 316: '360007',\n",
       " 317: '360112',\n",
       " 318: '360175',\n",
       " 319: '360183',\n",
       " 320: '360235',\n",
       " 321: '360268',\n",
       " 322: '360426',\n",
       " 323: '360492',\n",
       " 324: '360513',\n",
       " 325: '360562',\n",
       " 326: '360631',\n",
       " 327: '360641',\n",
       " 328: '360657',\n",
       " 329: '360787',\n",
       " 330: '360807',\n",
       " 331: '360816',\n",
       " 332: '361293',\n",
       " 333: '361309',\n",
       " 334: '361323',\n",
       " 335: '361451',\n",
       " 336: '361513',\n",
       " 337: '361545',\n",
       " 338: '361674',\n",
       " 339: '361726',\n",
       " 340: '361799',\n",
       " 341: '361875',\n",
       " 342: '361927',\n",
       " 343: '362002',\n",
       " 344: '362158',\n",
       " 345: '362349',\n",
       " 346: '362376',\n",
       " 347: '362460',\n",
       " 348: '362500',\n",
       " 349: '362527',\n",
       " 350: '362558',\n",
       " 351: '362651',\n",
       " 352: '362671',\n",
       " 353: '362894',\n",
       " 354: '362899',\n",
       " 355: '362956',\n",
       " 356: '363077',\n",
       " 357: '363379',\n",
       " 358: '366072',\n",
       " 359: '384899',\n",
       " 360: '400005',\n",
       " 361: '401427',\n",
       " 362: '402924',\n",
       " 363: '404689',\n",
       " 364: '404692',\n",
       " 365: '404700',\n",
       " 366: '404769',\n",
       " 367: '404830',\n",
       " 368: '404871',\n",
       " 369: '411310',\n",
       " 370: '415836',\n",
       " 371: '418427',\n",
       " 372: '418438',\n",
       " 373: '418454',\n",
       " 374: '418553',\n",
       " 375: '418565',\n",
       " 376: '418592',\n",
       " 377: '418594',\n",
       " 378: '418861',\n",
       " 379: '418956',\n",
       " 380: '419117',\n",
       " 381: '419124',\n",
       " 382: '419155',\n",
       " 383: '419182',\n",
       " 384: '419328',\n",
       " 385: '419334',\n",
       " 386: '419363',\n",
       " 387: '419402',\n",
       " 388: '419497',\n",
       " 389: '419587',\n",
       " 390: '419641',\n",
       " 391: '419666',\n",
       " 392: '436578',\n",
       " 393: '436636',\n",
       " 394: '436890',\n",
       " 395: '437038',\n",
       " 396: '446369',\n",
       " 397: '446455',\n",
       " 398: '446509',\n",
       " 399: '446510',\n",
       " 400: '446527',\n",
       " 401: '446570',\n",
       " 402: '447834',\n",
       " 403: '448981',\n",
       " 404: '449140',\n",
       " 405: '449206',\n",
       " 406: '449469',\n",
       " 407: '449504',\n",
       " 408: '450488',\n",
       " 409: '450523',\n",
       " 410: '450534',\n",
       " 411: '450562',\n",
       " 412: '450585',\n",
       " 413: '450749',\n",
       " 414: '450805',\n",
       " 415: '450821',\n",
       " 416: '450830',\n",
       " 417: '450854',\n",
       " 418: '450876',\n",
       " 419: '450911',\n",
       " 420: '450927',\n",
       " 421: '450949',\n",
       " 422: '450957',\n",
       " 423: '450985',\n",
       " 424: '453228',\n",
       " 425: '459226',\n",
       " 426: '459276',\n",
       " 427: '459290',\n",
       " 428: '459324',\n",
       " 429: '459358',\n",
       " 430: '459554',\n",
       " 431: '459599',\n",
       " 432: '473487',\n",
       " 433: '479637',\n",
       " 434: '481220',\n",
       " 435: '481282',\n",
       " 436: '481302',\n",
       " 437: '484410',\n",
       " 438: '484525',\n",
       " 439: '484588',\n",
       " 440: '484632',\n",
       " 441: '484735',\n",
       " 442: '484829',\n",
       " 443: '485042',\n",
       " 444: '485266',\n",
       " 445: '485492',\n",
       " 446: '487157',\n",
       " 447: '487226',\n",
       " 448: '493411',\n",
       " 449: '493763',\n",
       " 450: '494427',\n",
       " 451: '494534',\n",
       " 452: '499852',\n",
       " 453: '501955',\n",
       " 454: '501976',\n",
       " 455: '502256',\n",
       " 456: '502465',\n",
       " 457: '502551',\n",
       " 458: '502784',\n",
       " 459: '502888',\n",
       " 460: '502963',\n",
       " 461: '503106',\n",
       " 462: '503163',\n",
       " 463: '503229',\n",
       " 464: '505647',\n",
       " 465: '505650',\n",
       " 466: '505659',\n",
       " 467: '505666',\n",
       " 468: '505692',\n",
       " 469: '505706',\n",
       " 470: '505711',\n",
       " 471: '505726',\n",
       " 472: '505757',\n",
       " 473: '505783',\n",
       " 474: '505788',\n",
       " 475: '505805',\n",
       " 476: '505807',\n",
       " 477: '507635',\n",
       " 478: '507637',\n",
       " 479: '507638',\n",
       " 480: '507640',\n",
       " 481: '507642',\n",
       " 482: '507643',\n",
       " 483: '507644',\n",
       " 484: '507645',\n",
       " 485: '510029',\n",
       " 486: '514152',\n",
       " 487: '515508',\n",
       " 488: '519718',\n",
       " 489: '521760',\n",
       " 490: '521790',\n",
       " 491: '521929',\n",
       " 492: '521998',\n",
       " 493: '522052',\n",
       " 494: '522188',\n",
       " 495: '522274',\n",
       " 496: '522314',\n",
       " 497: '522467',\n",
       " 498: '522582',\n",
       " 499: '522600',\n",
       " 500: '522657',\n",
       " 501: '522695',\n",
       " 502: '522731',\n",
       " 503: '522806',\n",
       " 504: '522836',\n",
       " 505: '522871',\n",
       " 506: '522951',\n",
       " 507: '523456',\n",
       " 508: '523688',\n",
       " 509: '523690',\n",
       " 510: '523691',\n",
       " 511: '523693',\n",
       " 512: '523695',\n",
       " 513: '523696',\n",
       " 514: '523704',\n",
       " 515: '523708',\n",
       " 516: '523709',\n",
       " 517: '523721',\n",
       " 518: '523725',\n",
       " 519: '523735',\n",
       " 520: '523756',\n",
       " 521: '523761',\n",
       " 522: '523788',\n",
       " 523: '523795',\n",
       " 524: '523826',\n",
       " 525: '523830',\n",
       " 526: '526489',\n",
       " 527: '526532',\n",
       " 528: '526597',\n",
       " 529: '538094',\n",
       " 530: '538124',\n",
       " 531: '538150',\n",
       " 532: '538161',\n",
       " 533: '538186',\n",
       " 534: '538219',\n",
       " 535: '538238',\n",
       " 536: '538281',\n",
       " 537: '538297',\n",
       " 538: '538354',\n",
       " 539: '542302',\n",
       " 540: '542307',\n",
       " 541: '542319',\n",
       " 542: '542416',\n",
       " 543: '542445',\n",
       " 544: '542462',\n",
       " 545: '542471',\n",
       " 546: '542511',\n",
       " 547: '542512',\n",
       " 548: '542537',\n",
       " 549: '542548',\n",
       " 550: '542577',\n",
       " 551: '542725',\n",
       " 552: '542752',\n",
       " 553: '542803',\n",
       " 554: '542818',\n",
       " 555: '542819',\n",
       " 556: '542854',\n",
       " 557: '542863',\n",
       " 558: '542910',\n",
       " 559: '542922',\n",
       " 560: '542926',\n",
       " 561: '542930',\n",
       " 562: '543546',\n",
       " 563: '543572',\n",
       " 564: '543684',\n",
       " 565: '543746',\n",
       " 566: '543791',\n",
       " 567: '543832',\n",
       " 568: '543904',\n",
       " 569: '543966',\n",
       " 570: '544014',\n",
       " 571: '544322',\n",
       " 572: '544410',\n",
       " 573: '544730',\n",
       " 574: '544781',\n",
       " 575: '544822',\n",
       " 576: '544858',\n",
       " 577: '544910',\n",
       " 578: '544966',\n",
       " 579: '545011',\n",
       " 580: '545037',\n",
       " 581: '545098',\n",
       " 582: '545214',\n",
       " 583: '545246',\n",
       " 584: '545247',\n",
       " 585: '545256',\n",
       " 586: '545257',\n",
       " 587: '545274',\n",
       " 588: '545375',\n",
       " 589: '545444',\n",
       " 590: '545499',\n",
       " 591: '545879',\n",
       " 592: '545895',\n",
       " 593: '545908',\n",
       " 594: '545946',\n",
       " 595: '545971',\n",
       " 596: '545989',\n",
       " 597: '546018',\n",
       " 598: '546058',\n",
       " 599: '546065',\n",
       " 600: '546096',\n",
       " 601: '546128',\n",
       " 602: '546147',\n",
       " 603: '546160',\n",
       " 604: '546253',\n",
       " 605: '546303',\n",
       " 606: '546442',\n",
       " 607: '546457',\n",
       " 608: '546465',\n",
       " 609: '546567',\n",
       " 610: '546704',\n",
       " 611: '546775',\n",
       " 612: '546800',\n",
       " 613: '546816',\n",
       " 614: '548036',\n",
       " 615: '548124',\n",
       " 616: '551414',\n",
       " 617: '551872',\n",
       " 618: '557929',\n",
       " 619: '557939',\n",
       " 620: '557988',\n",
       " 621: '557994',\n",
       " 622: '558002',\n",
       " 623: '563929',\n",
       " 624: '566475',\n",
       " 625: '567752',\n",
       " 626: '570349',\n",
       " 627: '571088',\n",
       " 628: '571090',\n",
       " 629: '574671',\n",
       " 630: '574772',\n",
       " 631: '574774',\n",
       " 632: '574814',\n",
       " 633: '574830',\n",
       " 634: '575064',\n",
       " 635: '575294',\n",
       " 636: '575295',\n",
       " 637: '575382',\n",
       " 638: '575427',\n",
       " 639: '575520',\n",
       " 640: '575554',\n",
       " 641: '575614',\n",
       " 642: '580431',\n",
       " 643: '588498',\n",
       " 644: '588790',\n",
       " 645: '588810',\n",
       " 646: '588811',\n",
       " 647: '588915',\n",
       " 648: '589174',\n",
       " 649: '589805',\n",
       " 650: '590085',\n",
       " 651: '592807',\n",
       " 652: '592903',\n",
       " 653: '593027',\n",
       " 654: '593039',\n",
       " 655: '593058',\n",
       " 656: '593087',\n",
       " 657: '593119',\n",
       " 658: '593179',\n",
       " 659: '593180',\n",
       " 660: '593223',\n",
       " 661: '593243',\n",
       " 662: '593248',\n",
       " 663: '593270',\n",
       " 664: '593321',\n",
       " 665: '593344',\n",
       " 666: '593404',\n",
       " 667: '593438',\n",
       " 668: '593444',\n",
       " 669: '593482',\n",
       " 670: '593516',\n",
       " 671: '593527',\n",
       " 672: '593536',\n",
       " 673: '593540',\n",
       " 674: '593571',\n",
       " 675: '593585',\n",
       " 676: '593586',\n",
       " 677: '593616',\n",
       " 678: '593649',\n",
       " 679: '593680',\n",
       " 680: '593716',\n",
       " 681: '593824',\n",
       " 682: '593826',\n",
       " 683: '593852',\n",
       " 684: '593856',\n",
       " 685: '593858',\n",
       " 686: '593876',\n",
       " 687: '593880',\n",
       " 688: '593884',\n",
       " 689: '593889',\n",
       " 690: '593902',\n",
       " 691: '593914',\n",
       " 692: '593921',\n",
       " 693: '593931',\n",
       " 694: '594123',\n",
       " 695: '594162',\n",
       " 696: '594174',\n",
       " 697: '594379',\n",
       " 698: '594544',\n",
       " 699: '594792',\n",
       " 700: '594827',\n",
       " 701: '594968',\n",
       " 702: '594973',\n",
       " 703: '595201',\n",
       " 704: '595385',\n",
       " 705: '595509',\n",
       " 706: '595557',\n",
       " 707: '595559',\n",
       " 708: '595718',\n",
       " 709: '595733',\n",
       " 710: '595764',\n",
       " 711: '595767',\n",
       " 712: '595800',\n",
       " 713: '596023',\n",
       " 714: '596839',\n",
       " 715: '596882',\n",
       " 716: '596987',\n",
       " 717: '597057',\n",
       " 718: '597072',\n",
       " 719: '597089',\n",
       " 720: '597230',\n",
       " 721: '597242',\n",
       " 722: '597255',\n",
       " 723: '597380',\n",
       " 724: '597430',\n",
       " 725: '597499',\n",
       " 726: '597545',\n",
       " 727: '597947',\n",
       " 728: '597964',\n",
       " 729: '597978',\n",
       " 730: '598196',\n",
       " 731: '598349',\n",
       " 732: '598373',\n",
       " 733: '598387',\n",
       " 734: '598442',\n",
       " 735: '598608',\n",
       " 736: '598611',\n",
       " 737: '598657',\n",
       " 738: '598697',\n",
       " 739: '598763',\n",
       " 740: '598819',\n",
       " 741: '598850',\n",
       " 742: '598941',\n",
       " 743: '599060',\n",
       " 744: '599108',\n",
       " 745: '599114',\n",
       " 746: '599145',\n",
       " 747: '599168',\n",
       " 748: '599199',\n",
       " 749: '599235',\n",
       " 750: '599365',\n",
       " 751: '599434',\n",
       " 752: '599453',\n",
       " 753: '599472',\n",
       " 754: '599495',\n",
       " 755: '599552',\n",
       " 756: '599562',\n",
       " 757: '599584',\n",
       " 758: '599606',\n",
       " 759: '599623',\n",
       " 760: '599668',\n",
       " 761: '599895',\n",
       " 762: '599994',\n",
       " 763: '600314',\n",
       " 764: '600401',\n",
       " 765: '600478',\n",
       " 766: '600500',\n",
       " 767: '600506',\n",
       " 768: '600516',\n",
       " 769: '600528',\n",
       " 770: '600530',\n",
       " 771: '600652',\n",
       " 772: '601090',\n",
       " 773: '601116',\n",
       " 774: '601125',\n",
       " 775: '601148',\n",
       " 776: '601169',\n",
       " 777: '601177',\n",
       " 778: '601219',\n",
       " 779: '601233',\n",
       " 780: '601246',\n",
       " 781: '601300',\n",
       " 782: '601320',\n",
       " 783: '601336',\n",
       " 784: '604530',\n",
       " 785: '604577',\n",
       " 786: '604639',\n",
       " 787: '608621',\n",
       " 788: '609780',\n",
       " 789: '609817',\n",
       " 790: '609819',\n",
       " 791: '609849',\n",
       " 792: '609890',\n",
       " 793: '610140',\n",
       " 794: '610625',\n",
       " 795: '613968',\n",
       " 796: '613995',\n",
       " 797: '614060',\n",
       " 798: '614062',\n",
       " 799: '614118',\n",
       " 800: '614224',\n",
       " 801: '614438',\n",
       " 802: '614459',\n",
       " 803: '614460',\n",
       " 804: '614549',\n",
       " 805: '619820',\n",
       " 806: '619835',\n",
       " 807: '619919',\n",
       " 808: '620063',\n",
       " 809: '620503',\n",
       " 810: '620506',\n",
       " 811: '620581',\n",
       " 812: '621229',\n",
       " 813: '621363',\n",
       " 814: '621469',\n",
       " 815: '621517',\n",
       " 816: '621603',\n",
       " 817: '621749',\n",
       " 818: '621797',\n",
       " 819: '621811',\n",
       " 820: '621980',\n",
       " 821: '622541',\n",
       " 822: '622681',\n",
       " 823: '624028',\n",
       " 824: '624084',\n",
       " 825: '624099',\n",
       " 826: '624223',\n",
       " 827: '624325',\n",
       " 828: '627770',\n",
       " 829: '627781',\n",
       " 830: '627808',\n",
       " 831: '628117',\n",
       " 832: '634179',\n",
       " 833: '634259',\n",
       " 834: '635146',\n",
       " 835: '635270',\n",
       " 836: '635865',\n",
       " 837: '635888',\n",
       " 838: '635913',\n",
       " 839: '636273',\n",
       " 840: '636338',\n",
       " 841: '636368',\n",
       " 842: '636412',\n",
       " 843: '636465',\n",
       " 844: '639308',\n",
       " 845: '639427',\n",
       " 846: '639539',\n",
       " 847: '639546',\n",
       " 848: '639557',\n",
       " 849: '639572',\n",
       " 850: '639646',\n",
       " 851: '639660',\n",
       " 852: '639693',\n",
       " 853: '639710',\n",
       " 854: '639726',\n",
       " 855: '639831',\n",
       " 856: '639853',\n",
       " 857: '639890',\n",
       " 858: '640215',\n",
       " 859: '640254',\n",
       " 860: '640414',\n",
       " 861: '640422',\n",
       " 862: '640451',\n",
       " 863: '640491',\n",
       " 864: '640506',\n",
       " 865: '642218',\n",
       " 866: '643721',\n",
       " 867: '647880',\n",
       " 868: '648252',\n",
       " 869: '648266',\n",
       " 870: '648308',\n",
       " 871: '648357',\n",
       " 872: '651726',\n",
       " 873: '651729',\n",
       " 874: '653564',\n",
       " 875: '653838',\n",
       " 876: '653966',\n",
       " 877: '660631',\n",
       " 878: '660783',\n",
       " 879: '665094',\n",
       " 880: '665116',\n",
       " 881: '666767',\n",
       " 882: '666823',\n",
       " 883: '667720',\n",
       " 884: '668151',\n",
       " 885: '668192',\n",
       " 886: '668985',\n",
       " 887: '669231',\n",
       " 888: '669424',\n",
       " 889: '670426',\n",
       " 890: '670567',\n",
       " 891: '670594',\n",
       " 892: '670997',\n",
       " 893: '681920',\n",
       " 894: '717612',\n",
       " 895: '738653',\n",
       " 896: '738663',\n",
       " 897: '738667',\n",
       " 898: '738683',\n",
       " 899: '738693',\n",
       " 900: '738704',\n",
       " 901: '738707',\n",
       " 902: '738709',\n",
       " 903: '746454',\n",
       " 904: '746556',\n",
       " 905: '748574',\n",
       " 906: '748734',\n",
       " 907: '751345',\n",
       " 908: '751372',\n",
       " 909: '751386',\n",
       " 910: '751503',\n",
       " 911: '751547',\n",
       " 912: '751699',\n",
       " 913: '751713',\n",
       " 914: '751755',\n",
       " 915: '753215',\n",
       " 916: '753228',\n",
       " 917: '753237',\n",
       " 918: '753245',\n",
       " 919: '753351',\n",
       " 920: '753360',\n",
       " 921: '753376',\n",
       " 922: '753381',\n",
       " 923: '753385',\n",
       " 924: '754747',\n",
       " 925: '754840',\n",
       " 926: '754876',\n",
       " 927: '754907',\n",
       " 928: '754916',\n",
       " 929: '757708',\n",
       " 930: '757977',\n",
       " 931: '757998',\n",
       " 932: '760833',\n",
       " 933: '760867',\n",
       " 934: '760947',\n",
       " 935: '760981',\n",
       " 936: '760987',\n",
       " 937: '760988',\n",
       " 938: '761041',\n",
       " 939: '786536',\n",
       " 940: '786750',\n",
       " 941: '786807',\n",
       " 942: '786868',\n",
       " 943: '792807',\n",
       " 944: '792810',\n",
       " 945: '792900',\n",
       " 946: '792919',\n",
       " 947: '792941',\n",
       " 948: '797398',\n",
       " 949: '797434',\n",
       " 950: '797488',\n",
       " 951: '797535',\n",
       " 952: '797548',\n",
       " 953: '797624',\n",
       " 954: '805703',\n",
       " 955: '805828',\n",
       " 956: '805863',\n",
       " 957: '805889',\n",
       " 958: '805914',\n",
       " 959: '805918',\n",
       " 960: '808239',\n",
       " 961: '808634',\n",
       " 962: '808919',\n",
       " 963: '811918',\n",
       " 964: '811930',\n",
       " 965: '815852',\n",
       " 966: '815889',\n",
       " 967: '815898',\n",
       " 968: '815910',\n",
       " 969: '816290',\n",
       " 970: '816728',\n",
       " 971: '816782',\n",
       " 972: '816798',\n",
       " 973: '816809',\n",
       " 974: '816811',\n",
       " 975: '816829',\n",
       " 976: '817134',\n",
       " 977: '817433',\n",
       " 978: '817565',\n",
       " 979: '817571',\n",
       " 980: '817899',\n",
       " 981: '817903',\n",
       " 982: '818284',\n",
       " 983: '818289',\n",
       " 984: '818375',\n",
       " 985: '818379',\n",
       " 986: '818393',\n",
       " 987: '818405',\n",
       " 988: '818408',\n",
       " 989: '818413',\n",
       " 990: '818984',\n",
       " 991: '819047',\n",
       " 992: '819078',\n",
       " 993: '819572',\n",
       " 994: '830867',\n",
       " 995: '831541',\n",
       " 996: '831595',\n",
       " 997: '831611',\n",
       " 998: '831651',\n",
       " 999: '831736',\n",
       " ...}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_2_paper_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5cdb7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_author_idx = {}\n",
    "for k,v in paper_author.items():\n",
    "    paper_author_idx[paper_index_2_idx[k]] = {'author': [author_name_2_idx[item] for item in v['author']], 'year': v['year']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d8fecc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23333: {'author': [82210, 126930], 'year': '2003'},\n",
       " 54982: {'author': [88346, 123957], 'year': '2003'},\n",
       " 51116: {'author': [103107, 123957, 123010], 'year': '2003'},\n",
       " 26669: {'author': [91157], 'year': '2003'},\n",
       " 2882: {'author': [135240, 112147, 98020], 'year': '2003'},\n",
       " 26654: {'author': [123348, 82552, 123995], 'year': '2003'},\n",
       " 23472: {'author': [77339, 127058, 139042], 'year': '2003'},\n",
       " 3309: {'author': [119626, 118034, 104362, 80829], 'year': '2003'},\n",
       " 31232: {'author': [87459, 73641, 122377, 101879, 82983], 'year': '2003'},\n",
       " 42141: {'author': [82210, 126930], 'year': '2002'},\n",
       " 50805: {'author': [106374, 94159], 'year': '2005'},\n",
       " 50136: {'author': [91947], 'year': '2005'},\n",
       " 11522: {'author': [84390, 80487, 140799, 122593], 'year': '2003'},\n",
       " 35816: {'author': [118425, 125463, 114507], 'year': '2003'},\n",
       " 62122: {'author': [116840, 132181], 'year': '2003'},\n",
       " 55456: {'author': [85925, 78041], 'year': '2003'},\n",
       " 24617: {'author': [113519], 'year': '2003'},\n",
       " 9508: {'author': [113519, 81415], 'year': '2003'},\n",
       " 55401: {'author': [81415], 'year': '2003'},\n",
       " 46181: {'author': [81415], 'year': '2003'},\n",
       " 55864: {'author': [127315, 93657, 122249], 'year': '2003'},\n",
       " 44204: {'author': [129172], 'year': '2003'},\n",
       " 5501: {'author': [91947], 'year': '2003'},\n",
       " 21210: {'author': [74120], 'year': '2003'},\n",
       " 5241: {'author': [123957, 131423, 123010], 'year': '2002'},\n",
       " 38398: {'author': [105632, 138134], 'year': '2002'},\n",
       " 1844: {'author': [91228], 'year': '2002'},\n",
       " 28775: {'author': [137961], 'year': '2002'},\n",
       " 51257: {'author': [104975, 109239, 130881], 'year': '2002'},\n",
       " 17014: {'author': [114147, 123010], 'year': '2002'},\n",
       " 32680: {'author': [80849, 100232], 'year': '2002'},\n",
       " 42003: {'author': [137193, 118873], 'year': '2002'},\n",
       " 23283: {'author': [82113], 'year': '2000'},\n",
       " 37803: {'author': [119610], 'year': '2001'},\n",
       " 42597: {'author': [112663], 'year': '2002'},\n",
       " 10659: {'author': [113927], 'year': '2001'},\n",
       " 14407: {'author': [81349], 'year': '2000'},\n",
       " 6650: {'author': [91815], 'year': '2002'},\n",
       " 9980: {'author': [113270], 'year': '2002'},\n",
       " 43888: {'author': [74351, 73559], 'year': '2003'},\n",
       " 25537: {'author': [72748, 85968, 109945], 'year': '2002'},\n",
       " 28972: {'author': [133933, 144394], 'year': '2004'},\n",
       " 41996: {'author': [70423, 119933, 78801], 'year': '2002'},\n",
       " 38192: {'author': [125806, 84680, 138888, 116945, 80729, 71891],\n",
       "  'year': '2004'},\n",
       " 33553: {'author': [77387, 100793, 131862, 125368], 'year': '2007'},\n",
       " 22883: {'author': [85708, 135421], 'year': '2004'},\n",
       " 61701: {'author': [100081, 90684, 108400], 'year': '2004'},\n",
       " 15888: {'author': [94123, 73628], 'year': '2004'},\n",
       " 5253: {'author': [100699, 119316, 73158], 'year': '2007'},\n",
       " 34903: {'author': [93676, 107490, 112499], 'year': '2007'},\n",
       " 65823: {'author': [78747, 99606, 132138, 128517, 132356], 'year': '2007'},\n",
       " 8444: {'author': [140877, 88126, 107579, 132560, 125451], 'year': '2007'},\n",
       " 46936: {'author': [132774, 125393], 'year': '2007'},\n",
       " 39054: {'author': [122152, 116351, 130977], 'year': '2006'},\n",
       " 12176: {'author': [90776, 101971, 99813], 'year': '2006'},\n",
       " 39612: {'author': [91795, 142432, 82554], 'year': '2006'},\n",
       " 16017: {'author': [104288, 75873, 96527], 'year': '2006'},\n",
       " 3103: {'author': [81222, 93927, 113872], 'year': '2005'},\n",
       " 31288: {'author': [74571, 131240, 91386, 132861, 105444, 100695],\n",
       "  'year': '2006'},\n",
       " 24447: {'author': [124027, 74350], 'year': '2005'},\n",
       " 16008: {'author': [104083, 116351, 75986, 71857, 108771, 123277],\n",
       "  'year': '2008'},\n",
       " 34027: {'author': [123840, 105219, 136406], 'year': '2005'},\n",
       " 49124: {'author': [80701, 73280, 102341], 'year': '2008'},\n",
       " 6469: {'author': [138974, 142567], 'year': '2005'},\n",
       " 3427: {'author': [117904], 'year': '2005'},\n",
       " 19989: {'author': [88211, 117441, 86764, 82554], 'year': '2005'},\n",
       " 62421: {'author': [120099,\n",
       "   99838,\n",
       "   80271,\n",
       "   90734,\n",
       "   133913,\n",
       "   95384,\n",
       "   100263,\n",
       "   112442,\n",
       "   140494,\n",
       "   105640],\n",
       "  'year': '2005'},\n",
       " 30606: {'author': [96341, 118778, 117098, 129955, 119744, 131928],\n",
       "  'year': '2005'},\n",
       " 20657: {'author': [95918,\n",
       "   131928,\n",
       "   86883,\n",
       "   80368,\n",
       "   135684,\n",
       "   118891,\n",
       "   106453,\n",
       "   70910,\n",
       "   130766,\n",
       "   119744,\n",
       "   129955],\n",
       "  'year': '2008'},\n",
       " 66084: {'author': [93792, 141791], 'year': '2008'},\n",
       " 37533: {'author': [85328, 99242, 99665], 'year': '2004'},\n",
       " 47535: {'author': [96341, 135668, 93265, 73893], 'year': '2006'},\n",
       " 43393: {'author': [74714, 86974], 'year': '2008'},\n",
       " 7661: {'author': [101046, 121061, 102846, 86546, 84807, 116690, 94860],\n",
       "  'year': '2006'},\n",
       " 24197: {'author': [111798, 86974], 'year': '2002'},\n",
       " 55237: {'author': [126304], 'year': '2006'},\n",
       " 55484: {'author': [114831, 103636], 'year': '2004'},\n",
       " 5562: {'author': [108040, 123480], 'year': '2004'},\n",
       " 56508: {'author': [88003], 'year': '2000'},\n",
       " 41218: {'author': [84336, 94708, 134632], 'year': '2004'},\n",
       " 59125: {'author': [132156,\n",
       "   129575,\n",
       "   111317,\n",
       "   140068,\n",
       "   124497,\n",
       "   109395,\n",
       "   104493,\n",
       "   126026],\n",
       "  'year': '2002'},\n",
       " 5823: {'author': [90064, 84385], 'year': '2004'},\n",
       " 66085: {'author': [126680,\n",
       "   79046,\n",
       "   73512,\n",
       "   88242,\n",
       "   124458,\n",
       "   126758,\n",
       "   140823,\n",
       "   124066],\n",
       "  'year': '2006'},\n",
       " 7695: {'author': [81216, 112387], 'year': '2006'},\n",
       " 11325: {'author': [144161, 87240], 'year': '2006'},\n",
       " 23403: {'author': [123756, 85502, 92420, 141777, 123130, 117397, 117911],\n",
       "  'year': '2006'},\n",
       " 52333: {'author': [137706, 142858, 122109, 132810, 93162], 'year': '2004'},\n",
       " 62825: {'author': [91567, 121752, 121725], 'year': '2006'},\n",
       " 10452: {'author': [76950, 93922, 108534, 120517, 96447], 'year': '2006'},\n",
       " 50524: {'author': [127674, 131687, 119475, 112650, 90064, 89322],\n",
       "  'year': '2006'},\n",
       " 24990: {'author': [111508], 'year': '2004'},\n",
       " 18412: {'author': [84684, 113770, 136468, 73893], 'year': '2004'},\n",
       " 53980: {'author': [102027, 135286, 138118], 'year': '2006'},\n",
       " 22758: {'author': [77347, 105143, 73536], 'year': '2006'},\n",
       " 59192: {'author': [82590, 102539, 142912, 114831, 120096, 73436],\n",
       "  'year': '2004'},\n",
       " 0: {'author': [89857, 118643, 79210], 'year': '2000'},\n",
       " 64137: {'author': [79224, 97717, 95391], 'year': '2008'},\n",
       " 53985: {'author': [79250, 75992], 'year': '2006'},\n",
       " 8870: {'author': [78228, 142602, 100718], 'year': '2004'},\n",
       " 53639: {'author': [134991, 71826, 137813], 'year': '2006'},\n",
       " 1: {'author': [97713, 97795, 142858], 'year': '2000'},\n",
       " 38108: {'author': [125297, 88126, 139543, 140222], 'year': '2004'},\n",
       " 31806: {'author': [75101, 86902, 138012], 'year': '2006'},\n",
       " 64680: {'author': [121090, 106647, 78979, 87240, 103851], 'year': '2004'},\n",
       " 27798: {'author': [80034, 127674, 140532, 116230, 91962, 92821, 86404],\n",
       "  'year': '2004'},\n",
       " 8614: {'author': [135668, 102206, 106388, 73893], 'year': '2006'},\n",
       " 16469: {'author': [143024, 94690], 'year': '2006'},\n",
       " 36619: {'author': [110408], 'year': '2008'},\n",
       " 57515: {'author': [121562], 'year': '2000'},\n",
       " 3038: {'author': [120429, 135664], 'year': '2006'},\n",
       " 62506: {'author': [77527, 139214, 86009], 'year': '2006'},\n",
       " 45850: {'author': [103674, 78228], 'year': '2006'},\n",
       " 48605: {'author': [119829, 128372, 104402, 92375], 'year': '2002'},\n",
       " 26012: {'author': [109621, 73457], 'year': '2008'},\n",
       " 26972: {'author': [91647, 135300, 73251], 'year': '2008'},\n",
       " 21945: {'author': [81731, 127089], 'year': '2008'},\n",
       " 17491: {'author': [105462, 115691, 104402, 130094], 'year': '2008'},\n",
       " 66443: {'author': [96125, 128279, 110932], 'year': '2008'},\n",
       " 18491: {'author': [130828, 135300, 115613, 142525, 81207], 'year': '2008'},\n",
       " 42989: {'author': [121725, 138367, 143044], 'year': '2008'},\n",
       " 38831: {'author': [86599, 92948, 124538], 'year': '2008'},\n",
       " 65833: {'author': [143047, 76465, 79715, 143024], 'year': '2008'},\n",
       " 62736: {'author': [143047, 142990, 143024], 'year': '2008'},\n",
       " 45348: {'author': [94441, 133855, 70039, 91521], 'year': '2008'},\n",
       " 52331: {'author': [103227, 140824], 'year': '2008'},\n",
       " 34869: {'author': [85568, 130822, 103663, 121356, 97525], 'year': '2008'},\n",
       " 36286: {'author': [141252, 84464, 108720, 101661], 'year': '2008'},\n",
       " 43885: {'author': [94796, 115562, 115479, 136751], 'year': '2008'},\n",
       " 31473: {'author': [118178, 90777, 108252], 'year': '2008'},\n",
       " 43264: {'author': [132934, 129158], 'year': '2008'},\n",
       " 16159: {'author': [112387], 'year': '2008'},\n",
       " 45415: {'author': [99665, 91103], 'year': '2008'},\n",
       " 43090: {'author': [93586,\n",
       "   112109,\n",
       "   106938,\n",
       "   136950,\n",
       "   104243,\n",
       "   120474,\n",
       "   90403,\n",
       "   132928],\n",
       "  'year': '2008'},\n",
       " 18292: {'author': [127038, 116565, 125402, 134785, 137811], 'year': '2005'},\n",
       " 12786: {'author': [127038, 116565, 125402, 134785, 137811, 99891],\n",
       "  'year': '2006'},\n",
       " 23175: {'author': [76693, 133726, 115970], 'year': '2004'},\n",
       " 3938: {'author': [127038, 116565, 125402, 71000, 134785], 'year': '2004'},\n",
       " 49091: {'author': [84155, 99665], 'year': '2006'},\n",
       " 16953: {'author': [143786, 103594, 141638], 'year': '2006'},\n",
       " 23288: {'author': [106800, 98787, 143030, 70575], 'year': '2004'},\n",
       " 64790: {'author': [140969, 94497, 120924], 'year': '2004'},\n",
       " 7597: {'author': [123287, 78739], 'year': '2004'},\n",
       " 39689: {'author': [119425], 'year': '2006'},\n",
       " 33130: {'author': [77740, 135195], 'year': '2004'},\n",
       " 60196: {'author': [111241, 128670, 96166, 81531], 'year': '2004'},\n",
       " 3994: {'author': [135138, 125293], 'year': '2005'},\n",
       " 38526: {'author': [127496, 93647], 'year': '2005'},\n",
       " 31040: {'author': [122081, 127496, 86712], 'year': '2005'},\n",
       " 30226: {'author': [88800, 103463, 78031], 'year': '2004'},\n",
       " 10782: {'author': [105646, 99362], 'year': '2006'},\n",
       " 14348: {'author': [137168, 143339, 104094, 85917], 'year': '2006'},\n",
       " 28703: {'author': [99362, 81302, 114783, 79400], 'year': '2006'},\n",
       " 14747: {'author': [86821, 74547], 'year': '2004'},\n",
       " 60688: {'author': [128986, 82667, 96551], 'year': '2003'},\n",
       " 54242: {'author': [74931, 123708, 133038, 94776], 'year': '2004'},\n",
       " 59356: {'author': [119635, 108305], 'year': '2006'},\n",
       " 51042: {'author': [122766, 110328, 139177], 'year': '2003'},\n",
       " 17263: {'author': [100786, 100902], 'year': '2003'},\n",
       " 15761: {'author': [134786, 102753], 'year': '2005'},\n",
       " 30962: {'author': [142188], 'year': '2006'},\n",
       " 51349: {'author': [90654, 140264], 'year': '2007'},\n",
       " 6905: {'author': [123635, 138334], 'year': '2007'},\n",
       " 8278: {'author': [128628], 'year': '2007'},\n",
       " 1980: {'author': [140638, 83505], 'year': '2007'},\n",
       " 23376: {'author': [102960, 93647], 'year': '2004'},\n",
       " 65728: {'author': [91682], 'year': '2007'},\n",
       " 22700: {'author': [102960, 93647], 'year': '2005'},\n",
       " 53338: {'author': [98688, 70596], 'year': '2007'},\n",
       " 48631: {'author': [130135, 123975], 'year': '2007'},\n",
       " 43680: {'author': [102322, 116565, 125402, 108049, 113488], 'year': '2007'},\n",
       " 58174: {'author': [114882, 120091], 'year': '2007'},\n",
       " 29310: {'author': [126201, 105820, 92217], 'year': '2007'},\n",
       " 37755: {'author': [86421, 142929], 'year': '2007'},\n",
       " 60941: {'author': [126780, 121383], 'year': '2007'},\n",
       " 25049: {'author': [100371, 127543, 73197], 'year': '2007'},\n",
       " 3448: {'author': [141560, 100963, 104094], 'year': '2007'},\n",
       " 25133: {'author': [116854, 98836], 'year': '2008'},\n",
       " 13647: {'author': [125654, 139276, 143503], 'year': '2008'},\n",
       " 56765: {'author': [82505, 80786, 80317, 110191, 97776], 'year': '2008'},\n",
       " 13752: {'author': [97669], 'year': '2008'},\n",
       " 60705: {'author': [118773, 92767, 79358], 'year': '2008'},\n",
       " 13174: {'author': [101480, 130507], 'year': '2008'},\n",
       " 56426: {'author': [119694, 127955], 'year': '2008'},\n",
       " 5725: {'author': [128041, 87065, 80117, 70219], 'year': '2008'},\n",
       " 38735: {'author': [102322, 116565, 125402, 108049, 113488], 'year': '2008'},\n",
       " 10865: {'author': [115238, 96163, 71268, 117512, 72882, 128833],\n",
       "  'year': '2008'},\n",
       " 29203: {'author': [90654, 140264], 'year': '2008'},\n",
       " 60536: {'author': [76708, 127398, 86018, 78378], 'year': '2008'},\n",
       " 4551: {'author': [117860, 80883, 74547, 133765], 'year': '2008'},\n",
       " 65509: {'author': [119431, 136017, 109147, 80970], 'year': '2005'},\n",
       " 33696: {'author': [126031, 119669, 86631, 71201], 'year': '2004'},\n",
       " 18418: {'author': [125686, 74635], 'year': '2005'},\n",
       " 38122: {'author': [116282, 139249], 'year': '2005'},\n",
       " 20057: {'author': [115646, 129817, 136135], 'year': '2004'},\n",
       " 17180: {'author': [123709, 85932, 93251], 'year': '2004'},\n",
       " 17246: {'author': [74542, 88578], 'year': '2005'},\n",
       " 14551: {'author': [130324, 117239], 'year': '2005'},\n",
       " 66137: {'author': [140205, 130373], 'year': '2004'},\n",
       " 62275: {'author': [133156, 111475, 96221], 'year': '2005'},\n",
       " 10117: {'author': [90721, 84860, 75680, 82711, 137709], 'year': '2004'},\n",
       " 43088: {'author': [92533, 106230, 71749], 'year': '2006'},\n",
       " 12629: {'author': [73285, 115765], 'year': '2006'},\n",
       " 55906: {'author': [97400, 92533], 'year': '2006'},\n",
       " 29249: {'author': [82416, 71004, 100201], 'year': '2006'},\n",
       " 19458: {'author': [121122, 119467, 137262], 'year': '2006'},\n",
       " 8931: {'author': [78158, 83257, 91864, 76871, 98915], 'year': '2006'},\n",
       " 13616: {'author': [119397, 96868, 99815, 90947, 136839], 'year': '2006'},\n",
       " 41975: {'author': [119882, 135034], 'year': '2006'},\n",
       " 17849: {'author': [105838, 118484, 130442, 118122, 112989, 74997],\n",
       "  'year': '2006'},\n",
       " 39906: {'author': [92768, 114017, 109149], 'year': '2000'},\n",
       " 36992: {'author': [109673, 98735], 'year': '2000'},\n",
       " 2: {'author': [86065, 88252], 'year': '2000'},\n",
       " 44152: {'author': [135196, 114542], 'year': '2004'},\n",
       " 20156: {'author': [73205, 94702], 'year': '2004'},\n",
       " 4313: {'author': [104397, 95423, 91679, 74615], 'year': '2006'},\n",
       " 32616: {'author': [114506, 106460], 'year': '2006'},\n",
       " 15131: {'author': [90527], 'year': '2000'},\n",
       " 21015: {'author': [86812, 73900], 'year': '2004'},\n",
       " 14823: {'author': [101267, 73900, 95652], 'year': '2004'},\n",
       " 54008: {'author': [104772, 81515, 96298], 'year': '2004'},\n",
       " 14379: {'author': [112023], 'year': '2006'},\n",
       " 33089: {'author': [133528], 'year': '2006'},\n",
       " 51279: {'author': [137991, 124893, 108297], 'year': '2006'},\n",
       " 4618: {'author': [124031], 'year': '2006'},\n",
       " 3529: {'author': [83879, 80902], 'year': '2006'},\n",
       " 2825: {'author': [91896, 81785], 'year': '2006'},\n",
       " 23675: {'author': [113995, 78042, 110469], 'year': '2006'},\n",
       " 3411: {'author': [108563, 107987, 128198], 'year': '2006'},\n",
       " 37215: {'author': [98814, 111863], 'year': '2006'},\n",
       " 17134: {'author': [97367, 71227], 'year': '2006'},\n",
       " 34165: {'author': [102286, 84187], 'year': '2006'},\n",
       " 59907: {'author': [127804, 104759, 91783], 'year': '2006'},\n",
       " 20566: {'author': [114506, 106460], 'year': '2005'},\n",
       " 56938: {'author': [94764, 109149, 109129, 121797], 'year': '2005'},\n",
       " 52629: {'author': [111052, 71577], 'year': '2005'},\n",
       " 47342: {'author': [115171, 129215, 127530, 100700, 70395, 133966],\n",
       "  'year': '2005'},\n",
       " 48537: {'author': [87313, 94085, 132971, 128378], 'year': '2005'},\n",
       " 30380: {'author': [109851, 138792], 'year': '2005'},\n",
       " 16698: {'author': [84425, 100487, 99242], 'year': '2005'},\n",
       " 30856: {'author': [95020, 71856, 74470, 92536], 'year': '2005'},\n",
       " 61412: {'author': [108095, 140650, 127772], 'year': '2005'},\n",
       " 27487: {'author': [123625, 118231], 'year': '2005'},\n",
       " 49104: {'author': [129298, 128554, 133944], 'year': '2005'},\n",
       " 14954: {'author': [125267, 124465, 89640, 82991, 77369, 76814, 103198],\n",
       "  'year': '2005'},\n",
       " 33429: {'author': [115752, 132315], 'year': '2005'},\n",
       " 15563: {'author': [106983, 82881], 'year': '2005'},\n",
       " 51167: {'author': [90906], 'year': '2005'},\n",
       " 65716: {'author': [77752, 131835, 142223], 'year': '2007'},\n",
       " 15056: {'author': [117230, 131977], 'year': '2007'},\n",
       " 24335: {'author': [142517, 77155], 'year': '2007'},\n",
       " 28561: {'author': [78614, 96184, 143349], 'year': '2007'},\n",
       " 33681: {'author': [79999, 84994, 89533, 113170, 109584, 134602],\n",
       "  'year': '2007'},\n",
       " 4908: {'author': [87679, 77973, 138712], 'year': '2007'},\n",
       " 9816: {'author': [124732, 129696, 124279], 'year': '2006'},\n",
       " 8507: {'author': [121562, 79755, 71881], 'year': '2006'},\n",
       " 24315: {'author': [97688, 71938, 111396], 'year': '2006'},\n",
       " 26163: {'author': [86766, 72833, 119398, 115649], 'year': '2006'},\n",
       " 56762: {'author': [100483], 'year': '2007'},\n",
       " 29791: {'author': [138057, 129210, 70879], 'year': '2004'},\n",
       " 59666: {'author': [83669, 105271], 'year': '2004'},\n",
       " 10805: {'author': [116282, 139249], 'year': '2006'},\n",
       " 47596: {'author': [144263,\n",
       "   122479,\n",
       "   75570,\n",
       "   108625,\n",
       "   77944,\n",
       "   126208,\n",
       "   129224,\n",
       "   90048,\n",
       "   97467,\n",
       "   143723,\n",
       "   109184],\n",
       "  'year': '2004'},\n",
       " 4924: {'author': [82901, 97273, 80886, 138254, 143763, 123786],\n",
       "  'year': '2006'},\n",
       " 22195: {'author': [70137, 143741, 76327, 143498], 'year': '2006'},\n",
       " 60395: {'author': [139465], 'year': '2004'},\n",
       " 31373: {'author': [116317, 138397, 87131], 'year': '2005'},\n",
       " 18895: {'author': [91679, 95423, 74615, 104397], 'year': '2006'},\n",
       " 64547: {'author': [94304, 116671, 108619], 'year': '2006'},\n",
       " 21185: {'author': [132326, 137167, 131203], 'year': '2006'},\n",
       " 13985: {'author': [141228, 76287, 113073], 'year': '2004'},\n",
       " 43713: {'author': [125856, 94564], 'year': '2000'},\n",
       " 10803: {'author': [76017, 121209], 'year': '2006'},\n",
       " 15052: {'author': [72531, 73686, 89551, 97317], 'year': '2006'},\n",
       " 45981: {'author': [97176, 104789], 'year': '2006'},\n",
       " 30520: {'author': [101165, 102592], 'year': '2004'},\n",
       " 3: {'author': [70667, 105057], 'year': '2000'},\n",
       " 49351: {'author': [138130, 75059], 'year': '2006'},\n",
       " 12262: {'author': [109374, 80161, 141999], 'year': '2006'},\n",
       " 2757: {'author': [141119, 114896], 'year': '2006'},\n",
       " 34167: {'author': [114670, 94761, 82616, 90651, 97049], 'year': '2004'},\n",
       " 13698: {'author': [114406, 109646, 126727], 'year': '2006'},\n",
       " 36721: {'author': [93390, 124187, 70472, 130175, 138677, 129263],\n",
       "  'year': '2006'},\n",
       " 24198: {'author': [106597, 111137, 127338], 'year': '2006'},\n",
       " 13321: {'author': [92087, 94213, 84690], 'year': '2006'},\n",
       " 14493: {'author': [110796, 140857, 93516, 98860, 127653, 118671],\n",
       "  'year': '2004'},\n",
       " 65163: {'author': [83096, 111295], 'year': '2001'},\n",
       " 30057: {'author': [103865, 130727, 133679, 130234, 93761, 137897, 97287],\n",
       "  'year': '2006'},\n",
       " 49017: {'author': [102941, 116498, 100856], 'year': '2006'},\n",
       " 5632: {'author': [92953, 88137], 'year': '2006'},\n",
       " 48785: {'author': [142033, 82022, 123370, 117239], 'year': '2004'},\n",
       " 66024: {'author': [85106, 120940, 91251], 'year': '2006'},\n",
       " 24294: {'author': [89894, 127237, 122396, 96497], 'year': '2006'},\n",
       " 35859: {'author': [133354, 134190, 133466, 74425], 'year': '2006'},\n",
       " 7851: {'author': [81560, 134463, 135674, 74044], 'year': '2004'},\n",
       " 11153: {'author': [143741, 70137, 76327], 'year': '2006'},\n",
       " 15608: {'author': [88887, 96182, 106930], 'year': '2006'},\n",
       " 3192: {'author': [136843, 122294, 111203, 139667], 'year': '2006'},\n",
       " 3544: {'author': [142959, 141453], 'year': '2006'},\n",
       " 44627: {'author': [119180, 79756, 113495, 124810], 'year': '2006'},\n",
       " 15397: {'author': [99611, 77545, 78015, 126837], 'year': '2001'},\n",
       " 62561: {'author': [103211, 110024, 95118, 119431, 125645], 'year': '2004'},\n",
       " 47231: {'author': [130487, 133529], 'year': '2007'},\n",
       " 32837: {'author': [91591, 104499, 107169], 'year': '2007'},\n",
       " 56904: {'author': [140560,\n",
       "   123359,\n",
       "   99192,\n",
       "   112954,\n",
       "   78070,\n",
       "   104091,\n",
       "   129622,\n",
       "   131771,\n",
       "   72006,\n",
       "   119379],\n",
       "  'year': '2008'},\n",
       " 12785: {'author': [131063, 83703, 84860], 'year': '2008'},\n",
       " 27141: {'author': [75397, 132859, 133466, 74425], 'year': '2007'},\n",
       " 55897: {'author': [87679, 77916, 77405, 138712, 125960, 119847],\n",
       "  'year': '2008'},\n",
       " 53548: {'author': [72864, 100639, 91084, 75643, 116671], 'year': '2008'},\n",
       " 57423: {'author': [143369, 123411, 94213], 'year': '2008'},\n",
       " 31645: {'author': [84727, 75560, 91251, 70193, 93753], 'year': '2008'},\n",
       " 51020: {'author': [102831, 129382, 128773], 'year': '2008'},\n",
       " 54107: {'author': [75475, 113226], 'year': '2008'},\n",
       " 35079: {'author': [131163, 125721, 127781, 121197, 127487], 'year': '2008'},\n",
       " 46738: {'author': [91688, 77930, 133513, 118671, 93516], 'year': '2008'},\n",
       " 46742: {'author': [137672, 91536, 92116, 117490, 144128], 'year': '2008'},\n",
       " 51395: {'author': [102166, 108020], 'year': '2008'},\n",
       " 10430: {'author': [77469, 128550, 101467, 132848, 130461, 79013],\n",
       "  'year': '2008'},\n",
       " 23090: {'author': [142479, 116558, 99449], 'year': '2008'},\n",
       " 35262: {'author': [98942, 136729, 123370], 'year': '2008'},\n",
       " 11967: {'author': [142065, 112151, 84728, 94037, 129651, 122826],\n",
       "  'year': '2008'},\n",
       " 25875: {'author': [91591, 73855], 'year': '2008'},\n",
       " 5332: {'author': [124808, 143038], 'year': '2008'},\n",
       " 39026: {'author': [72324, 116558, 98915, 110024, 73611, 124955],\n",
       "  'year': '2008'},\n",
       " 23775: {'author': [122952, 103799, 101856, 128773], 'year': '2008'},\n",
       " 51075: {'author': [110640, 130481, 114100, 88890], 'year': '2008'},\n",
       " 9044: {'author': [78659, 86704, 80572], 'year': '2008'},\n",
       " 4138: {'author': [100394, 121027], 'year': '2008'},\n",
       " 46812: {'author': [131014, 109147, 88966, 143041], 'year': '2008'},\n",
       " 61693: {'author': [98721, 78583, 98670, 92358, 110254, 78088],\n",
       "  'year': '2008'},\n",
       " 56923: {'author': [138846, 124886, 70245, 74773, 93516, 98860],\n",
       "  'year': '2008'},\n",
       " 42612: {'author': [141035, 108804, 108032, 70667, 111738, 86790],\n",
       "  'year': '2008'},\n",
       " 60843: {'author': [73453,\n",
       "   75941,\n",
       "   115072,\n",
       "   77911,\n",
       "   127394,\n",
       "   129730,\n",
       "   135519,\n",
       "   138981,\n",
       "   78453,\n",
       "   79630],\n",
       "  'year': '2008'},\n",
       " 51943: {'author': [71040,\n",
       "   103111,\n",
       "   97731,\n",
       "   92845,\n",
       "   130117,\n",
       "   135519,\n",
       "   116963,\n",
       "   100998],\n",
       "  'year': '2008'},\n",
       " 26110: {'author': [75695], 'year': '2001'},\n",
       " 23865: {'author': [138382], 'year': '2004'},\n",
       " 37196: {'author': [118052], 'year': '2008'},\n",
       " 46144: {'author': [139876, 91304], 'year': '2008'},\n",
       " 53692: {'author': [112714], 'year': '2008'},\n",
       " 62620: {'author': [99763, 83900, 101597, 122246, 130244, 140454],\n",
       "  'year': '2001'},\n",
       " 38072: {'author': [143883, 73521], 'year': '2001'},\n",
       " 13774: {'author': [98889, 81657, 138898], 'year': '2005'},\n",
       " 26918: {'author': [125741, 71978, 134223], 'year': '2006'},\n",
       " 22392: {'author': [88970, 133744], 'year': '2006'},\n",
       " 13510: {'author': [98743, 98529, 142603], 'year': '2002'},\n",
       " 43564: {'author': [95137, 128564], 'year': '2004'},\n",
       " 61956: {'author': [140060, 112969], 'year': '2006'},\n",
       " 15826: {'author': [71978, 87913], 'year': '2005'},\n",
       " 43623: {'author': [100148, 104946, 103918, 102911], 'year': '2004'},\n",
       " 33499: {'author': [132115], 'year': '2002'},\n",
       " 33804: {'author': [98363, 125857, 101568], 'year': '2005'},\n",
       " 25745: {'author': [105674, 88256, 80487, 140799, 122593], 'year': '2005'},\n",
       " 43384: {'author': [101344, 105837], 'year': '2004'},\n",
       " 27598: {'author': [81651], 'year': '2004'},\n",
       " 65962: {'author': [99055, 97439], 'year': '2002'},\n",
       " 8558: {'author': [142926, 142522], 'year': '2005'},\n",
       " 49359: {'author': [111776], 'year': '2006'},\n",
       " 57570: {'author': [111776, 98305, 144375], 'year': '2004'},\n",
       " 32083: {'author': [111776, 144375, 108452], 'year': '2005'},\n",
       " 65217: {'author': [134212, 118701, 137803, 141846], 'year': '2005'},\n",
       " 12250: {'author': [93830, 116524, 108572], 'year': '2006'},\n",
       " 4433: {'author': [108322, 70607], 'year': '2002'},\n",
       " 22878: {'author': [125304, 142181, 70403, 134364, 129644], 'year': '2006'},\n",
       " 9205: {'author': [103758, 78648, 101315], 'year': '2005'},\n",
       " 27834: {'author': [109130, 93225, 75006, 120719], 'year': '2005'},\n",
       " 32663: {'author': [109130, 117622], 'year': '2002'},\n",
       " 49334: {'author': [109130, 71129], 'year': '2006'},\n",
       " 55616: {'author': [126391,\n",
       "   71778,\n",
       "   113691,\n",
       "   134521,\n",
       "   138303,\n",
       "   133861,\n",
       "   75132,\n",
       "   78850],\n",
       "  'year': '2004'},\n",
       " 20493: {'author': [86556, 95452, 107388, 72587], 'year': '2006'},\n",
       " 7573: {'author': [113873, 91154, 122344], 'year': '2002'},\n",
       " 40530: {'author': [113873, 122344, 91154], 'year': '2002'},\n",
       " 54253: {'author': [125919, 88201, 80434], 'year': '2004'},\n",
       " 13048: {'author': [123620, 70607, 115434], 'year': '2002'},\n",
       " 28232: {'author': [107680, 78082, 107613, 113707], 'year': '2004'},\n",
       " 6980: {'author': [83347, 70607], 'year': '2006'},\n",
       " 7649: {'author': [100550, 105952], 'year': '2004'},\n",
       " 44556: {'author': [111472, 100871], 'year': '2006'},\n",
       " 30367: {'author': [129138, 130601], 'year': '2006'},\n",
       " 31192: {'author': [125233, 102953], 'year': '2006'},\n",
       " 65943: {'author': [87913], 'year': '2002'},\n",
       " 35108: {'author': [92733, 133024, 127412], 'year': '2004'},\n",
       " 44119: {'author': [92733, 102451, 72150], 'year': '2002'},\n",
       " 25419: {'author': [121068, 120505, 123136], 'year': '2006'},\n",
       " 45499: {'author': [121092, 70607, 115434], 'year': '2006'},\n",
       " 20588: {'author': [142502, 115375, 73995], 'year': '2004'},\n",
       " 29185: {'author': [136042, 134438, 114748], 'year': '2005'},\n",
       " 58378: {'author': [143635, 103908], 'year': '2006'},\n",
       " 35055: {'author': [140273, 78381], 'year': '2006'},\n",
       " 44391: {'author': [128202, 98939, 89675], 'year': '2002'},\n",
       " 5772: {'author': [139725, 140847], 'year': '2006'},\n",
       " 28757: {'author': [82065, 112969], 'year': '2006'},\n",
       " 4: {'author': [73166, 72399], 'year': '2000'},\n",
       " 37323: {'author': [98288, 107883], 'year': '2005'},\n",
       " 36481: {'author': [99940, 91615, 106014, 122131, 110493], 'year': '2002'},\n",
       " 29454: {'author': [85385, 77343, 81219, 102911], 'year': '2006'},\n",
       " 22324: {'author': [95710, 142497], 'year': '2002'},\n",
       " 49792: {'author': [127594, 70321], 'year': '2004'},\n",
       " 23541: {'author': [115058, 101463], 'year': '2005'},\n",
       " 29188: {'author': [115058, 101463], 'year': '2006'},\n",
       " 50354: {'author': [115774, 84643, 120972], 'year': '2006'},\n",
       " 56876: {'author': [123125, 142170, 116203, 98538], 'year': '2006'},\n",
       " 52869: {'author': [125326, 140900], 'year': '2002'},\n",
       " 34617: {'author': [98413, 144375], 'year': '2004'},\n",
       " 3633: {'author': [106080, 80133], 'year': '2006'},\n",
       " 28599: {'author': [121253, 101781], 'year': '2002'},\n",
       " 39029: {'author': [142604, 136193, 140652, 87267], 'year': '2006'},\n",
       " 52552: {'author': [83968, 135426], 'year': '2004'},\n",
       " 33289: {'author': [101818, 128738, 79884, 91951], 'year': '2006'},\n",
       " 42775: {'author': [114303, 77281, 139503, 110342, 124671], 'year': '2006'},\n",
       " 53143: {'author': [79910, 143284], 'year': '2005'},\n",
       " 41631: {'author': [79910, 143284, 105004], 'year': '2006'},\n",
       " 53689: {'author': [94054, 87309], 'year': '2006'},\n",
       " 15722: {'author': [112969, 121522, 137689, 88146], 'year': '2004'},\n",
       " 32107: {'author': [135426, 89349], 'year': '2004'},\n",
       " 61083: {'author': [93745, 115094, 143680], 'year': '2005'},\n",
       " 14056: {'author': [83169], 'year': '2002'},\n",
       " 58388: {'author': [108541, 126193], 'year': '2004'},\n",
       " 6961: {'author': [107260], 'year': '2005'},\n",
       " 36483: {'author': [87614, 71184, 138531], 'year': '2005'},\n",
       " 55546: {'author': [116349, 136606], 'year': '2004'},\n",
       " 66349: {'author': [107554,\n",
       "   96290,\n",
       "   110303,\n",
       "   90717,\n",
       "   94050,\n",
       "   74040,\n",
       "   117270,\n",
       "   109885,\n",
       "   134510],\n",
       "  'year': '2006'},\n",
       " 54662: {'author': [96732, 125300, 125304, 139796], 'year': '2002'},\n",
       " 26169: {'author': [102404, 81728, 79066, 132985, 102604], 'year': '2004'},\n",
       " 47523: {'author': [105609, 80820], 'year': '2006'},\n",
       " 10966: {'author': [100963, 137422, 110796], 'year': '2002'},\n",
       " 3136: {'author': [124290, 126241, 115269], 'year': '2005'},\n",
       " 34018: {'author': [73293, 105557, 84213], 'year': '2006'},\n",
       " 63869: {'author': [101038, 92409, 98065, 136149], 'year': '2006'},\n",
       " 14993: {'author': [89730, 131976], 'year': '2002'},\n",
       " 16981: {'author': [99015, 105674, 109130], 'year': '2006'},\n",
       " 10155: {'author': [94777, 96692], 'year': '2005'},\n",
       " 40873: {'author': [78231, 140848, 97286, 133850], 'year': '2006'},\n",
       " 57026: {'author': [93237, 105187, 110964, 123011, 98626], 'year': '2006'},\n",
       " 31158: {'author': [136162, 82011, 119245, 109459], 'year': '2005'},\n",
       " 49076: {'author': [141523, 132714, 129673], 'year': '2005'},\n",
       " 46551: {'author': [83498], 'year': '2004'},\n",
       " 44526: {'author': [121009, 107754, 103570], 'year': '2004'},\n",
       " 59035: {'author': [89410, 91434, 124641], 'year': '2006'},\n",
       " 63279: {'author': [119132, 114696], 'year': '2006'},\n",
       " 11211: {'author': [143704, 122332, 96565], 'year': '2005'},\n",
       " 4574: {'author': [118242], 'year': '2005'},\n",
       " 50794: {'author': [128711, 107543], 'year': '2006'},\n",
       " 2917: {'author': [144375], 'year': '2006'},\n",
       " 63988: {'author': [75814, 103152, 99940], 'year': '2005'},\n",
       " 43447: {'author': [143014, 81429, 112422], 'year': '2005'},\n",
       " 21964: {'author': [129644, 125304], 'year': '2005'},\n",
       " 64348: {'author': [129419, 94964, 126647], 'year': '2004'},\n",
       " 37202: {'author': [140723, 134894], 'year': '2006'},\n",
       " 46818: {'author': [83498, 119245, 71129, 138637, 73659, 114313],\n",
       "  'year': '2005'},\n",
       " 65165: {'author': [70245, 143339, 94054], 'year': '2005'},\n",
       " 19950: {'author': [83327, 100871], 'year': '2004'},\n",
       " 61430: {'author': [71129, 119245, 83498], 'year': '2005'},\n",
       " 26650: {'author': [133190], 'year': '2006'},\n",
       " 15485: {'author': [89197, 77765, 110493], 'year': '2004'},\n",
       " 8351: {'author': [88266, 87913], 'year': '2004'},\n",
       " 32626: {'author': [126929, 122332, 136046], 'year': '2004'},\n",
       " 65543: {'author': [121700, 104035], 'year': '2006'},\n",
       " 38561: {'author': [125108, 72189, 102042, 98857], 'year': '2005'},\n",
       " 22783: {'author': [129099, 87569, 84539], 'year': '2006'},\n",
       " 7958: {'author': [142671, 104035, 77010], 'year': '2006'},\n",
       " 56669: {'author': [105004, 143284, 132082, 79910], 'year': '2006'},\n",
       " 22277: {'author': [136520, 111420, 98150], 'year': '2006'},\n",
       " 14337: {'author': [117305, 112969], 'year': '2006'},\n",
       " 51380: {'author': [78267, 113833, 122439, 101229, 84722, 116484],\n",
       "  'year': '2006'},\n",
       " 12506: {'author': [138919, 133844, 84643], 'year': '2007'},\n",
       " 62063: {'author': [121068, 120505, 123136], 'year': '2007'},\n",
       " 17617: {'author': [99400, 104035], 'year': '2007'},\n",
       " 39817: {'author': [98598, 143284], 'year': '2007'},\n",
       " 60965: {'author': [78424, 78880], 'year': '2007'},\n",
       " 49598: {'author': [127151, 71830, 107267, 127021], 'year': '2007'},\n",
       " 50002: {'author': [126193], 'year': '2007'},\n",
       " 60309: {'author': [78231, 140848, 143588], 'year': '2007'},\n",
       " 62293: {'author': [134894, 140723], 'year': '2007'},\n",
       " 40674: {'author': [107539, 92157, 130819, 99747], 'year': '2007'},\n",
       " 15426: {'author': [85128, 118522, 143424], 'year': '2007'},\n",
       " 23881: {'author': [118645, 134709], 'year': '2007'},\n",
       " 35047: {'author': [140514, 70516, 139613, 90334, 75576], 'year': '2007'},\n",
       " 39895: {'author': [113164, 106643], 'year': '2007'},\n",
       " 54836: {'author': [73565, 80638, 111287, 134558], 'year': '2007'},\n",
       " 43252: {'author': [91792, 96634, 74329], 'year': '2007'},\n",
       " 57486: {'author': [138159, 136042, 134438, 114748], 'year': '2007'},\n",
       " 7205: {'author': [93720, 130819, 95670], 'year': '2007'},\n",
       " 66411: {'author': [70968, 93243, 104696], 'year': '2007'},\n",
       " 45774: {'author': [140775, 119565], 'year': '2007'},\n",
       " 42235: {'author': [118148, 76954], 'year': '2007'},\n",
       " 51475: {'author': [72267, 105726, 121209], 'year': '2007'},\n",
       " 44244: {'author': [104035, 90830, 140848], 'year': '2007'},\n",
       " 16386: {'author': [142404, 71443], 'year': '2007'},\n",
       " 64779: {'author': [79602], 'year': '2007'},\n",
       " 49429: {'author': [142678], 'year': '2007'},\n",
       " 42094: {'author': [138405, 113781, 114789], 'year': '2007'},\n",
       " 21274: {'author': [118966, 91892, 120748], 'year': '2008'},\n",
       " 41783: {'author': [91037], 'year': '2008'},\n",
       " 53582: {'author': [82050, 77984, 129459], 'year': '2008'},\n",
       " 6685: {'author': [143089, 91091, 114499, 109688], 'year': '2008'},\n",
       " 20699: {'author': [78059, 143284], 'year': '2008'},\n",
       " 8902: {'author': [133673, 118729, 144387, 119049], 'year': '2008'},\n",
       " 4944: {'author': [99511, 107766, 130799, 137996], 'year': '2007'},\n",
       " 53534: {'author': [136107, 103046, 144375], 'year': '2007'},\n",
       " 33098: {'author': [100730, 128754], 'year': '2007'},\n",
       " 13263: {'author': [117842, 107554, 134510], 'year': '2007'},\n",
       " 15596: {'author': [75132,\n",
       "   122279,\n",
       "   117924,\n",
       "   134746,\n",
       "   121216,\n",
       "   128925,\n",
       "   126092,\n",
       "   140711],\n",
       "  'year': '2008'},\n",
       " 14606: {'author': [128566, 89673], 'year': '2008'},\n",
       " 40386: {'author': [78034, 131027, 99056], 'year': '2008'},\n",
       " 23150: {'author': [111457, 83347, 70607], 'year': '2008'},\n",
       " 62888: {'author': [71091, 137796], 'year': '2008'},\n",
       " 57281: {'author': [137422], 'year': '2008'},\n",
       " 62829: {'author': [120426, 97499, 70550, 123289, 132422], 'year': '2008'},\n",
       " 42810: {'author': [78424, 82773], 'year': '2008'},\n",
       " 55664: {'author': [137521, 95701, 130819, 99747], 'year': '2008'},\n",
       " 32529: {'author': [106968, 123348, 82005], 'year': '2008'},\n",
       " 45856: {'author': [133851, 97318, 129419], 'year': '2008'},\n",
       " 50928: {'author': [83883, 107271], 'year': '2008'},\n",
       " 34487: {'author': [105870, 96539], 'year': '2008'},\n",
       " 66453: {'author': [99121, 77010, 88146, 107959], 'year': '2008'},\n",
       " 18230: {'author': [131027, 107026, 99056], 'year': '2008'},\n",
       " 59048: {'author': [109742], 'year': '2008'},\n",
       " 57076: {'author': [140643, 120748], 'year': '2008'},\n",
       " 12605: {'author': [71267, 117819, 143530, 94792, 87250], 'year': '2008'},\n",
       " 63595: {'author': [81276, 138347, 95811], 'year': '2008'},\n",
       " 30884: {'author': [92592, 96850], 'year': '2008'},\n",
       " 20442: {'author': [93421, 76361, 104035, 140848], 'year': '2008'},\n",
       " 26024: {'author': [99511, 116837, 137784, 73548], 'year': '2008'},\n",
       " 5923: {'author': [73199, 72267, 105726, 121209], 'year': '2008'},\n",
       " 33747: {'author': [142532, 107654, 88017], 'year': '2008'},\n",
       " 37397: {'author': [113045, 90334, 122810, 86347], 'year': '2008'},\n",
       " 2434: {'author': [111965, 87103, 132251, 70901, 117736, 140848],\n",
       "  'year': '2008'},\n",
       " 44088: {'author': [121068, 120505, 123136], 'year': '2008'},\n",
       " 26633: {'author': [126193, 120719], 'year': '2008'},\n",
       " 33831: {'author': [115530,\n",
       "   118983,\n",
       "   120062,\n",
       "   100234,\n",
       "   75778,\n",
       "   101016,\n",
       "   127786,\n",
       "   76599],\n",
       "  'year': '2008'},\n",
       " 36465: {'author': [89324, 76367], 'year': '2008'},\n",
       " 22197: {'author': [105337, 131615, 75889], 'year': '2008'},\n",
       " 3797: {'author': [127151, 71830, 127021], 'year': '2008'},\n",
       " 33604: {'author': [81830, 76482, 81688], 'year': '2008'},\n",
       " 13886: {'author': [72304, 135121, 120711], 'year': '2008'},\n",
       " 59796: {'author': [79910, 136686, 127815], 'year': '2008'},\n",
       " 28057: {'author': [104650, 93720, 130819, 99747], 'year': '2008'},\n",
       " 35456: {'author': [88933, 71892, 142323, 140905], 'year': '2008'},\n",
       " 11999: {'author': [136844, 104946, 103918, 97210, 92664, 102911],\n",
       "  'year': '2008'},\n",
       " 35837: {'author': [138208,\n",
       "   82690,\n",
       "   94360,\n",
       "   78345,\n",
       "   111317,\n",
       "   143971,\n",
       "   119350,\n",
       "   129368,\n",
       "   129531],\n",
       "  'year': '2008'},\n",
       " 3436: {'author': [105400, 103908], 'year': '2005'},\n",
       " 31904: {'author': [130122], 'year': '2005'},\n",
       " 61968: {'author': [84372], 'year': '2005'},\n",
       " 42051: {'author': [94310, 137107], 'year': '2005'},\n",
       " 32010: {'author': [104970, 119828, 84060, 74044], 'year': '2005'},\n",
       " 22823: {'author': [127088, 76410], 'year': '2005'},\n",
       " 9952: {'author': [89014, 97318, 129419], 'year': '2005'},\n",
       " 21741: {'author': [93320, 106221, 85098, 99702], 'year': '2005'},\n",
       " 64967: {'author': [132770, 135277], 'year': '2000'},\n",
       " 25693: {'author': [131363, 81301], 'year': '2005'},\n",
       " 52907: {'author': [123800, 125856, 86416], 'year': '2005'},\n",
       " 60014: {'author': [105977], 'year': '2005'},\n",
       " 18183: {'author': [95629, 79306], 'year': '2006'},\n",
       " 38382: {'author': [118746, 78235, 74118], 'year': '2008'},\n",
       " 33613: {'author': [105135, 106066, 74212], 'year': '2002'},\n",
       " 65764: {'author': [75847, 124038], 'year': '2002'},\n",
       " 42817: {'author': [120248], 'year': '2000'},\n",
       " 5: {'author': [124952, 83845, 122687], 'year': '2000'},\n",
       " 6: {'author': [141212, 101164, 141837, 92668], 'year': '2000'},\n",
       " 27753: {'author': [85478, 85208], 'year': '2001'},\n",
       " 7: {'author': [101164, 124375], 'year': '2000'},\n",
       " 3430: {'author': [129199, 89387], 'year': '2007'},\n",
       " 16556: {'author': [87446], 'year': '2007'},\n",
       " 13681: {'author': [73000, 73727], 'year': '2007'},\n",
       " 36742: {'author': [97759, 104662, 107284], 'year': '2007'},\n",
       " 49168: {'author': [70211, 125607], 'year': '2007'},\n",
       " 47422: {'author': [83775, 136386], 'year': '2007'},\n",
       " 36304: {'author': [105155, 114363, 104604], 'year': '2007'},\n",
       " 15149: {'author': [113455, 97310], 'year': '2007'},\n",
       " 36078: {'author': [83775, 96120], 'year': '2007'},\n",
       " 61199: {'author': [135014, 75747, 108212], 'year': '2007'},\n",
       " 39431: {'author': [121649, 94140, 97346], 'year': '2007'},\n",
       " 41452: {'author': [82000, 77734], 'year': '2007'},\n",
       " 43382: {'author': [107849, 95268], 'year': '2007'},\n",
       " 52702: {'author': [101398, 84477, 136158], 'year': '2007'},\n",
       " 21656: {'author': [129880, 93134, 98174], 'year': '2007'},\n",
       " 26636: {'author': [81596, 144394], 'year': '2007'},\n",
       " 18642: {'author': [106185, 98779, 83001], 'year': '2007'},\n",
       " 58673: {'author': [102102, 99264, 128088], 'year': '2007'},\n",
       " 43199: {'author': [87750, 75389], 'year': '2007'},\n",
       " 44931: {'author': [137259, 93174, 90814, 71317], 'year': '2007'},\n",
       " 17219: {'author': [74593, 88355, 104634], 'year': '2007'},\n",
       " 50603: {'author': [83880, 73703, 125792], 'year': '2007'},\n",
       " 53254: {'author': [124281, 95173, 98767], 'year': '2007'},\n",
       " 51170: {'author': [73784], 'year': '2007'},\n",
       " 6449: {'author': [101032, 134059], 'year': '2007'},\n",
       " 40239: {'author': [109652, 123058, 139915, 86244, 113707], 'year': '2007'},\n",
       " 30220: {'author': [124867, 80720], 'year': '2007'},\n",
       " 4926: {'author': [91561, 106134, 137725], 'year': '2007'},\n",
       " 19910: {'author': [141173, 99704, 99205, 108338, 135005, 83218],\n",
       "  'year': '2007'},\n",
       " 3822: {'author': [81133, 140804, 96399, 133684, 85860], 'year': '2007'},\n",
       " 19201: {'author': [95854, 71202, 130806], 'year': '2007'},\n",
       " 50369: {'author': [139398, 94142, 80819, 86767], 'year': '2007'},\n",
       " 21511: {'author': [123686, 90630], 'year': '2007'},\n",
       " 34868: {'author': [99205], 'year': '2007'},\n",
       " 48900: {'author': [140359, 138510, 83001], 'year': '2007'},\n",
       " 7249: {'author': [128787, 83001], 'year': '2007'},\n",
       " 10833: {'author': [117852, 94928, 80555, 139285], 'year': '2007'},\n",
       " 57141: {'author': [102731, 124202, 100402, 80353, 70636], 'year': '2007'},\n",
       " 50339: {'author': [95521, 127815], 'year': '2007'},\n",
       " 60186: {'author': [82246, 75277], 'year': '2007'},\n",
       " 35842: {'author': [111756, 102192, 92675], 'year': '2003'},\n",
       " 46715: {'author': [116117, 91020], 'year': '2004'},\n",
       " 26847: {'author': [87680], 'year': '2005'},\n",
       " 8511: {'author': [105141, 141092], 'year': '2005'},\n",
       " 34845: {'author': [104652, 70542, 129436], 'year': '2003'},\n",
       " 10854: {'author': [128004, 112145, 80550], 'year': '2004'},\n",
       " 44757: {'author': [73933, 70542, 121078], 'year': '2004'},\n",
       " 21993: {'author': [95993], 'year': '2004'},\n",
       " 58771: {'author': [128832], 'year': '2005'},\n",
       " 64195: {'author': [86171, 126244], 'year': '2006'},\n",
       " 11008: {'author': [106934, 126430], 'year': '2006'},\n",
       " 14218: {'author': [126648, 116105, 124691, 80247, 106970, 87261],\n",
       "  'year': '2003'},\n",
       " 5878: {'author': [104418, 123009], 'year': '2003'},\n",
       " 65609: {'author': [141008, 103221, 102049], 'year': '2006'},\n",
       " 47364: {'author': [136077, 72157], 'year': '2003'},\n",
       " 16185: {'author': [75500, 80610], 'year': '2003'},\n",
       " 54122: {'author': [94729, 88450], 'year': '2003'},\n",
       " 47240: {'author': [127046, 117024], 'year': '2006'},\n",
       " 28713: {'author': [130968], 'year': '2003'},\n",
       " 4309: {'author': [84076], 'year': '2003'},\n",
       " 31038: {'author': [133703, 78509], 'year': '2003'},\n",
       " 2774: {'author': [131850, 80001], 'year': '2003'},\n",
       " 14460: {'author': [102618, 96873], 'year': '2007'},\n",
       " 62848: {'author': [138215, 99444, 132552], 'year': '2003'},\n",
       " 36085: {'author': [84971, 136679], 'year': '2007'},\n",
       " 31061: {'author': [87332, 88753, 77330], 'year': '2003'},\n",
       " 24021: {'author': [128373, 137874, 104947], 'year': '2007'},\n",
       " 10251: {'author': [88029, 142109, 116492, 108909], 'year': '2007'},\n",
       " 30248: {'author': [127324, 118236, 124391], 'year': '2003'},\n",
       " 34412: {'author': [133151, 105055, 71417, 108380], 'year': '2007'},\n",
       " 27688: {'author': [95872, 86539], 'year': '2007'},\n",
       " 35183: {'author': [141419, 131903], 'year': '2007'},\n",
       " 60724: {'author': [140109, 101727, 124583], 'year': '2007'},\n",
       " 28035: {'author': [94934, 118274], 'year': '2007'},\n",
       " 31002: {'author': [125669], 'year': '2006'},\n",
       " 50907: {'author': [81212, 71101], 'year': '2006'},\n",
       " 4749: {'author': [107009, 99086], 'year': '2001'},\n",
       " 8: {'author': [107009, 116524, 76642], 'year': '2000'},\n",
       " 27539: {'author': [129695, 79204], 'year': '2005'},\n",
       " 15568: {'author': [84529, 96692], 'year': '2005'},\n",
       " 17484: {'author': [139722, 133543], 'year': '2006'},\n",
       " 4179: {'author': [93284], 'year': '2005'},\n",
       " 58554: {'author': [142723, 74639, 139256, 104681], 'year': '2006'},\n",
       " 6763: {'author': [120295], 'year': '2005'},\n",
       " 41344: {'author': [136159, 113324], 'year': '2001'},\n",
       " 52793: {'author': [83306, 89718], 'year': '2005'},\n",
       " 27796: {'author': [100854, 137539], 'year': '2006'},\n",
       " 4261: {'author': [118608, 106693, 121254, 82419], 'year': '2006'},\n",
       " 17201: {'author': [72272, 81806], 'year': '2006'},\n",
       " 36612: {'author': [127150, 119608], 'year': '2006'},\n",
       " 7368: {'author': [125669], 'year': '2001'},\n",
       " 14190: {'author': [123613, 88245, 117498], 'year': '2006'},\n",
       " 53275: {'author': [109885, 113376, 134510], 'year': '2006'},\n",
       " 13544: {'author': [75427, 97109], 'year': '2002'},\n",
       " 31733: {'author': [125427, 74368, 118040, 119542, 103366], 'year': '2004'},\n",
       " 44712: {'author': [125427, 119542], 'year': '2006'},\n",
       " 4599: {'author': [140315, 77009], 'year': '2005'},\n",
       " 40182: {'author': [100644, 71863, 78267, 91692], 'year': '2004'},\n",
       " 43518: {'author': [79883, 97439], 'year': '2006'},\n",
       " 31534: {'author': [136193], 'year': '2001'},\n",
       " 30275: {'author': [136193, 140652], 'year': '2005'},\n",
       " 3933: {'author': [103730, 106693, 142071, 119538], 'year': '2006'},\n",
       " 45147: {'author': [138175, 104359, 84643, 100025], 'year': '2006'},\n",
       " 13336: {'author': [138175, 104359, 84643, 100025], 'year': '2006'},\n",
       " 29135: {'author': [118115, 102901, 127051], 'year': '2006'},\n",
       " 27215: {'author': [84545, 75576, 106871], 'year': '2006'},\n",
       " 34326: {'author': [99368, 105187], 'year': '2006'},\n",
       " 9: {'author': [125944], 'year': '2000'},\n",
       " 62259: {'author': [99055, 97439, 135062], 'year': '2003'},\n",
       " 20350: {'author': [107936, 134880], 'year': '2004'},\n",
       " 8291: {'author': [119608, 100895, 76023], 'year': '2005'},\n",
       " 13440: {'author': [138218, 106043], 'year': '2005'},\n",
       " 14517: {'author': [94156], 'year': '2002'},\n",
       " 11582: {'author': [94156, 108501, 137810], 'year': '2005'},\n",
       " 55677: {'author': [94156, 92421], 'year': '2004'},\n",
       " 11311: {'author': [94156, 92421, 120496], 'year': '2005'},\n",
       " 10: {'author': [94571, 138529], 'year': '2000'},\n",
       " 8548: {'author': [134880], 'year': '2005'},\n",
       " 2875: {'author': [117059, 143163], 'year': '2006'},\n",
       " 57348: {'author': [108322, 125571], 'year': '2006'},\n",
       " 14988: {'author': [86346], 'year': '2005'},\n",
       " 31910: {'author': [88082, 116751], 'year': '2005'},\n",
       " 45007: {'author': [99936, 121600, 130211, 123062], 'year': '2001'},\n",
       " 34689: {'author': [91403, 110043, 136702, 76469, 101155], 'year': '2006'},\n",
       " 39545: {'author': [112580], 'year': '2005'},\n",
       " 6644: {'author': [112580, 89718], 'year': '2003'},\n",
       " 43250: {'author': [99637, 81494], 'year': '2001'},\n",
       " 24174: {'author': [90517], 'year': '2002'},\n",
       " 41310: {'author': [138943, 129791, 128913, 124833, 137724], 'year': '2005'},\n",
       " 27078: {'author': [129052, 108037], 'year': '2005'},\n",
       " 12022: {'author': [143638, 100176, 81494], 'year': '2006'},\n",
       " 62095: {'author': [93939, 119272, 82532, 116413], 'year': '2006'},\n",
       " 57410: {'author': [134374, 86124], 'year': '2006'},\n",
       " 45539: {'author': [85802, 143163], 'year': '2006'},\n",
       " 31429: {'author': [97924, 115380], 'year': '2001'},\n",
       " 61520: {'author': [107014, 81494, 118061, 119272], 'year': '2003'},\n",
       " 60408: {'author': [107816, 77457, 86056, 96108], 'year': '2001'},\n",
       " 10806: {'author': [121237, 71783], 'year': '2001'},\n",
       " 16300: {'author': [140120, 104344], 'year': '2006'},\n",
       " 11: {'author': [143068, 83149], 'year': '2000'},\n",
       " 3240: {'author': [135366, 118410, 80999], 'year': '2006'},\n",
       " 34173: {'author': [137198, 134880], 'year': '2006'},\n",
       " 24077: {'author': [82681,\n",
       "   113199,\n",
       "   93237,\n",
       "   73293,\n",
       "   128145,\n",
       "   99489,\n",
       "   143752,\n",
       "   107316,\n",
       "   132573],\n",
       "  'year': '2001'},\n",
       " 49010: {'author': [99415], 'year': '2005'},\n",
       " 3857: {'author': [124702], 'year': '2004'},\n",
       " 59022: {'author': [124702, 120170], 'year': '2005'},\n",
       " 12: {'author': [99805], 'year': '2000'},\n",
       " 48832: {'author': [87498, 89841, 96945], 'year': '2006'},\n",
       " 39722: {'author': [107819, 83914], 'year': '2006'},\n",
       " 13: {'author': [124750, 74826], 'year': '2000'},\n",
       " 56600: {'author': [136046, 143113, 134713, 118774], 'year': '2002'},\n",
       " 2211: {'author': [106723, 100802, 79674], 'year': '2006'},\n",
       " 39132: {'author': [75843, 127021], 'year': '2006'},\n",
       " 64145: {'author': [82532], 'year': '2006'},\n",
       " 14: {'author': [74075, 106043], 'year': '2000'},\n",
       " 18354: {'author': [128213, 76111, 135035], 'year': '2005'},\n",
       " 42210: {'author': [122562, 141340, 110128], 'year': '2006'},\n",
       " 43502: {'author': [115997, 72587], 'year': '2006'},\n",
       " 24943: {'author': [96205], 'year': '2006'},\n",
       " 36590: {'author': [87038, 99430], 'year': '2003'},\n",
       " 24519: {'author': [87038, 99430], 'year': '2004'},\n",
       " 12864: {'author': [113477, 75364], 'year': '2006'},\n",
       " 14503: {'author': [108501, 137263], 'year': '2003'},\n",
       " 31802: {'author': [95296, 99430], 'year': '2004'},\n",
       " 61867: {'author': [80802, 74075, 91905], 'year': '2003'},\n",
       " 55332: {'author': [94960, 116274], 'year': '2006'},\n",
       " 39860: {'author': [80802, 141768], 'year': '2003'},\n",
       " 60377: {'author': [96896, 105109], 'year': '2002'},\n",
       " 54556: {'author': [142532, 142894, 76928, 131526], 'year': '2006'},\n",
       " 50278: {'author': [128382, 82262, 78633], 'year': '2006'},\n",
       " 53007: {'author': [125130, 71017, 82114, 138384], 'year': '2004'},\n",
       " 58856: {'author': [132911, 110570], 'year': '2006'},\n",
       " 65996: {'author': [76876, 127009, 128849], 'year': '2005'},\n",
       " 3268: {'author': [127364, 80253], 'year': '2001'},\n",
       " 47539: {'author': [114168, 140627, 115566], 'year': '2005'},\n",
       " 48930: {'author': [114168, 140627, 115566], 'year': '2006'},\n",
       " 2254: {'author': [75402, 76407, 108834], 'year': '2006'},\n",
       " 14765: {'author': [91849], 'year': '2001'},\n",
       " 62835: {'author': [105253, 87569, 77709], 'year': '2005'},\n",
       " 25733: {'author': [105253, 77709, 82209, 108188, 111728, 112323],\n",
       "  'year': '2005'},\n",
       " 49907: {'author': [101013, 113987], 'year': '2004'},\n",
       " 46095: {'author': [127009, 128849], 'year': '2005'},\n",
       " 54454: {'author': [138551, 102931, 112675, 142071, 119538], 'year': '2006'},\n",
       " 6716: {'author': [83504, 75576], 'year': '2004'},\n",
       " 37362: {'author': [138226], 'year': '2004'},\n",
       " 18862: {'author': [89889, 76863, 103717, 143342, 70883, 118963],\n",
       "  'year': '2006'},\n",
       " 56334: {'author': [134878, 84529], 'year': '2006'},\n",
       " 51776: {'author': [96782, 138529], 'year': '2005'},\n",
       " 54990: {'author': [113199, 93237, 82681, 128145], 'year': '2002'},\n",
       " 29635: {'author': [107260, 99276, 138699], 'year': '2006'},\n",
       " 18814: {'author': [107260], 'year': '2005'},\n",
       " 15: {'author': [116780, 138529], 'year': '2000'},\n",
       " 16749: {'author': [70649, 118148, 127286], 'year': '2006'},\n",
       " 34664: {'author': [119178, 119817], 'year': '2003'},\n",
       " 33968: {'author': [100025, 104359, 84643], 'year': '2004'},\n",
       " 7338: {'author': [100025, 104359, 84643], 'year': '2005'},\n",
       " 38331: {'author': [118410, 80999], 'year': '2005'},\n",
       " 6766: {'author': [140201, 143163], 'year': '2006'},\n",
       " 43082: {'author': [117641, 79674], 'year': '2002'},\n",
       " 21262: {'author': [125294, 128911], 'year': '2006'},\n",
       " 29045: {'author': [94777, 96692], 'year': '2006'},\n",
       " 25794: {'author': [120676, 116599], 'year': '2004'},\n",
       " 35978: {'author': [120676, 116599], 'year': '2004'},\n",
       " 51525: {'author': [107260], 'year': '2004'},\n",
       " 32941: {'author': [107018, 76867], 'year': '2004'},\n",
       " 58537: {'author': [107018, 76867], 'year': '2005'},\n",
       " 16: {'author': [102984, 105187], 'year': '2000'},\n",
       " 19606: {'author': [102984, 115480], 'year': '2006'},\n",
       " 17: {'author': [109875, 74020, 127762], 'year': '2000'},\n",
       " 56665: {'author': [115182, 102984, 72587], 'year': '2005'},\n",
       " 31064: {'author': [93237, 105187, 110964, 123011, 98626], 'year': '2006'},\n",
       " 30664: {'author': [119063, 107308, 76227, 87038], 'year': '2006'},\n",
       " 61334: {'author': [85023, 93559], 'year': '2006'},\n",
       " 28117: {'author': [110546, 74155, 142865, 139957], 'year': '2006'},\n",
       " 58292: {'author': [95211, 116906], 'year': '2003'},\n",
       " 35297: {'author': [134495, 86376, 76931], 'year': '2006'},\n",
       " 63068: {'author': [98985, 119912, 75576, 112871], 'year': '2006'},\n",
       " 26757: {'author': [73044], 'year': '2002'},\n",
       " 29376: {'author': [98203, 116710, 89180], 'year': '2001'},\n",
       " 16359: {'author': [107059, 90517], 'year': '2005'},\n",
       " 3762: {'author': [107059, 90517], 'year': '2006'},\n",
       " 22608: {'author': [134423], 'year': '2006'},\n",
       " 56323: {'author': [73516, 90517], 'year': '2006'},\n",
       " 60750: {'author': [139303, 122097, 95713], 'year': '2006'},\n",
       " 15214: {'author': [128145, 82681, 100305, 76147], 'year': '2003'},\n",
       " 60706: {'author': [76111, 135035, 133684], 'year': '2004'},\n",
       " 7882: {'author': [76111, 76478, 135035], 'year': '2006'},\n",
       " 59703: {'author': [110095, 138226], 'year': '2006'},\n",
       " 46445: {'author': [136589], 'year': '2006'},\n",
       " 37404: {'author': [93393, 127051], 'year': '2001'},\n",
       " 57821: {'author': [121248, 128306, 138226], 'year': '2006'},\n",
       " 11384: {'author': [122332, 73293], 'year': '2006'},\n",
       " 44168: {'author': [81740, 141155], 'year': '2004'},\n",
       " 2453: {'author': [75952, 102931, 117667], 'year': '2006'},\n",
       " 38708: {'author': [134295, 140627, 115566], 'year': '2006'},\n",
       " 2517: {'author': [83242, 80802, 106919], 'year': '2005'},\n",
       " 6031: {'author': [91361, 137263], 'year': '2001'},\n",
       " 60573: {'author': [98213, 82419, 84643], 'year': '2005'},\n",
       " 56811: {'author': [98213, 82419, 84643], 'year': '2006'},\n",
       " 22135: {'author': [98213, 82419, 76280, 84643], 'year': '2004'},\n",
       " 50942: {'author': [98213, 76280, 82419, 84643], 'year': '2003'},\n",
       " 59619: {'author': [75952, 102931], 'year': '2007'},\n",
       " 13629: {'author': [118617, 143068], 'year': '2006'},\n",
       " 62310: {'author': [110759, 72728, 75250], 'year': '2006'},\n",
       " 34638: {'author': [106693, 85764, 82419, 76280], 'year': '2006'},\n",
       " 5874: {'author': [134894, 140723], 'year': '2007'},\n",
       " 2236: {'author': [118228], 'year': '2007'},\n",
       " 39086: {'author': [108118, 125134], 'year': '2007'},\n",
       " 18: {'author': [110164], 'year': '2000'},\n",
       " 19: {'author': [126313], 'year': '2000'},\n",
       " 20: {'author': [130200], 'year': '2000'},\n",
       " 12167: {'author': [140620, 82149, 128829, 118323, 118552], 'year': '2005'},\n",
       " 24878: {'author': [98555, 79554], 'year': '2008'},\n",
       " 58216: {'author': [110641, 129665], 'year': '2008'},\n",
       " 50334: {'author': [82982, 132473, 137874], 'year': '2008'},\n",
       " 55542: {'author': [134316, 116771, 92888, 128893], 'year': '2008'},\n",
       " 48123: {'author': [132780, 93943], 'year': '2008'},\n",
       " 55462: {'author': [70095, 80001, 131850, 143075], 'year': '2008'},\n",
       " 6913: {'author': [98426, 92454], 'year': '2008'},\n",
       " 57467: {'author': [89170], 'year': '2008'},\n",
       " 23798: {'author': [81785, 96526], 'year': '2008'},\n",
       " 28504: {'author': [112008, 104424, 87338], 'year': '2008'},\n",
       " 41851: {'author': [133343, 107762, 74077], 'year': '2005'},\n",
       " 7496: {'author': [121484, 118468], 'year': '2002'},\n",
       " 35091: {'author': [101100], 'year': '2005'},\n",
       " 58825: {'author': [110751, 72816, 121994], 'year': '2005'},\n",
       " 11388: {'author': [101100, 118172], 'year': '2002'},\n",
       " 63357: {'author': [84974, 83256, 113146, 114607], 'year': '2004'},\n",
       " 54124: {'author': [138582, 72365, 98505, 71855], 'year': '2004'},\n",
       " 38996: {'author': [131037, 140743], 'year': '2004'},\n",
       " 29387: {'author': [140662, 142305], 'year': '2005'},\n",
       " 23019: {'author': [83856, 98708, 94736], 'year': '2005'},\n",
       " 63942: {'author': [107757, 82933], 'year': '2003'},\n",
       " 47996: {'author': [100736, 107162], 'year': '2001'},\n",
       " 65371: {'author': [88495, 90350], 'year': '2005'},\n",
       " 45116: {'author': [83986, 94212, 91142], 'year': '2003'},\n",
       " 41342: {'author': [128233, 83273], 'year': '2003'},\n",
       " 62478: {'author': [70198, 95392], 'year': '2001'},\n",
       " 52961: {'author': [88612, 90831], 'year': '2003'},\n",
       " 47876: {'author': [70198, 95392, 96201], 'year': '2002'},\n",
       " 44742: {'author': [99179, 85512, 82333], 'year': '2003'},\n",
       " 40444: {'author': [93843, 102192, 111756], 'year': '2003'},\n",
       " 16085: {'author': [98880, 133211], 'year': '2005'},\n",
       " 53543: {'author': [141779, 94870], 'year': '2002'},\n",
       " 61170: {'author': [110607, 118468, 135639], 'year': '2004'},\n",
       " 3735: {'author': [122469, 118505], 'year': '2005'},\n",
       " 15475: {'author': [141110, 131254], 'year': '2005'},\n",
       " 24214: {'author': [139922, 82933], 'year': '2004'},\n",
       " 2569: {'author': [109656, 85620], 'year': '2001'},\n",
       " 63028: {'author': [92046, 98708], 'year': '2003'},\n",
       " 23070: {'author': [134470, 98670], 'year': '2004'},\n",
       " 20919: {'author': [134470, 98670], 'year': '2005'},\n",
       " 55765: {'author': [102192, 111756, 92675], 'year': '2002'},\n",
       " 11338: {'author': [100675, 90831, 139200], 'year': '2005'},\n",
       " 7706: {'author': [96624, 102192], 'year': '2001'},\n",
       " 10590: {'author': [71336, 135639, 109891], 'year': '2003'},\n",
       " 29414: {'author': [72893, 86234], 'year': '2005'},\n",
       " 27491: {'author': [121172], 'year': '2004'},\n",
       " 41836: {'author': [124903, 75555, 94574], 'year': '2005'},\n",
       " 59406: {'author': [77413, 118468], 'year': '2007'},\n",
       " 46933: {'author': [81071, 101349, 104724], 'year': '2003'},\n",
       " 45775: {'author': [118965, 130007, 116727, 106887], 'year': '2003'},\n",
       " 28610: {'author': [81071, 133972, 104724, 101349], 'year': '2004'},\n",
       " 7701: {'author': [81071, 133972, 104724, 101349], 'year': '2005'},\n",
       " 50704: {'author': [97190, 85097, 78084], 'year': '2003'},\n",
       " 26741: {'author': [77413, 85934, 113706, 75108, 137625], 'year': '2003'},\n",
       " 45353: {'author': [112175, 136093, 123767], 'year': '2006'},\n",
       " 28990: {'author': [81071, 101349, 130081], 'year': '2002'},\n",
       " 29648: {'author': [82854, 100675, 104661], 'year': '2008'},\n",
       " 49066: {'author': [131224, 113013], 'year': '2006'},\n",
       " 50559: {'author': [102899, 98102, 110649], 'year': '2006'},\n",
       " 15350: {'author': [99096, 86819], 'year': '2006'},\n",
       " 34790: {'author': [70261, 106442], 'year': '2008'},\n",
       " 43373: {'author': [81071, 117708, 87775, 120278, 121658], 'year': '2006'},\n",
       " 32133: {'author': [81071, 130081], 'year': '2001'},\n",
       " 64775: {'author': [107711, 139905, 118468], 'year': '2008'},\n",
       " 43003: {'author': [75886, 117708, 81071], 'year': '2008'},\n",
       " 10580: {'author': [92764, 124874], 'year': '2006'},\n",
       " 16385: {'author': [87977, 134778, 81071, 104664], 'year': '2008'},\n",
       " 39626: {'author': [81729, 88755, 99801, 111927], 'year': '2006'},\n",
       " 45662: {'author': [128743, 142275], 'year': '2007'},\n",
       " 40062: {'author': [126123, 126573, 91489], 'year': '2007'},\n",
       " 33240: {'author': [124874, 70057, 79320], 'year': '2006'},\n",
       " 38276: {'author': [109603, 118017], 'year': '2008'},\n",
       " 30446: {'author': [122045, 112780], 'year': '2007'},\n",
       " 65104: {'author': [140231, 136528], 'year': '2001'},\n",
       " 9821: {'author': [107923, 102350], 'year': '2005'},\n",
       " 58961: {'author': [119787, 92439, 101451], 'year': '2004'},\n",
       " 19659: {'author': [118618, 120864, 115268], 'year': '2007'},\n",
       " 34118: {'author': [139971, 138040], 'year': '2008'},\n",
       " 19212: {'author': [94162, 97196], 'year': '2006'},\n",
       " 66023: {'author': [118618, 139971, 133479], 'year': '2006'},\n",
       " 38297: {'author': [127156], 'year': '2006'},\n",
       " 1865: {'author': [81958, 143979], 'year': '2003'},\n",
       " 64277: {'author': [143755, 127266, 106800], 'year': '2006'},\n",
       " 14102: {'author': [108811, 77330, 88753], 'year': '2006'},\n",
       " 40579: {'author': [87362, 82658], 'year': '2006'},\n",
       " 28783: {'author': [140662], 'year': '2006'},\n",
       " 65925: {'author': [79363, 137346, 114110], 'year': '2004'},\n",
       " 8403: {'author': [133275], 'year': '2003'},\n",
       " 34146: {'author': [133275, 114750], 'year': '2004'},\n",
       " 32500: {'author': [89886, 76142, 76435], 'year': '2005'},\n",
       " 47386: {'author': [139922, 82933], 'year': '2004'},\n",
       " 35370: {'author': [119591, 113694, 74794, 73417], 'year': '2006'},\n",
       " 13009: {'author': [121487, 125181, 118974], 'year': '2004'},\n",
       " 57905: {'author': [108314, 107096], 'year': '2003'},\n",
       " 56079: {'author': [87922, 139592], 'year': '2003'},\n",
       " 66129: {'author': [123159, 98150], 'year': '2007'},\n",
       " 15940: {'author': [141862, 118353], 'year': '2005'},\n",
       " 56006: {'author': [122218, 85146], 'year': '2003'},\n",
       " 65807: {'author': [99230, 92330, 113013], 'year': '2004'},\n",
       " 29097: {'author': [73167, 143979], 'year': '2007'},\n",
       " 57393: {'author': [90990, 88753, 102807, 134882], 'year': '2003'},\n",
       " 36625: {'author': [140789, 86043, 138139], 'year': '2007'},\n",
       " 15715: {'author': [110421], 'year': '2007'},\n",
       " 9418: {'author': [143867, 83206, 113407], 'year': '2007'},\n",
       " 36097: {'author': [124204, 130836], 'year': '2007'},\n",
       " 56673: {'author': [88440, 86125], 'year': '2004'},\n",
       " 20194: {'author': [102620, 82444], 'year': '2003'},\n",
       " 43043: {'author': [120268, 126330], 'year': '2003'},\n",
       " 17719: {'author': [72668, 109267, 119161, 81467], 'year': '2003'},\n",
       " 16060: {'author': [119074], 'year': '2003'},\n",
       " 64260: {'author': [129205, 136257], 'year': '2003'},\n",
       " 3886: {'author': [81700, 77644, 121387], 'year': '2007'},\n",
       " 31785: {'author': [102620, 82444, 72499], 'year': '2007'},\n",
       " 39186: {'author': [119952, 72675, 107151], 'year': '2007'},\n",
       " 3225: {'author': [119151, 88743, 84944, 100237], 'year': '2007'},\n",
       " 17397: {'author': [134984, 107539, 111144, 90587], 'year': '2007'},\n",
       " 56641: {'author': [128687, 121710, 85448], 'year': '2007'},\n",
       " 10551: {'author': [116652, 128226], 'year': '2003'},\n",
       " 3361: {'author': [119630, 82019, 79899], 'year': '2007'},\n",
       " 29096: {'author': [131369, 104378, 88577], 'year': '2007'},\n",
       " 50323: {'author': [107913, 72677, 83147, 129633], 'year': '2007'},\n",
       " 18060: {'author': [75552, 144069], 'year': '2007'},\n",
       " 33791: {'author': [70392, 100516], 'year': '2007'},\n",
       " 40702: {'author': [95665, 116833, 128369], 'year': '2007'},\n",
       " 51172: {'author': [87727], 'year': '2007'},\n",
       " 9969: {'author': [99018, 121482, 115567, 97879], 'year': '2007'},\n",
       " 31458: {'author': [113217, 137799, 120913], 'year': '2007'},\n",
       " 30424: {'author': [111550, 136353], 'year': '2007'},\n",
       " 9037: {'author': [144282, 119138, 83091, 92653, 98984, 82688],\n",
       "  'year': '2007'},\n",
       " 61573: {'author': [115844, 91640], 'year': '2007'},\n",
       " 19885: {'author': [114193], 'year': '2007'},\n",
       " 34637: {'author': [131870, 105349, 75257, 99501, 99006], 'year': '2007'},\n",
       " 43329: {'author': [127404, 74367], 'year': '2001'},\n",
       " 22898: {'author': [73659, 113315, 107509], 'year': '2004'},\n",
       " 35597: {'author': [80586, 123754, 103243, 124159, 131033, 127061],\n",
       "  'year': '2002'},\n",
       " 63515: {'author': [97146, 95392], 'year': '2002'},\n",
       " 61557: {'author': [132591, 114972], 'year': '2005'},\n",
       " 49505: {'author': [125455, 132969], 'year': '2005'},\n",
       " 60967: {'author': [89850, 109668, 133740], 'year': '2001'},\n",
       " 4321: {'author': [139231], 'year': '2005'},\n",
       " 4717: {'author': [123306, 78137], 'year': '2001'},\n",
       " 63906: {'author': [128743], 'year': '2003'},\n",
       " 25742: {'author': [74636, 70145, 117245, 74245], 'year': '2004'},\n",
       " 39849: {'author': [91466, 118468], 'year': '2004'},\n",
       " 51948: {'author': [116076, 104724], 'year': '2003'},\n",
       " 59397: {'author': [110638, 143197, 94249], 'year': '2004'},\n",
       " 37359: {'author': [142179, 117708], 'year': '2005'},\n",
       " 38388: {'author': [76709, 93270], 'year': '2002'},\n",
       " 28891: {'author': [107900, 132028, 116518, 76781], 'year': '2002'},\n",
       " 65931: {'author': [138418, 96557, 87504], 'year': '2001'},\n",
       " 55799: {'author': [121908, 73659], 'year': '2003'},\n",
       " 45435: {'author': [94339, 89073, 75012], 'year': '2005'},\n",
       " 23879: {'author': [143479, 86653, 143941, 99115], 'year': '2005'},\n",
       " 49917: {'author': [94339, 122302, 119417], 'year': '2003'},\n",
       " 64409: {'author': [128914, 86819], 'year': '2004'},\n",
       " 34880: {'author': [109598, 140938], 'year': '2002'},\n",
       " 44708: {'author': [126879], 'year': '2003'},\n",
       " 50043: {'author': [77011, 133259], 'year': '2003'},\n",
       " 65334: {'author': [132345, 117245, 74636], 'year': '2003'},\n",
       " 29939: {'author': [143197, 94249, 82463, 117872], 'year': '2005'},\n",
       " 66132: {'author': [127021, 106389, 98395], 'year': '2002'},\n",
       " 44974: {'author': [120278, 111716], 'year': '2004'},\n",
       " 63406: {'author': [101095], 'year': '2004'},\n",
       " 7608: {'author': [136014, 95071], 'year': '2003'},\n",
       " 20749: {'author': [136733, 128982], 'year': '2004'},\n",
       " 25129: {'author': [142275, 128743], 'year': '2004'},\n",
       " 20116: {'author': [102283, 82336, 87408], 'year': '2002'},\n",
       " 42046: {'author': [112798, 79359, 125434], 'year': '2003'},\n",
       " 29479: {'author': [83337, 79359, 132969], 'year': '2004'},\n",
       " 4715: {'author': [115660, 100846], 'year': '2002'},\n",
       " 7867: {'author': [130819], 'year': '2001'},\n",
       " 19422: {'author': [138257, 129039], 'year': '2001'},\n",
       " 55622: {'author': [119417, 94339], 'year': '2001'},\n",
       " 5368: {'author': [85435, 119465], 'year': '2001'},\n",
       " 17102: {'author': [77054, 101771, 133872, 116062, 70298], 'year': '2002'},\n",
       " 28700: {'author': [98453, 103064, 122779], 'year': '2005'},\n",
       " 3796: {'author': [104836, 104724, 87203, 112594], 'year': '2005'},\n",
       " 7815: {'author': [112898, 79833, 92987], 'year': '2003'},\n",
       " 7013: {'author': [124475, 117872], 'year': '2002'},\n",
       " 58594: {'author': [136560, 94312], 'year': '2001'},\n",
       " 47702: {'author': [95940, 105264, 125463], 'year': '2002'},\n",
       " 37061: {'author': [104748, 114884, 90265, 129195, 114398], 'year': '2003'},\n",
       " 6142: {'author': [118602, 118425], 'year': '2008'},\n",
       " 31938: {'author': [116692, 86819], 'year': '2002'},\n",
       " 34331: {'author': [87505, 106978, 80183], 'year': '2001'},\n",
       " 56977: {'author': [100421, 80528], 'year': '2002'},\n",
       " 16337: {'author': [113414, 112594], 'year': '2004'},\n",
       " 50517: {'author': [110080, 101693], 'year': '2004'},\n",
       " ...}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_author_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ee6e6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_citation = {}\n",
    "for item in paper_list_filter:\n",
    "    if item['index'] in index_cite_count_nonzero:\n",
    "        paper_citation[item['index']] = item['ref_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bc7dbd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_citation_idx = {}\n",
    "for k,v in paper_citation.items():\n",
    "    tmp = []\n",
    "    for item in v['paper']:\n",
    "        if item in paper_index_2_idx:\n",
    "            tmp.append(item)\n",
    "    paper_citation_idx[paper_index_2_idx[k]] = {'paper': [paper_index_2_idx[item] for item in tmp], 'year': v['year']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1b874c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23333: {'paper': [829, 1500, 17973], 'year': '2003'},\n",
       " 54982: {'paper': [1000, 829, 17973], 'year': '2003'},\n",
       " 51116: {'paper': [1300], 'year': '2003'},\n",
       " 26669: {'paper': [], 'year': '2003'},\n",
       " 2882: {'paper': [], 'year': '2003'},\n",
       " 26654: {'paper': [52990, 64141], 'year': '2003'},\n",
       " 23472: {'paper': [], 'year': '2003'},\n",
       " 3309: {'paper': [1357, 758, 18613, 1036], 'year': '2003'},\n",
       " 31232: {'paper': [19949, 708], 'year': '2003'},\n",
       " 42141: {'paper': [1500], 'year': '2002'},\n",
       " 50805: {'paper': [37465, 650], 'year': '2005'},\n",
       " 50136: {'paper': [27679, 42152, 642, 1360, 1363], 'year': '2005'},\n",
       " 11522: {'paper': [1360, 52990], 'year': '2003'},\n",
       " 35816: {'paper': [1363], 'year': '2003'},\n",
       " 62122: {'paper': [582], 'year': '2003'},\n",
       " 55456: {'paper': [], 'year': '2003'},\n",
       " 24617: {'paper': [], 'year': '2003'},\n",
       " 9508: {'paper': [1363], 'year': '2003'},\n",
       " 55401: {'paper': [], 'year': '2003'},\n",
       " 46181: {'paper': [1363], 'year': '2003'},\n",
       " 55864: {'paper': [60897], 'year': '2003'},\n",
       " 44204: {'paper': [26329], 'year': '2003'},\n",
       " 5501: {'paper': [1363, 581, 27679, 1360, 42152], 'year': '2003'},\n",
       " 21210: {'paper': [], 'year': '2003'},\n",
       " 5241: {'paper': [], 'year': '2002'},\n",
       " 38398: {'paper': [], 'year': '2002'},\n",
       " 1844: {'paper': [], 'year': '2002'},\n",
       " 28775: {'paper': [], 'year': '2002'},\n",
       " 51257: {'paper': [], 'year': '2002'},\n",
       " 17014: {'paper': [], 'year': '2002'},\n",
       " 32680: {'paper': [], 'year': '2002'},\n",
       " 42003: {'paper': [1036], 'year': '2002'},\n",
       " 23283: {'paper': [], 'year': '2000'},\n",
       " 37803: {'paper': [1015], 'year': '2001'},\n",
       " 42597: {'paper': [], 'year': '2002'},\n",
       " 10659: {'paper': [], 'year': '2001'},\n",
       " 14407: {'paper': [], 'year': '2000'},\n",
       " 6650: {'paper': [], 'year': '2002'},\n",
       " 9980: {'paper': [], 'year': '2002'},\n",
       " 43888: {'paper': [678, 677, 673], 'year': '2003'},\n",
       " 25537: {'paper': [20169, 1089, 1096, 1101, 19656, 62695, 3272, 36927],\n",
       "  'year': '2002'},\n",
       " 28972: {'paper': [660], 'year': '2004'},\n",
       " 41996: {'paper': [46837], 'year': '2002'},\n",
       " 38192: {'paper': [54128], 'year': '2004'},\n",
       " 33553: {'paper': [40485, 29216, 39358, 58175, 6793], 'year': '2007'},\n",
       " 22883: {'paper': [1684], 'year': '2004'},\n",
       " 61701: {'paper': [674, 19722, 19059, 32440, 48492, 25329, 45707],\n",
       "  'year': '2004'},\n",
       " 15888: {'paper': [], 'year': '2004'},\n",
       " 5253: {'paper': [6274], 'year': '2007'},\n",
       " 34903: {'paper': [13418, 47999], 'year': '2007'},\n",
       " 65823: {'paper': [40485, 35027, 35397, 9295, 1079, 16758], 'year': '2007'},\n",
       " 8444: {'paper': [32196, 16470, 38108, 24094], 'year': '2007'},\n",
       " 46936: {'paper': [7668, 15458, 10904], 'year': '2007'},\n",
       " 39054: {'paper': [60758, 29138, 23446, 49195, 45617], 'year': '2006'},\n",
       " 12176: {'paper': [27831], 'year': '2006'},\n",
       " 39612: {'paper': [19989, 30398, 6274], 'year': '2006'},\n",
       " 16017: {'paper': [940, 11001], 'year': '2006'},\n",
       " 3103: {'paper': [657, 27536, 54233, 60534, 1097], 'year': '2005'},\n",
       " 31288: {'paper': [66176, 55484, 18387], 'year': '2006'},\n",
       " 24447: {'paper': [20967], 'year': '2005'},\n",
       " 16008: {'paper': [35397, 19989, 49386, 29627], 'year': '2008'},\n",
       " 34027: {'paper': [66356], 'year': '2005'},\n",
       " 49124: {'paper': [59259, 13933, 32263, 25585, 22318], 'year': '2008'},\n",
       " 6469: {'paper': [46746, 48696, 34795, 53053], 'year': '2005'},\n",
       " 3427: {'paper': [], 'year': '2005'},\n",
       " 19989: {'paper': [30398, 6274], 'year': '2005'},\n",
       " 62421: {'paper': [39074, 22182, 6431], 'year': '2005'},\n",
       " 30606: {'paper': [], 'year': '2005'},\n",
       " 20657: {'paper': [45776, 52553, 19989], 'year': '2008'},\n",
       " 66084: {'paper': [43099, 19761, 17554, 65912], 'year': '2008'},\n",
       " 37533: {'paper': [], 'year': '2004'},\n",
       " 47535: {'paper': [1457, 51916, 59192, 18387, 34026, 60455], 'year': '2006'},\n",
       " 43393: {'paper': [134, 29183, 33467], 'year': '2008'},\n",
       " 7661: {'paper': [1463, 65428, 134, 52142], 'year': '2006'},\n",
       " 24197: {'paper': [997], 'year': '2002'},\n",
       " 55237: {'paper': [14369, 52653, 55146, 65527], 'year': '2006'},\n",
       " 55484: {'paper': [28797], 'year': '2004'},\n",
       " 5562: {'paper': [0, 12601, 60212], 'year': '2004'},\n",
       " 56508: {'paper': [], 'year': '2000'},\n",
       " 41218: {'paper': [0, 1, 1079], 'year': '2004'},\n",
       " 59125: {'paper': [6611], 'year': '2002'},\n",
       " 5823: {'paper': [], 'year': '2004'},\n",
       " 66085: {'paper': [27798, 58578, 35167], 'year': '2006'},\n",
       " 7695: {'paper': [24609], 'year': '2006'},\n",
       " 11325: {'paper': [34709, 16470, 1079], 'year': '2006'},\n",
       " 23403: {'paper': [65583, 29103], 'year': '2006'},\n",
       " 52333: {'paper': [43773, 7847], 'year': '2004'},\n",
       " 62825: {'paper': [], 'year': '2006'},\n",
       " 10452: {'paper': [57551, 20435, 1461, 3194], 'year': '2006'},\n",
       " 50524: {'paper': [48605, 5782, 48215, 1005, 13418, 35167, 3449],\n",
       "  'year': '2006'},\n",
       " 24990: {'paper': [], 'year': '2004'},\n",
       " 18412: {'paper': [8545], 'year': '2004'},\n",
       " 53980: {'paper': [15491, 36550, 57073, 16470], 'year': '2006'},\n",
       " 22758: {'paper': [57382], 'year': '2006'},\n",
       " 59192: {'paper': [83, 1460, 36443, 7423, 52477], 'year': '2004'},\n",
       " 0: {'paper': [75, 57], 'year': '2000'},\n",
       " 64137: {'paper': [60212], 'year': '2008'},\n",
       " 53985: {'paper': [4586, 53381], 'year': '2006'},\n",
       " 8870: {'paper': [19534, 24336, 26833, 56949], 'year': '2004'},\n",
       " 53639: {'paper': [41000], 'year': '2006'},\n",
       " 1: {'paper': [], 'year': '2000'},\n",
       " 38108: {'paper': [], 'year': '2004'},\n",
       " 31806: {'paper': [48605, 0, 60212, 3449], 'year': '2006'},\n",
       " 64680: {'paper': [939], 'year': '2004'},\n",
       " 27798: {'paper': [5782, 60212, 51916], 'year': '2004'},\n",
       " 8614: {'paper': [55218], 'year': '2006'},\n",
       " 16469: {'paper': [60212, 13631, 30854, 6415, 46892, 36714], 'year': '2006'},\n",
       " 36619: {'paper': [5782, 14821, 38108, 24094], 'year': '2008'},\n",
       " 57515: {'paper': [], 'year': '2000'},\n",
       " 3038: {'paper': [55086, 38108, 16470, 34456, 55647], 'year': '2006'},\n",
       " 62506: {'paper': [48215, 20435], 'year': '2006'},\n",
       " 45850: {'paper': [44295, 8870, 38121], 'year': '2006'},\n",
       " 48605: {'paper': [1461], 'year': '2002'},\n",
       " 26012: {'paper': [14625, 51602, 46655, 24842], 'year': '2008'},\n",
       " 26972: {'paper': [16949, 5174, 44910], 'year': '2008'},\n",
       " 21945: {'paper': [56, 1466, 16469, 47535], 'year': '2008'},\n",
       " 17491: {'paper': [31744, 62179, 5782, 51916, 64514], 'year': '2008'},\n",
       " 66443: {'paper': [133, 1463, 50124], 'year': '2008'},\n",
       " 18491: {'paper': [127,\n",
       "   43773,\n",
       "   54636,\n",
       "   61811,\n",
       "   22514,\n",
       "   34297,\n",
       "   28507,\n",
       "   23573,\n",
       "   59057],\n",
       "  'year': '2008'},\n",
       " 42989: {'paper': [4673, 53980, 16469, 24208], 'year': '2008'},\n",
       " 38831: {'paper': [54640, 55629, 2389, 29750, 65620, 50582, 60559, 9661, 2726],\n",
       "  'year': '2008'},\n",
       " 65833: {'paper': [57161], 'year': '2008'},\n",
       " 62736: {'paper': [2804, 58578, 18346, 13245, 47445, 3849], 'year': '2008'},\n",
       " 45348: {'paper': [28797, 18492, 22758], 'year': '2008'},\n",
       " 52331: {'paper': [58992, 28797, 41719, 24609, 18665, 54449, 16469, 5794],\n",
       "  'year': '2008'},\n",
       " 34869: {'paper': [66272, 27857, 24990], 'year': '2008'},\n",
       " 36286: {'paper': [13631, 52477, 36714, 4773, 58578, 64559], 'year': '2008'},\n",
       " 43885: {'paper': [14856, 22838, 18492, 53980, 31806, 45960], 'year': '2008'},\n",
       " 31473: {'paper': [36550, 44416, 53307], 'year': '2008'},\n",
       " 43264: {'paper': [1008, 48355, 1520, 9359, 46507, 61484], 'year': '2008'},\n",
       " 16159: {'paper': [61352, 38108, 55484, 7695], 'year': '2008'},\n",
       " 45415: {'paper': [54438, 2227], 'year': '2008'},\n",
       " 43090: {'paper': [16949, 10101, 52333, 19066, 36658, 35271], 'year': '2008'},\n",
       " 18292: {'paper': [48647, 2814, 3938, 50671, 52990], 'year': '2005'},\n",
       " 12786: {'paper': [18292], 'year': '2006'},\n",
       " 23175: {'paper': [52047, 230], 'year': '2004'},\n",
       " 3938: {'paper': [50671, 49474, 50491, 48647, 8821, 2814, 52990],\n",
       "  'year': '2004'},\n",
       " 49091: {'paper': [28801, 1689, 63646], 'year': '2006'},\n",
       " 16953: {'paper': [42512], 'year': '2006'},\n",
       " 23288: {'paper': [59063], 'year': '2004'},\n",
       " 64790: {'paper': [10220, 36108], 'year': '2004'},\n",
       " 7597: {'paper': [28671, 38548, 14067, 20453], 'year': '2004'},\n",
       " 39689: {'paper': [51224, 6740, 50915], 'year': '2006'},\n",
       " 33130: {'paper': [], 'year': '2004'},\n",
       " 60196: {'paper': [27044, 16600], 'year': '2004'},\n",
       " 3994: {'paper': [7531], 'year': '2005'},\n",
       " 38526: {'paper': [31040], 'year': '2005'},\n",
       " 31040: {'paper': [6025, 1457, 4865, 43736], 'year': '2005'},\n",
       " 30226: {'paper': [], 'year': '2004'},\n",
       " 10782: {'paper': [28703, 36238, 65317], 'year': '2006'},\n",
       " 14348: {'paper': [41934, 17726, 48001, 52891], 'year': '2006'},\n",
       " 28703: {'paper': [37163, 50261, 29682, 28864, 56049, 53464], 'year': '2006'},\n",
       " 14747: {'paper': [2127, 4, 19010], 'year': '2004'},\n",
       " 60688: {'paper': [], 'year': '2003'},\n",
       " 54242: {'paper': [34554, 20040], 'year': '2004'},\n",
       " 59356: {'paper': [], 'year': '2006'},\n",
       " 51042: {'paper': [29532], 'year': '2003'},\n",
       " 17263: {'paper': [25709, 29954, 22053], 'year': '2003'},\n",
       " 15761: {'paper': [35407, 622, 4715, 619, 65631], 'year': '2005'},\n",
       " 30962: {'paper': [31959, 44365, 60459], 'year': '2006'},\n",
       " 51349: {'paper': [19645, 36641, 10955, 34805], 'year': '2007'},\n",
       " 6905: {'paper': [22129, 29902, 37278, 22622, 29129, 62471, 35586],\n",
       "  'year': '2007'},\n",
       " 8278: {'paper': [65703, 9250, 10955, 40318, 47349, 55504], 'year': '2007'},\n",
       " 1980: {'paper': [1036, 48635], 'year': '2007'},\n",
       " 23376: {'paper': [], 'year': '2004'},\n",
       " 65728: {'paper': [36641,\n",
       "   48306,\n",
       "   12391,\n",
       "   44758,\n",
       "   22290,\n",
       "   59529,\n",
       "   8671,\n",
       "   41934,\n",
       "   60201],\n",
       "  'year': '2007'},\n",
       " 22700: {'paper': [23376], 'year': '2005'},\n",
       " 53338: {'paper': [23469], 'year': '2007'},\n",
       " 48631: {'paper': [55754, 31859], 'year': '2007'},\n",
       " 43680: {'paper': [12786, 49474], 'year': '2007'},\n",
       " 58174: {'paper': [], 'year': '2007'},\n",
       " 29310: {'paper': [5947, 63152, 29649], 'year': '2007'},\n",
       " 37755: {'paper': [65703, 15021, 40318, 47349, 32941, 58537, 10210],\n",
       "  'year': '2007'},\n",
       " 60941: {'paper': [971, 60956, 1008], 'year': '2007'},\n",
       " 25049: {'paper': [36641, 32009, 10955], 'year': '2007'},\n",
       " 3448: {'paper': [22129, 42378, 18537, 1071, 47559, 55534, 41934, 4578, 23300],\n",
       "  'year': '2007'},\n",
       " 25133: {'paper': [58560], 'year': '2008'},\n",
       " 13647: {'paper': [38526, 23028, 30779, 36199, 14067], 'year': '2008'},\n",
       " 56765: {'paper': [], 'year': '2008'},\n",
       " 13752: {'paper': [], 'year': '2008'},\n",
       " 60705: {'paper': [61801, 40909, 28115], 'year': '2008'},\n",
       " 13174: {'paper': [10955, 19645, 41134, 65268, 36641], 'year': '2008'},\n",
       " 56426: {'paper': [], 'year': '2008'},\n",
       " 5725: {'paper': [48604, 36108, 46150], 'year': '2008'},\n",
       " 38735: {'paper': [43680, 37376], 'year': '2008'},\n",
       " 10865: {'paper': [11532, 58614, 21299], 'year': '2008'},\n",
       " 29203: {'paper': [10955, 51349, 55334, 27432, 64371], 'year': '2008'},\n",
       " 60536: {'paper': [36641, 10955, 1074, 33035, 47379], 'year': '2008'},\n",
       " 4551: {'paper': [18268], 'year': '2008'},\n",
       " 65509: {'paper': [25973,\n",
       "   13451,\n",
       "   47533,\n",
       "   58283,\n",
       "   14355,\n",
       "   14667,\n",
       "   51770,\n",
       "   26702,\n",
       "   60021,\n",
       "   44593],\n",
       "  'year': '2005'},\n",
       " 33696: {'paper': [29293, 42576], 'year': '2004'},\n",
       " 18418: {'paper': [33417, 30853, 14355, 14347], 'year': '2005'},\n",
       " 38122: {'paper': [330, 7497, 44424, 37966], 'year': '2005'},\n",
       " 20057: {'paper': [23369, 2553, 10278, 51090, 63890], 'year': '2004'},\n",
       " 17180: {'paper': [46283, 17559, 543, 11315, 26346, 30502, 8573, 55102],\n",
       "  'year': '2004'},\n",
       " 17246: {'paper': [46495, 63521, 543, 26945, 22951], 'year': '2005'},\n",
       " 14551: {'paper': [63521, 10067], 'year': '2005'},\n",
       " 66137: {'paper': [29293, 13859, 62489], 'year': '2004'},\n",
       " 62275: {'paper': [50628], 'year': '2005'},\n",
       " 10117: {'paper': [321, 314, 55689], 'year': '2004'},\n",
       " 43088: {'paper': [39834,\n",
       "   23146,\n",
       "   41674,\n",
       "   53832,\n",
       "   7422,\n",
       "   57086,\n",
       "   12052,\n",
       "   34117,\n",
       "   3799,\n",
       "   29406,\n",
       "   56035,\n",
       "   19398],\n",
       "  'year': '2006'},\n",
       " 12629: {'paper': [45074, 60286, 64831, 292, 19737, 54599], 'year': '2006'},\n",
       " 55906: {'paper': [4708,\n",
       "   31440,\n",
       "   11209,\n",
       "   31242,\n",
       "   23146,\n",
       "   53832,\n",
       "   8876,\n",
       "   7422,\n",
       "   57086,\n",
       "   31122,\n",
       "   12052,\n",
       "   29406,\n",
       "   35411,\n",
       "   3799,\n",
       "   57763,\n",
       "   56035,\n",
       "   19398],\n",
       "  'year': '2006'},\n",
       " 29249: {'paper': [7625, 55303, 51168, 10959, 46203, 43685, 19109, 61014],\n",
       "  'year': '2006'},\n",
       " 19458: {'paper': [31440, 3799], 'year': '2006'},\n",
       " 8931: {'paper': [387, 22731, 4298, 17994, 15178], 'year': '2006'},\n",
       " 13616: {'paper': [9384, 28987, 52355, 33419, 56084, 6707], 'year': '2006'},\n",
       " 41975: {'paper': [24127, 9456, 28899], 'year': '2006'},\n",
       " 17849: {'paper': [740, 54205, 48120, 14800, 1437, 42443], 'year': '2006'},\n",
       " 39906: {'paper': [], 'year': '2000'},\n",
       " 36992: {'paper': [], 'year': '2000'},\n",
       " 2: {'paper': [], 'year': '2000'},\n",
       " 44152: {'paper': [1817], 'year': '2004'},\n",
       " 20156: {'paper': [8735], 'year': '2004'},\n",
       " 4313: {'paper': [30636, 31103], 'year': '2006'},\n",
       " 32616: {'paper': [39277, 62690, 20566, 26244], 'year': '2006'},\n",
       " 15131: {'paper': [], 'year': '2000'},\n",
       " 21015: {'paper': [40643, 1008, 24173], 'year': '2004'},\n",
       " 14823: {'paper': [26931, 54238, 50021], 'year': '2004'},\n",
       " 54008: {'paper': [54682, 14827, 52918, 9737, 39747, 33390, 9497],\n",
       "  'year': '2004'},\n",
       " 14379: {'paper': [993, 13441], 'year': '2006'},\n",
       " 33089: {'paper': [9722, 15079, 37432, 23338, 60683], 'year': '2006'},\n",
       " 51279: {'paper': [48680, 21773], 'year': '2006'},\n",
       " 4618: {'paper': [13340, 4836, 32159], 'year': '2006'},\n",
       " 3529: {'paper': [25568, 13076, 44119, 47378, 30931, 58252], 'year': '2006'},\n",
       " 2825: {'paper': [3679, 17027], 'year': '2006'},\n",
       " 23675: {'paper': [26793], 'year': '2006'},\n",
       " 3411: {'paper': [1702, 730, 755, 13380, 49329, 21746], 'year': '2006'},\n",
       " 37215: {'paper': [17249], 'year': '2006'},\n",
       " 17134: {'paper': [643, 26861, 39650, 25491], 'year': '2006'},\n",
       " 34165: {'paper': [671, 11142, 28113, 1683], 'year': '2006'},\n",
       " 59907: {'paper': [], 'year': '2006'},\n",
       " 20566: {'paper': [39277, 62690, 26244], 'year': '2005'},\n",
       " 56938: {'paper': [30744], 'year': '2005'},\n",
       " 52629: {'paper': [34764], 'year': '2005'},\n",
       " 47342: {'paper': [40118, 46532], 'year': '2005'},\n",
       " 48537: {'paper': [14050], 'year': '2005'},\n",
       " 30380: {'paper': [9563, 29899, 58249], 'year': '2005'},\n",
       " 16698: {'paper': [42225, 1467], 'year': '2005'},\n",
       " 30856: {'paper': [], 'year': '2005'},\n",
       " 61412: {'paper': [1456, 50052], 'year': '2005'},\n",
       " 27487: {'paper': [33259], 'year': '2005'},\n",
       " 49104: {'paper': [], 'year': '2005'},\n",
       " 14954: {'paper': [60874], 'year': '2005'},\n",
       " 33429: {'paper': [693,\n",
       "   13933,\n",
       "   3127,\n",
       "   32440,\n",
       "   22798,\n",
       "   34772,\n",
       "   29545,\n",
       "   25329,\n",
       "   18923,\n",
       "   18942],\n",
       "  'year': '2005'},\n",
       " 15563: {'paper': [56178], 'year': '2005'},\n",
       " 51167: {'paper': [], 'year': '2005'},\n",
       " 65716: {'paper': [18689, 29432, 1617, 23024, 20274, 5563], 'year': '2007'},\n",
       " 15056: {'paper': [], 'year': '2007'},\n",
       " 24335: {'paper': [45341], 'year': '2007'},\n",
       " 28561: {'paper': [1676, 50847, 13981], 'year': '2007'},\n",
       " 33681: {'paper': [15180], 'year': '2007'},\n",
       " 4908: {'paper': [4557, 7497, 21969, 22569, 33700, 18498], 'year': '2007'},\n",
       " 9816: {'paper': [6882, 23089, 19530, 15639, 57347, 66033, 39199],\n",
       "  'year': '2006'},\n",
       " 8507: {'paper': [54456, 655, 44252, 23238, 14871, 35027, 19255, 28768, 55260],\n",
       "  'year': '2006'},\n",
       " 24315: {'paper': [6758, 1079], 'year': '2006'},\n",
       " 26163: {'paper': [56572], 'year': '2006'},\n",
       " 56762: {'paper': [5033], 'year': '2007'},\n",
       " 29791: {'paper': [12477, 11455], 'year': '2004'},\n",
       " 59666: {'paper': [37295, 55752, 39051], 'year': '2004'},\n",
       " 10805: {'paper': [24425, 44690, 330, 22731, 24848, 3686, 54591],\n",
       "  'year': '2006'},\n",
       " 47596: {'paper': [6748, 11039], 'year': '2004'},\n",
       " 4924: {'paper': [9130, 52444, 62919, 59717, 58512, 21266, 52],\n",
       "  'year': '2006'},\n",
       " 22195: {'paper': [25037,\n",
       "   27020,\n",
       "   62427,\n",
       "   43966,\n",
       "   376,\n",
       "   36663,\n",
       "   4015,\n",
       "   45096,\n",
       "   35947,\n",
       "   6788,\n",
       "   43911,\n",
       "   29665,\n",
       "   8283,\n",
       "   34142,\n",
       "   7142,\n",
       "   8355,\n",
       "   28702,\n",
       "   62558],\n",
       "  'year': '2006'},\n",
       " 60395: {'paper': [1128, 1129], 'year': '2004'},\n",
       " 31373: {'paper': [1230, 375, 36857, 21925, 52424], 'year': '2005'},\n",
       " 18895: {'paper': [30636, 31103, 4313, 34587, 44046], 'year': '2006'},\n",
       " 64547: {'paper': [10742, 51], 'year': '2006'},\n",
       " 21185: {'paper': [7038, 4973, 11039, 11644, 52682, 15772, 22209, 36825, 827],\n",
       "  'year': '2006'},\n",
       " 13985: {'paper': [65265, 5036, 43892, 14390], 'year': '2004'},\n",
       " 43713: {'paper': [], 'year': '2000'},\n",
       " 10803: {'paper': [7497,\n",
       "   18498,\n",
       "   47533,\n",
       "   13451,\n",
       "   38967,\n",
       "   44593,\n",
       "   51770,\n",
       "   60021,\n",
       "   44640],\n",
       "  'year': '2006'},\n",
       " 15052: {'paper': [43685, 29068, 42587, 46203, 48], 'year': '2006'},\n",
       " 45981: {'paper': [11258,\n",
       "   18498,\n",
       "   7497,\n",
       "   47533,\n",
       "   51770,\n",
       "   62058,\n",
       "   29319,\n",
       "   6457,\n",
       "   60021],\n",
       "  'year': '2006'},\n",
       " 30520: {'paper': [41152, 48146, 27557, 34339, 42576, 23513, 55922],\n",
       "  'year': '2004'},\n",
       " 3: {'paper': [], 'year': '2000'},\n",
       " 49351: {'paper': [53615, 38359, 27341, 33148, 53332, 11534, 8355],\n",
       "  'year': '2006'},\n",
       " 12262: {'paper': [2044, 65621, 7497, 54418, 5937, 48], 'year': '2006'},\n",
       " 2757: {'paper': [13346,\n",
       "   11503,\n",
       "   10035,\n",
       "   1541,\n",
       "   33781,\n",
       "   63172,\n",
       "   9440,\n",
       "   48568,\n",
       "   556,\n",
       "   541,\n",
       "   67,\n",
       "   20554,\n",
       "   12928,\n",
       "   22219],\n",
       "  'year': '2006'},\n",
       " 34167: {'paper': [46283, 24461], 'year': '2004'},\n",
       " 13698: {'paper': [16396, 41496, 4490, 43215], 'year': '2006'},\n",
       " 36721: {'paper': [65445, 7422, 42732, 57763, 29381], 'year': '2006'},\n",
       " 24198: {'paper': [37874, 26702, 56963, 1545, 22951, 16669, 23585],\n",
       "  'year': '2006'},\n",
       " 13321: {'paper': [62427,\n",
       "   375,\n",
       "   316,\n",
       "   66296,\n",
       "   66293,\n",
       "   55051,\n",
       "   8682,\n",
       "   6788,\n",
       "   43911,\n",
       "   10797,\n",
       "   39472,\n",
       "   2884,\n",
       "   44522],\n",
       "  'year': '2006'},\n",
       " 14493: {'paper': [31735, 22284, 371, 24921, 51073, 42195, 9002],\n",
       "  'year': '2004'},\n",
       " 65163: {'paper': [8543], 'year': '2001'},\n",
       " 30057: {'paper': [381, 49516, 9002, 17559, 8085, 62349, 14493, 50681],\n",
       "  'year': '2006'},\n",
       " 49017: {'paper': [378, 43685, 51718, 7777, 35863, 20104, 26124, 40296],\n",
       "  'year': '2006'},\n",
       " 5632: {'paper': [378, 18046, 20104], 'year': '2006'},\n",
       " 48785: {'paper': [52988, 388, 24425, 35306, 374, 29786, 24848, 54922],\n",
       "  'year': '2004'},\n",
       " 66024: {'paper': [58512], 'year': '2006'},\n",
       " 24294: {'paper': [42534, 16918, 1997, 22484], 'year': '2006'},\n",
       " 35859: {'paper': [50324, 52424, 62738, 542, 31871, 8355], 'year': '2006'},\n",
       " 7851: {'paper': [26360, 7497, 53, 48], 'year': '2004'},\n",
       " 11153: {'paper': [8682, 7497, 50040, 10632, 62558, 37837], 'year': '2006'},\n",
       " 15608: {'paper': [53615, 65621, 42587, 32295, 34007, 51218, 59698, 46879],\n",
       "  'year': '2006'},\n",
       " 3192: {'paper': [25037, 17358, 19982, 44690, 1011, 329, 37226],\n",
       "  'year': '2006'},\n",
       " 3544: {'paper': [1230, 12011, 375, 39472], 'year': '2006'},\n",
       " 44627: {'paper': [66350, 37874, 18278, 63965, 23976], 'year': '2006'},\n",
       " 15397: {'paper': [36857, 389], 'year': '2001'},\n",
       " 62561: {'paper': [25973, 13859, 375, 58283, 17167, 6645, 23000, 46725, 31443],\n",
       "  'year': '2004'},\n",
       " 47231: {'paper': [17180, 25436], 'year': '2007'},\n",
       " 32837: {'paper': [39998, 65467, 47534, 29587, 1594, 42534], 'year': '2007'},\n",
       " 56904: {'paper': [12262, 2044, 65621, 54418, 23338, 34222], 'year': '2008'},\n",
       " 12785: {'paper': [], 'year': '2008'},\n",
       " 27141: {'paper': [10632, 15670, 21803, 49017, 15052, 42643], 'year': '2007'},\n",
       " 55897: {'paper': [19862,\n",
       "   54334,\n",
       "   18498,\n",
       "   52359,\n",
       "   21803,\n",
       "   53435,\n",
       "   42743,\n",
       "   43860,\n",
       "   21969,\n",
       "   61440,\n",
       "   6630],\n",
       "  'year': '2008'},\n",
       " 53548: {'paper': [62462,\n",
       "   59916,\n",
       "   18909,\n",
       "   31443,\n",
       "   2672,\n",
       "   7804,\n",
       "   23338,\n",
       "   4490,\n",
       "   34416,\n",
       "   62032,\n",
       "   4557,\n",
       "   10413],\n",
       "  'year': '2008'},\n",
       " 57423: {'paper': [17614,\n",
       "   22480,\n",
       "   55429,\n",
       "   19109,\n",
       "   51718,\n",
       "   51168,\n",
       "   10959,\n",
       "   61014,\n",
       "   22884],\n",
       "  'year': '2008'},\n",
       " 31645: {'paper': [29068, 53369, 9388, 36187], 'year': '2008'},\n",
       " 51020: {'paper': [17614, 22480, 51168, 10959, 46203, 61014, 22884],\n",
       "  'year': '2008'},\n",
       " 54107: {'paper': [37548,\n",
       "   12639,\n",
       "   41809,\n",
       "   14118,\n",
       "   65784,\n",
       "   13958,\n",
       "   52034,\n",
       "   26484,\n",
       "   56418],\n",
       "  'year': '2008'},\n",
       " 35079: {'paper': [20821,\n",
       "   37936,\n",
       "   20480,\n",
       "   65827,\n",
       "   23338,\n",
       "   33035,\n",
       "   30836,\n",
       "   39877,\n",
       "   33094,\n",
       "   26228,\n",
       "   47379],\n",
       "  'year': '2008'},\n",
       " 46738: {'paper': [2044, 12639, 65621, 19109, 54418, 519], 'year': '2008'},\n",
       " 46742: {'paper': [58430,\n",
       "   40148,\n",
       "   388,\n",
       "   374,\n",
       "   17586,\n",
       "   25300,\n",
       "   14022,\n",
       "   20624,\n",
       "   53600,\n",
       "   36003,\n",
       "   44404,\n",
       "   33821,\n",
       "   57763,\n",
       "   46238],\n",
       "  'year': '2008'},\n",
       " 51395: {'paper': [31622,\n",
       "   1882,\n",
       "   10916,\n",
       "   62533,\n",
       "   26635,\n",
       "   15059,\n",
       "   26113,\n",
       "   31509,\n",
       "   35949],\n",
       "  'year': '2008'},\n",
       " 10430: {'paper': [19637, 43685, 59084, 44789, 15670, 42643, 7777, 48],\n",
       "  'year': '2008'},\n",
       " 23090: {'paper': [17614,\n",
       "   15670,\n",
       "   40148,\n",
       "   19637,\n",
       "   2907,\n",
       "   51718,\n",
       "   51168,\n",
       "   46203,\n",
       "   61014],\n",
       "  'year': '2008'},\n",
       " 35262: {'paper': [10392, 31295, 54650, 47040, 12720, 48785, 18661],\n",
       "  'year': '2008'},\n",
       " 11967: {'paper': [31735, 9630, 28144, 8085, 18557], 'year': '2008'},\n",
       " 25875: {'paper': [41735, 33552, 10025, 29587, 31266, 32837, 23561],\n",
       "  'year': '2008'},\n",
       " 5332: {'paper': [43310, 1687, 23672, 34735, 61353, 45138, 35900, 35231],\n",
       "  'year': '2008'},\n",
       " 39026: {'paper': [375, 2044, 39608, 65621, 10632, 13260], 'year': '2008'},\n",
       " 23775: {'paper': [54334,\n",
       "   15670,\n",
       "   12011,\n",
       "   375,\n",
       "   18498,\n",
       "   44593,\n",
       "   2044,\n",
       "   27399,\n",
       "   21803,\n",
       "   7497,\n",
       "   51770,\n",
       "   56443,\n",
       "   59698],\n",
       "  'year': '2008'},\n",
       " 51075: {'paper': [6546, 26255, 22276, 8645], 'year': '2008'},\n",
       " 9044: {'paper': [11153, 50040, 375, 51245, 10632, 50069], 'year': '2008'},\n",
       " 4138: {'paper': [64665,\n",
       "   53615,\n",
       "   35306,\n",
       "   37548,\n",
       "   4015,\n",
       "   23432,\n",
       "   60118,\n",
       "   14489,\n",
       "   53332,\n",
       "   62558,\n",
       "   50,\n",
       "   6788],\n",
       "  'year': '2008'},\n",
       " 46812: {'paper': [22473,\n",
       "   15052,\n",
       "   49017,\n",
       "   12262,\n",
       "   3506,\n",
       "   7497,\n",
       "   51245,\n",
       "   46203,\n",
       "   16217,\n",
       "   7851,\n",
       "   60021,\n",
       "   17409,\n",
       "   48],\n",
       "  'year': '2008'},\n",
       " 61693: {'paper': [64547, 41035, 23338, 40476], 'year': '2008'},\n",
       " 56923: {'paper': [57481,\n",
       "   60095,\n",
       "   16224,\n",
       "   24785,\n",
       "   19482,\n",
       "   39832,\n",
       "   9630,\n",
       "   37592,\n",
       "   8085,\n",
       "   57225,\n",
       "   62349,\n",
       "   63857],\n",
       "  'year': '2008'},\n",
       " 42612: {'paper': [12829,\n",
       "   25063,\n",
       "   66054,\n",
       "   516,\n",
       "   14738,\n",
       "   16563,\n",
       "   55116,\n",
       "   32458,\n",
       "   6659,\n",
       "   8543,\n",
       "   53082,\n",
       "   36251,\n",
       "   57540],\n",
       "  'year': '2008'},\n",
       " 60843: {'paper': [60979, 58018, 62032, 50702, 30319], 'year': '2008'},\n",
       " 51943: {'paper': [65467, 30319, 50702], 'year': '2008'},\n",
       " 26110: {'paper': [6050], 'year': '2001'},\n",
       " 23865: {'paper': [], 'year': '2004'},\n",
       " 37196: {'paper': [7981, 7345], 'year': '2008'},\n",
       " 46144: {'paper': [7981, 19370, 62498], 'year': '2008'},\n",
       " 53692: {'paper': [28533, 7981, 19370], 'year': '2008'},\n",
       " 62620: {'paper': [], 'year': '2001'},\n",
       " 38072: {'paper': [], 'year': '2001'},\n",
       " 13774: {'paper': [55754], 'year': '2005'},\n",
       " 26918: {'paper': [60336, 25429, 27679, 5020, 7867, 762], 'year': '2006'},\n",
       " 22392: {'paper': [914, 49394], 'year': '2006'},\n",
       " 13510: {'paper': [118, 55935], 'year': '2002'},\n",
       " 43564: {'paper': [], 'year': '2004'},\n",
       " 61956: {'paper': [64791, 65703, 7864, 40318, 47349, 22261, 10210, 55504],\n",
       "  'year': '2006'},\n",
       " 15826: {'paper': [], 'year': '2005'},\n",
       " 43623: {'paper': [], 'year': '2004'},\n",
       " 33499: {'paper': [1151, 54307], 'year': '2002'},\n",
       " 33804: {'paper': [62196, 9629, 49061], 'year': '2005'},\n",
       " 25745: {'paper': [28205], 'year': '2005'},\n",
       " 43384: {'paper': [36481], 'year': '2004'},\n",
       " 27598: {'paper': [], 'year': '2004'},\n",
       " 65962: {'paper': [], 'year': '2002'},\n",
       " 8558: {'paper': [25330], 'year': '2005'},\n",
       " 49359: {'paper': [22509, 4987, 11937], 'year': '2006'},\n",
       " 57570: {'paper': [25762], 'year': '2004'},\n",
       " 32083: {'paper': [16276, 55754, 25762, 34575, 56049, 56202, 46588, 53519],\n",
       "  'year': '2005'},\n",
       " 65217: {'paper': [5020, 28138, 39476], 'year': '2005'},\n",
       " 12250: {'paper': [65703,\n",
       "   40318,\n",
       "   32941,\n",
       "   58537,\n",
       "   10210,\n",
       "   55504,\n",
       "   59494,\n",
       "   40758,\n",
       "   13688],\n",
       "  'year': '2006'},\n",
       " 4433: {'paper': [17517, 586], 'year': '2002'},\n",
       " 22878: {'paper': [46528, 46222], 'year': '2006'},\n",
       " 9205: {'paper': [13340, 32159], 'year': '2005'},\n",
       " 27834: {'paper': [31887, 18180, 3367, 11903, 35112], 'year': '2005'},\n",
       " 32663: {'paper': [9369, 12526], 'year': '2002'},\n",
       " 49334: {'paper': [18180, 58010, 33539], 'year': '2006'},\n",
       " 55616: {'paper': [190, 27008, 31050, 1244, 43147], 'year': '2004'},\n",
       " 20493: {'paper': [27422, 19969, 23394, 1244, 23836], 'year': '2006'},\n",
       " 7573: {'paper': [40530], 'year': '2002'},\n",
       " 40530: {'paper': [7573], 'year': '2002'},\n",
       " 54253: {'paper': [57103, 27904], 'year': '2004'},\n",
       " 13048: {'paper': [], 'year': '2002'},\n",
       " 28232: {'paper': [], 'year': '2004'},\n",
       " 6980: {'paper': [31417, 36698, 10210, 34227], 'year': '2006'},\n",
       " 7649: {'paper': [27629, 30175], 'year': '2004'},\n",
       " 44556: {'paper': [10966, 10955, 19645, 12391, 65268], 'year': '2006'},\n",
       " 30367: {'paper': [27008, 14581, 47123, 53910], 'year': '2006'},\n",
       " 31192: {'paper': [66295, 27008, 54106, 8830, 66033, 18237], 'year': '2006'},\n",
       " 65943: {'paper': [], 'year': '2002'},\n",
       " 35108: {'paper': [29186, 16790], 'year': '2004'},\n",
       " 44119: {'paper': [], 'year': '2002'},\n",
       " 25419: {'paper': [6401,\n",
       "   53074,\n",
       "   25419,\n",
       "   57536,\n",
       "   53195,\n",
       "   63138,\n",
       "   44088,\n",
       "   28239,\n",
       "   66320],\n",
       "  'year': '2006'},\n",
       " 45499: {'paper': [8968, 32990, 47559, 3684], 'year': '2006'},\n",
       " 20588: {'paper': [41511], 'year': '2004'},\n",
       " 29185: {'paper': [61937, 32581], 'year': '2005'},\n",
       " 58378: {'paper': [], 'year': '2006'},\n",
       " 35055: {'paper': [18013, 28367, 60430, 7827, 33625, 11482], 'year': '2006'},\n",
       " 44391: {'paper': [], 'year': '2002'},\n",
       " 5772: {'paper': [51728, 30633, 52407, 10838, 47285, 8226], 'year': '2006'},\n",
       " 28757: {'paper': [64791, 65703, 40318, 26839, 22261, 10210, 55504],\n",
       "  'year': '2006'},\n",
       " 4: {'paper': [], 'year': '2000'},\n",
       " 37323: {'paper': [], 'year': '2005'},\n",
       " 36481: {'paper': [1178], 'year': '2002'},\n",
       " 29454: {'paper': [36481, 12888], 'year': '2006'},\n",
       " 22324: {'paper': [], 'year': '2002'},\n",
       " 49792: {'paper': [22459, 10311, 914], 'year': '2004'},\n",
       " 23541: {'paper': [433], 'year': '2005'},\n",
       " 29188: {'paper': [433, 55170], 'year': '2006'},\n",
       " 50354: {'paper': [], 'year': '2006'},\n",
       " 56876: {'paper': [16191, 35193], 'year': '2006'},\n",
       " 52869: {'paper': [1160], 'year': '2002'},\n",
       " 34617: {'paper': [60575], 'year': '2004'},\n",
       " 3633: {'paper': [], 'year': '2006'},\n",
       " 28599: {'paper': [8268], 'year': '2002'},\n",
       " 39029: {'paper': [17124, 30275, 31534, 28802, 30394, 15214], 'year': '2006'},\n",
       " 52552: {'paper': [36351, 35108, 48015], 'year': '2004'},\n",
       " 33289: {'paper': [39047, 38290, 43542, 56263, 34647, 87], 'year': '2006'},\n",
       " 42775: {'paper': [29435, 740, 57587, 61596], 'year': '2006'},\n",
       " 53143: {'paper': [22475, 1331, 47872], 'year': '2005'},\n",
       " 41631: {'paper': [], 'year': '2006'},\n",
       " 53689: {'paper': [], 'year': '2006'},\n",
       " 15722: {'paper': [52665, 1244], 'year': '2004'},\n",
       " 32107: {'paper': [53074, 2947], 'year': '2004'},\n",
       " 61083: {'paper': [], 'year': '2005'},\n",
       " 14056: {'paper': [41672], 'year': '2002'},\n",
       " 58388: {'paper': [58010, 31887, 11903, 33539, 35112], 'year': '2004'},\n",
       " 6961: {'paper': [50942, 20629, 22135, 953, 45777, 11894], 'year': '2005'},\n",
       " 36483: {'paper': [38602], 'year': '2005'},\n",
       " 55546: {'paper': [792], 'year': '2004'},\n",
       " 66349: {'paper': [50226, 23089, 61260, 54827, 62471], 'year': '2006'},\n",
       " 54662: {'paper': [35050, 60336, 8424, 57279], 'year': '2002'},\n",
       " 26169: {'paper': [], 'year': '2004'},\n",
       " 47523: {'paper': [54291], 'year': '2006'},\n",
       " 10966: {'paper': [194], 'year': '2002'},\n",
       " 3136: {'paper': [], 'year': '2005'},\n",
       " 34018: {'paper': [27539], 'year': '2006'},\n",
       " 63869: {'paper': [], 'year': '2006'},\n",
       " 14993: {'paper': [40381, 1507, 35050, 8424], 'year': '2002'},\n",
       " 16981: {'paper': [64141], 'year': '2006'},\n",
       " 10155: {'paper': [22356, 11, 15214, 56600, 65897], 'year': '2005'},\n",
       " 40873: {'paper': [23580], 'year': '2006'},\n",
       " 57026: {'paper': [190, 47379], 'year': '2006'},\n",
       " 31158: {'paper': [28312, 23307], 'year': '2005'},\n",
       " 49076: {'paper': [700, 56066, 30873, 35050], 'year': '2005'},\n",
       " 46551: {'paper': [], 'year': '2004'},\n",
       " 44526: {'paper': [9085, 1284, 308], 'year': '2004'},\n",
       " 59035: {'paper': [18079, 48687, 41998, 41300], 'year': '2006'},\n",
       " 63279: {'paper': [54968, 66477, 37385, 25066, 64744, 14351, 24772],\n",
       "  'year': '2006'},\n",
       " 11211: {'paper': [65703, 64791, 35428, 40318, 62259, 32941, 13688, 59494],\n",
       "  'year': '2005'},\n",
       " 4574: {'paper': [65665, 48023, 47148, 18927], 'year': '2005'},\n",
       " 50794: {'paper': [7213], 'year': '2006'},\n",
       " 2917: {'paper': [25762, 55754, 28864, 56049], 'year': '2006'},\n",
       " 63988: {'paper': [], 'year': '2005'},\n",
       " 43447: {'paper': [7475, 18253, 53228, 35050, 63442, 58409, 19161],\n",
       "  'year': '2005'},\n",
       " 21964: {'paper': [7475], 'year': '2005'},\n",
       " 64348: {'paper': [31887], 'year': '2004'},\n",
       " 37202: {'paper': [15611, 65897, 21779, 45777, 11], 'year': '2006'},\n",
       " 46818: {'paper': [48685, 61128, 897], 'year': '2005'},\n",
       " 65165: {'paper': [53595], 'year': '2005'},\n",
       " 19950: {'paper': [34110, 59448, 12803, 677, 694, 139], 'year': '2004'},\n",
       " 61430: {'paper': [60754, 30655, 28869, 45443, 41560, 988], 'year': '2005'},\n",
       " 26650: {'paper': [1163], 'year': '2006'},\n",
       " 15485: {'paper': [55935], 'year': '2004'},\n",
       " 8351: {'paper': [16802, 19107], 'year': '2004'},\n",
       " 32626: {'paper': [64791, 65703, 10210, 59494, 13688], 'year': '2004'},\n",
       " 65543: {'paper': [46686], 'year': '2006'},\n",
       " 38561: {'paper': [49282, 41934, 11229], 'year': '2005'},\n",
       " 22783: {'paper': [21746, 16924], 'year': '2006'},\n",
       " 7958: {'paper': [35594,\n",
       "   64575,\n",
       "   36867,\n",
       "   12551,\n",
       "   23237,\n",
       "   4064,\n",
       "   29864,\n",
       "   15875,\n",
       "   48355,\n",
       "   28277,\n",
       "   1520,\n",
       "   19186],\n",
       "  'year': '2006'},\n",
       " 56669: {'paper': [1911, 8960, 57793], 'year': '2006'},\n",
       " 22277: {'paper': [65897, 50116, 11], 'year': '2006'},\n",
       " 14337: {'paper': [26009, 66295, 27008, 9936, 8830, 47123, 53910, 18237],\n",
       "  'year': '2006'},\n",
       " 51380: {'paper': [15453, 43468, 47412, 23515], 'year': '2006'},\n",
       " 12506: {'paper': [48903, 3859, 55593], 'year': '2007'},\n",
       " 62063: {'paper': [53576, 2361, 38288], 'year': '2007'},\n",
       " 17617: {'paper': [35729, 34227, 55338, 63568, 62252], 'year': '2007'},\n",
       " 39817: {'paper': [41934], 'year': '2007'},\n",
       " 60965: {'paper': [15082, 21778, 26329], 'year': '2007'},\n",
       " 49598: {'paper': [28939, 30959, 47579, 49276, 7826, 31111, 48572],\n",
       "  'year': '2007'},\n",
       " 50002: {'paper': [58054, 35814, 11903, 35112], 'year': '2007'},\n",
       " 60309: {'paper': [12101], 'year': '2007'},\n",
       " 62293: {'paper': [14888, 54782, 65249, 47545], 'year': '2007'},\n",
       " 40674: {'paper': [62542, 65918, 3422, 51504, 50104], 'year': '2007'},\n",
       " 15426: {'paper': [34126, 26517, 65390, 66338, 45547], 'year': '2007'},\n",
       " 23881: {'paper': [29055, 32325, 35738, 16786], 'year': '2007'},\n",
       " 35047: {'paper': [14780, 26839, 7790], 'year': '2007'},\n",
       " 39895: {'paper': [39726, 54123, 1187], 'year': '2007'},\n",
       " 54836: {'paper': [36238, 3640, 4386, 27983, 61128], 'year': '2007'},\n",
       " 43252: {'paper': [64554, 34804, 34892, 970, 27], 'year': '2007'},\n",
       " 57486: {'paper': [32581, 29185, 58776], 'year': '2007'},\n",
       " 7205: {'paper': [58952,\n",
       "   1331,\n",
       "   40529,\n",
       "   39769,\n",
       "   21326,\n",
       "   63977,\n",
       "   49563,\n",
       "   45956,\n",
       "   59552,\n",
       "   34270],\n",
       "  'year': '2007'},\n",
       " 66411: {'paper': [49417, 50210], 'year': '2007'},\n",
       " 45774: {'paper': [65703, 40318, 10210, 32941], 'year': '2007'},\n",
       " 42235: {'paper': [24034, 23300, 36649, 1140], 'year': '2007'},\n",
       " 51475: {'paper': [1331, 35332, 48248, 64537], 'year': '2007'},\n",
       " 44244: {'paper': [35332, 1244, 50065, 38010, 4644], 'year': '2007'},\n",
       " 16386: {'paper': [44555, 29864, 48064, 58537, 43178, 52788], 'year': '2007'},\n",
       " 64779: {'paper': [16022, 38421, 46532, 58560], 'year': '2007'},\n",
       " 49429: {'paper': [1131], 'year': '2007'},\n",
       " 42094: {'paper': [53910, 66295, 18237, 44206, 39371, 66033, 12551, 9320],\n",
       "  'year': '2007'},\n",
       " 21274: {'paper': [50746, 7475, 16232, 43312, 39491, 64709, 34832],\n",
       "  'year': '2008'},\n",
       " 41783: {'paper': [39694, 57365, 10391], 'year': '2008'},\n",
       " 53582: {'paper': [8738], 'year': '2008'},\n",
       " 6685: {'paper': [22318], 'year': '2008'},\n",
       " 20699: {'paper': [25298,\n",
       "   43293,\n",
       "   38334,\n",
       "   29864,\n",
       "   27039,\n",
       "   13024,\n",
       "   47800,\n",
       "   37261,\n",
       "   58678,\n",
       "   3600],\n",
       "  'year': '2008'},\n",
       " 8902: {'paper': [46148, 54428, 26727, 18144, 21737], 'year': '2008'},\n",
       " 4944: {'paper': [16689, 4463], 'year': '2007'},\n",
       " 53534: {'paper': [651, 21003, 47537, 52891, 31051, 28534, 43971],\n",
       "  'year': '2007'},\n",
       " 33098: {'paper': [32192], 'year': '2007'},\n",
       " 13263: {'paper': [190, 17012, 38020, 45777], 'year': '2007'},\n",
       " 15596: {'paper': [35495,\n",
       "   64251,\n",
       "   31767,\n",
       "   23526,\n",
       "   33591,\n",
       "   43147,\n",
       "   17762,\n",
       "   31111,\n",
       "   66445],\n",
       "  'year': '2008'},\n",
       " 14606: {'paper': [16092, 28657], 'year': '2008'},\n",
       " 40386: {'paper': [4870, 6656, 23266], 'year': '2008'},\n",
       " 23150: {'paper': [1520, 45245, 48268, 27014, 18646, 32370, 37219],\n",
       "  'year': '2008'},\n",
       " 62888: {'paper': [2005], 'year': '2008'},\n",
       " 57281: {'paper': [42428, 18013, 36821], 'year': '2008'},\n",
       " 62829: {'paper': [63927, 59923, 44350, 13459, 66145], 'year': '2008'},\n",
       " 42810: {'paper': [26329, 24034, 23300, 34059, 1140], 'year': '2008'},\n",
       " 55664: {'paper': [14623], 'year': '2008'},\n",
       " 32529: {'paper': [14714, 29560, 37854, 13696], 'year': '2008'},\n",
       " 45856: {'paper': [31887, 38946, 14714, 13939, 11378, 29560, 29186, 16790],\n",
       "  'year': '2008'},\n",
       " 50928: {'paper': [56273, 48680, 7475, 53228, 13353, 50226, 48054],\n",
       "  'year': '2008'},\n",
       " 34487: {'paper': [39726], 'year': '2008'},\n",
       " 66453: {'paper': [16323, 3623, 1331], 'year': '2008'},\n",
       " 18230: {'paper': [2877, 28286, 36927, 17380, 63217, 45876, 47276],\n",
       "  'year': '2008'},\n",
       " 59048: {'paper': [], 'year': '2008'},\n",
       " 57076: {'paper': [43204, 36900, 53126], 'year': '2008'},\n",
       " 12605: {'paper': [930, 14917], 'year': '2008'},\n",
       " 63595: {'paper': [4870, 62718, 34654, 27611, 46015, 50210, 46167, 23266],\n",
       "  'year': '2008'},\n",
       " 30884: {'paper': [25617, 63444], 'year': '2008'},\n",
       " 20442: {'paper': [45257, 39134, 9759, 26095, 35248, 45359], 'year': '2008'},\n",
       " 26024: {'paper': [28177, 37075, 54123, 59403], 'year': '2008'},\n",
       " 5923: {'paper': [49765, 1131, 62606, 64877, 64537], 'year': '2008'},\n",
       " 33747: {'paper': [49486, 49391, 45956], 'year': '2008'},\n",
       " 37397: {'paper': [60513, 30751, 16886, 50104], 'year': '2008'},\n",
       " 2434: {'paper': [66329, 46597, 22340], 'year': '2008'},\n",
       " 44088: {'paper': [57536, 6401, 53195], 'year': '2008'},\n",
       " 26633: {'paper': [58054, 35814, 5503, 11903, 37014, 35112], 'year': '2008'},\n",
       " 33831: {'paper': [19645], 'year': '2008'},\n",
       " 36465: {'paper': [52260, 1244, 34804, 36649, 61520, 48375], 'year': '2008'},\n",
       " 22197: {'paper': [], 'year': '2008'},\n",
       " 3797: {'paper': [32070], 'year': '2008'},\n",
       " 33604: {'paper': [26845, 1338, 7227], 'year': '2008'},\n",
       " 13886: {'paper': [59290,\n",
       "   2298,\n",
       "   39769,\n",
       "   1325,\n",
       "   53808,\n",
       "   43725,\n",
       "   40212,\n",
       "   24639,\n",
       "   27519,\n",
       "   10662,\n",
       "   58983],\n",
       "  'year': '2008'},\n",
       " 59796: {'paper': [1108, 38380, 27901, 46080, 12527, 50210], 'year': '2008'},\n",
       " 28057: {'paper': [53817,\n",
       "   33345,\n",
       "   20987,\n",
       "   3422,\n",
       "   61827,\n",
       "   18218,\n",
       "   24612,\n",
       "   55539,\n",
       "   12527],\n",
       "  'year': '2008'},\n",
       " 35456: {'paper': [2932, 53503, 29864, 52082, 47654, 6071, 10944],\n",
       "  'year': '2008'},\n",
       " 11999: {'paper': [55886], 'year': '2008'},\n",
       " 35837: {'paper': [39047, 40095, 46376, 42592], 'year': '2008'},\n",
       " 3436: {'paper': [12195, 25844, 56633], 'year': '2005'},\n",
       " 31904: {'paper': [338, 21898, 35386, 58914, 56472, 27908, 246, 21305],\n",
       "  'year': '2005'},\n",
       " 61968: {'paper': [11893, 47428, 50245], 'year': '2005'},\n",
       " 42051: {'paper': [54354, 33569, 65640], 'year': '2005'},\n",
       " 32010: {'paper': [36420], 'year': '2005'},\n",
       " 22823: {'paper': [33172, 22823, 54545, 11029], 'year': '2005'},\n",
       " 9952: {'paper': [20348, 38228], 'year': '2005'},\n",
       " 21741: {'paper': [25844, 40708, 23673, 10392, 39128], 'year': '2005'},\n",
       " 64967: {'paper': [], 'year': '2000'},\n",
       " 25693: {'paper': [11711, 51218], 'year': '2005'},\n",
       " 52907: {'paper': [24102, 50324, 542, 38400, 16866, 34641], 'year': '2005'},\n",
       " 60014: {'paper': [296], 'year': '2005'},\n",
       " 18183: {'paper': [51860,\n",
       "   7433,\n",
       "   64619,\n",
       "   28525,\n",
       "   25262,\n",
       "   29377,\n",
       "   10696,\n",
       "   32925,\n",
       "   11514,\n",
       "   40308,\n",
       "   49424,\n",
       "   34781],\n",
       "  'year': '2006'},\n",
       " 38382: {'paper': [12360, 2029, 8575, 1230, 59728, 35134, 22932],\n",
       "  'year': '2008'},\n",
       " 33613: {'paper': [60919, 22333, 46981, 37679], 'year': '2002'},\n",
       " 65764: {'paper': [1166], 'year': '2002'},\n",
       " 42817: {'paper': [], 'year': '2000'},\n",
       " 5: {'paper': [819], 'year': '2000'},\n",
       " 6: {'paper': [], 'year': '2000'},\n",
       " 27753: {'paper': [], 'year': '2001'},\n",
       " 7: {'paper': [], 'year': '2000'},\n",
       " 3430: {'paper': [49697, 12840], 'year': '2007'},\n",
       " 16556: {'paper': [14233], 'year': '2007'},\n",
       " 13681: {'paper': [20802, 22840], 'year': '2007'},\n",
       " 36742: {'paper': [39426, 45876, 44971, 55593], 'year': '2007'},\n",
       " 49168: {'paper': [12527, 50210, 1331], 'year': '2007'},\n",
       " 47422: {'paper': [58497, 44729, 58193, 1323, 36462, 43256], 'year': '2007'},\n",
       " 36304: {'paper': [58918, 58270], 'year': '2007'},\n",
       " 15149: {'paper': [58130, 44449, 50914, 44368, 36513], 'year': '2007'},\n",
       " 36078: {'paper': [58497, 25297, 36462, 31831, 24203, 44729], 'year': '2007'},\n",
       " 61199: {'paper': [36927, 1101, 31249], 'year': '2007'},\n",
       " 39431: {'paper': [49911, 28153], 'year': '2007'},\n",
       " 41452: {'paper': [1320, 27195], 'year': '2007'},\n",
       " 43382: {'paper': [7084], 'year': '2007'},\n",
       " 52702: {'paper': [1339], 'year': '2007'},\n",
       " 21656: {'paper': [3388, 50848, 41135, 4870], 'year': '2007'},\n",
       " 26636: {'paper': [38198, 5526], 'year': '2007'},\n",
       " 18642: {'paper': [3388, 4870], 'year': '2007'},\n",
       " 58673: {'paper': [36927, 24305, 42333, 60430], 'year': '2007'},\n",
       " 43199: {'paper': [36285, 2743, 43922, 1305], 'year': '2007'},\n",
       " 44931: {'paper': [42333, 31355, 33606], 'year': '2007'},\n",
       " 17219: {'paper': [1101, 42333, 51152, 56859], 'year': '2007'},\n",
       " 50603: {'paper': [13714, 48996, 1320], 'year': '2007'},\n",
       " 53254: {'paper': [65234, 26329, 19244, 49246], 'year': '2007'},\n",
       " 51170: {'paper': [47267, 36927, 42333], 'year': '2007'},\n",
       " 6449: {'paper': [26132, 1304, 58420], 'year': '2007'},\n",
       " 40239: {'paper': [26132, 5745, 56474], 'year': '2007'},\n",
       " 30220: {'paper': [45525, 10046], 'year': '2007'},\n",
       " 4926: {'paper': [37780], 'year': '2007'},\n",
       " 19910: {'paper': [1331, 60430, 678, 65817, 57188], 'year': '2007'},\n",
       " 3822: {'paper': [48996,\n",
       "   34309,\n",
       "   13514,\n",
       "   64727,\n",
       "   1320,\n",
       "   33522,\n",
       "   27195,\n",
       "   36269,\n",
       "   32210],\n",
       "  'year': '2007'},\n",
       " 19201: {'paper': [], 'year': '2007'},\n",
       " 50369: {'paper': [64392], 'year': '2007'},\n",
       " 21511: {'paper': [], 'year': '2007'},\n",
       " 34868: {'paper': [57188, 40648, 30684], 'year': '2007'},\n",
       " 48900: {'paper': [35393, 59491, 44093, 35820, 7114, 5146], 'year': '2007'},\n",
       " 7249: {'paper': [47075, 62718, 50210, 36974, 1105, 29749], 'year': '2007'},\n",
       " 10833: {'paper': [47075, 38380, 48683], 'year': '2007'},\n",
       " 57141: {'paper': [], 'year': '2007'},\n",
       " 50339: {'paper': [1745, 1339], 'year': '2007'},\n",
       " 60186: {'paper': [6812, 63000], 'year': '2007'},\n",
       " 35842: {'paper': [55765], 'year': '2003'},\n",
       " 46715: {'paper': [], 'year': '2004'},\n",
       " 26847: {'paper': [], 'year': '2005'},\n",
       " 8511: {'paper': [44601], 'year': '2005'},\n",
       " 34845: {'paper': [], 'year': '2003'},\n",
       " 10854: {'paper': [], 'year': '2004'},\n",
       " 44757: {'paper': [], 'year': '2004'},\n",
       " 21993: {'paper': [35500], 'year': '2004'},\n",
       " 58771: {'paper': [37465], 'year': '2005'},\n",
       " 64195: {'paper': [21057, 44766, 64865, 53778, 10202], 'year': '2006'},\n",
       " 11008: {'paper': [58014, 35283, 46061], 'year': '2006'},\n",
       " 14218: {'paper': [], 'year': '2003'},\n",
       " 5878: {'paper': [], 'year': '2003'},\n",
       " 65609: {'paper': [], 'year': '2006'},\n",
       " 47364: {'paper': [10079], 'year': '2003'},\n",
       " 16185: {'paper': [], 'year': '2003'},\n",
       " 54122: {'paper': [], 'year': '2003'},\n",
       " 47240: {'paper': [31684], 'year': '2006'},\n",
       " 28713: {'paper': [], 'year': '2003'},\n",
       " 4309: {'paper': [], 'year': '2003'},\n",
       " 31038: {'paper': [], 'year': '2003'},\n",
       " 2774: {'paper': [], 'year': '2003'},\n",
       " 14460: {'paper': [55666, 3846, 53294, 7438], 'year': '2007'},\n",
       " 62848: {'paper': [], 'year': '2003'},\n",
       " 36085: {'paper': [], 'year': '2007'},\n",
       " 31061: {'paper': [], 'year': '2003'},\n",
       " 24021: {'paper': [15845], 'year': '2007'},\n",
       " 10251: {'paper': [31684, 47188], 'year': '2007'},\n",
       " 30248: {'paper': [], 'year': '2003'},\n",
       " 34412: {'paper': [], 'year': '2007'},\n",
       " 27688: {'paper': [47267, 58420, 53664, 26132], 'year': '2007'},\n",
       " 35183: {'paper': [49417, 1304, 36974, 46167], 'year': '2007'},\n",
       " 60724: {'paper': [58420, 53664], 'year': '2007'},\n",
       " 28035: {'paper': [27674, 65618, 38309], 'year': '2007'},\n",
       " 31002: {'paper': [43890, 24519, 28802], 'year': '2006'},\n",
       " 50907: {'paper': [190, 48393], 'year': '2006'},\n",
       " 4749: {'paper': [], 'year': '2001'},\n",
       " 8: {'paper': [], 'year': '2000'},\n",
       " 27539: {'paper': [22356, 28396, 63723, 9539], 'year': '2005'},\n",
       " 15568: {'paper': [29864, 7493, 21294, 23344, 15568, 25298, 45777],\n",
       "  'year': '2005'},\n",
       " 17484: {'paper': [26554], 'year': '2006'},\n",
       " 4179: {'paper': [24210, 36900, 3823, 23344], 'year': '2005'},\n",
       " 58554: {'paper': [190, 19530, 51776, 9700], 'year': '2006'},\n",
       " 6763: {'paper': [31534, 6031], 'year': '2005'},\n",
       " 41344: {'paper': [], 'year': '2001'},\n",
       " 52793: {'paper': [36590, 6644], 'year': '2005'},\n",
       " 27796: {'paper': [2214, 31417, 14996], 'year': '2006'},\n",
       " 4261: {'paper': [], 'year': '2006'},\n",
       " 17201: {'paper': [22356, 38951, 949], 'year': '2006'},\n",
       " 36612: {'paper': [22356, 46804, 36900, 47539], 'year': '2006'},\n",
       " 7368: {'paper': [973, 9, 47545], 'year': '2001'},\n",
       " 14190: {'paper': [43488, 17124, 30394], 'year': '2006'},\n",
       " 53275: {'paper': [33705, 43250, 61736, 37267, 948], 'year': '2006'},\n",
       " 13544: {'paper': [4749], 'year': '2002'},\n",
       " 31733: {'paper': [36590, 47545], 'year': '2004'},\n",
       " 44712: {'paper': [31733], 'year': '2006'},\n",
       " 4599: {'paper': [22356, 60377], 'year': '2005'},\n",
       " 40182: {'paper': [15453], 'year': '2004'},\n",
       " 43518: {'paper': [], 'year': '2006'},\n",
       " 31534: {'paper': [18645, 47545, 30394, 7368], 'year': '2001'},\n",
       " 30275: {'paper': [2214, 36590, 43890, 30394], 'year': '2005'},\n",
       " 3933: {'paper': [65897, 36821, 21294, 53008, 11], 'year': '2006'},\n",
       " 45147: {'paper': [190, 12776, 53008], 'year': '2006'},\n",
       " 13336: {'paper': [190, 1331, 250, 12776, 53008], 'year': '2006'},\n",
       " 29135: {'paper': [54798, 23344, 64956, 36900, 6883], 'year': '2006'},\n",
       " 27215: {'paper': [30422, 18218, 52005, 50766], 'year': '2006'},\n",
       " 34326: {'paper': [949, 22356, 47539, 6031], 'year': '2006'},\n",
       " 9: {'paper': [47545], 'year': '2000'},\n",
       " 62259: {'paper': [65962, 47545, 11, 45777], 'year': '2003'},\n",
       " 20350: {'paper': [36900], 'year': '2004'},\n",
       " 8291: {'paper': [36900, 3823], 'year': '2005'},\n",
       " 13440: {'paper': [], 'year': '2005'},\n",
       " 14517: {'paper': [47545], 'year': '2002'},\n",
       " 11582: {'paper': [45008, 14503, 6031, 6644, 37362], 'year': '2005'},\n",
       " 55677: {'paper': [16426, 30394], 'year': '2004'},\n",
       " 11311: {'paper': [16426, 31534, 36900, 48518, 47545, 30394], 'year': '2005'},\n",
       " 10: {'paper': [], 'year': '2000'},\n",
       " 8548: {'paper': [14882, 37900], 'year': '2005'},\n",
       " 2875: {'paper': [49486, 12527], 'year': '2006'},\n",
       " 57348: {'paper': [], 'year': '2006'},\n",
       " 14988: {'paper': [], 'year': '2005'},\n",
       " 31910: {'paper': [22356, 6031, 56600, 18356, 37362, 62835], 'year': '2005'},\n",
       " 45007: {'paper': [], 'year': '2001'},\n",
       " 34689: {'paper': [14988], 'year': '2006'},\n",
       " 39545: {'paper': [2214, 36590, 6644, 47545, 31802], 'year': '2005'},\n",
       " 6644: {'paper': [], 'year': '2003'},\n",
       " 43250: {'paper': [], 'year': '2001'},\n",
       " 24174: {'paper': [950], 'year': '2002'},\n",
       " 41310: {'paper': [8696], 'year': '2005'},\n",
       " 27078: {'paper': [1315, 45574], 'year': '2005'},\n",
       " 12022: {'paper': [17012, 9539, 65897, 11, 63723], 'year': '2006'},\n",
       " 62095: {'paper': [24210], 'year': '2006'},\n",
       " 57410: {'paper': [47545, 39545, 50233, 16], 'year': '2006'},\n",
       " 45539: {'paper': [22356, 1244, 16905, 47539], 'year': '2006'},\n",
       " 31429: {'paper': [707], 'year': '2001'},\n",
       " 61520: {'paper': [34804, 37404, 970, 27], 'year': '2003'},\n",
       " 60408: {'paper': [], 'year': '2001'},\n",
       " 10806: {'paper': [], 'year': '2001'},\n",
       " 16300: {'paper': [38331], 'year': '2006'},\n",
       " 11: {'paper': [47545, 36], 'year': '2000'},\n",
       " 3240: {'paper': [30275, 50857, 47545, 23344, 62835], 'year': '2006'},\n",
       " 34173: {'paper': [62831, 54782, 56665, 10808], 'year': '2006'},\n",
       " 24077: {'paper': [], 'year': '2001'},\n",
       " 49010: {'paper': [974, 30], 'year': '2005'},\n",
       " 3857: {'paper': [28438, 50114, 43890, 47545], 'year': '2004'},\n",
       " 59022: {'paper': [50114, 43890, 55677, 47545, 3857], 'year': '2005'},\n",
       " 12: {'paper': [], 'year': '2000'},\n",
       " 48832: {'paper': [40318, 48393, 63723, 58537, 55504, 9539], 'year': '2006'},\n",
       " 39722: {'paper': [31534, 47545, 36590], 'year': '2006'},\n",
       " 13: {'paper': [], 'year': '2000'},\n",
       " 56600: {'paper': [31534, 6031, 39808, 4644], 'year': '2002'},\n",
       " 2211: {'paper': [11, 50942, 65897, 51525, 45777], 'year': '2006'},\n",
       " 39132: {'paper': [65703, 64791, 31759, 40318, 10210, 40758], 'year': '2006'},\n",
       " 64145: {'paper': [45777, 7765], 'year': '2006'},\n",
       " 14: {'paper': [], 'year': '2000'},\n",
       " 18354: {'paper': [17012,\n",
       "   7826,\n",
       "   14517,\n",
       "   36900,\n",
       "   7368,\n",
       "   23344,\n",
       "   18645,\n",
       "   47545,\n",
       "   25298,\n",
       "   7765],\n",
       "  'year': '2005'},\n",
       " 42210: {'paper': [56737], 'year': '2006'},\n",
       " 43502: {'paper': [14322, 37278, 31759, 40318, 20953, 10210, 55504, 40758],\n",
       "  'year': '2006'},\n",
       " 24943: {'paper': [11908, 11940, 45573], 'year': '2006'},\n",
       " 36590: {'paper': [31534, 47545], 'year': '2003'},\n",
       " 24519: {'paper': [47545], 'year': '2004'},\n",
       " 12864: {'paper': [12864], 'year': '2006'},\n",
       " 14503: {'paper': [108], 'year': '2003'},\n",
       " 31802: {'paper': [11, 36590, 6644, 47545, 65897], 'year': '2004'},\n",
       " 61867: {'paper': [63657], 'year': '2003'},\n",
       " 55332: {'paper': [63657, 2214, 62155, 47545, 972, 106], 'year': '2006'},\n",
       " 39860: {'paper': [972, 42683, 106], 'year': '2003'},\n",
       " 60377: {'paper': [], 'year': '2002'},\n",
       " 54556: {'paper': [37403], 'year': '2006'},\n",
       " 50278: {'paper': [34227], 'year': '2006'},\n",
       " 53007: {'paper': [1246, 951], 'year': '2004'},\n",
       " 58856: {'paper': [64791,\n",
       "   65703,\n",
       "   34227,\n",
       "   40318,\n",
       "   35428,\n",
       "   10210,\n",
       "   32941,\n",
       "   59494,\n",
       "   13688],\n",
       "  'year': '2006'},\n",
       " 65996: {'paper': [36590, 50114, 38247, 20350, 47545], 'year': '2005'},\n",
       " 3268: {'paper': [], 'year': '2001'},\n",
       " 47539: {'paper': [22356, 6031, 949], 'year': '2005'},\n",
       " 48930: {'paper': [46804, 6031, 37362, 31910, 47539], 'year': '2006'},\n",
       " 2254: {'paper': [1331, 42428, 5992, 50415, 35594, 50210, 64404],\n",
       "  'year': '2006'},\n",
       " 14765: {'paper': [], 'year': '2001'},\n",
       " 62835: {'paper': [2214, 36900, 20350, 55677, 47545, 48375, 9310, 31910],\n",
       "  'year': '2005'},\n",
       " 25733: {'paper': [11, 23344, 65897, 53008], 'year': '2005'},\n",
       " 49907: {'paper': [43493, 36590, 47545], 'year': '2004'},\n",
       " 46095: {'paper': [20350, 47545], 'year': '2005'},\n",
       " 54454: {'paper': [60377, 7338], 'year': '2006'},\n",
       " 6716: {'paper': [60377], 'year': '2004'},\n",
       " 37362: {'paper': [22356, 39808, 56600, 24174, 18356], 'year': '2004'},\n",
       " 18862: {'paper': [7201, 6071, 27076, 46507, 61484], 'year': '2006'},\n",
       " 56334: {'paper': [64554, 34804, 37404, 61520, 970, 1331, 27], 'year': '2006'},\n",
       " 51776: {'paper': [190, 64326, 29376], 'year': '2005'},\n",
       " 54990: {'paper': [64326, 707], 'year': '2002'},\n",
       " 29635: {'paper': [65703,\n",
       "   64791,\n",
       "   35428,\n",
       "   1244,\n",
       "   31759,\n",
       "   40318,\n",
       "   32941,\n",
       "   10210,\n",
       "   55504,\n",
       "   29788,\n",
       "   40758],\n",
       "  'year': '2006'},\n",
       " 18814: {'paper': [29864, 54204, 11414, 50942, 45777], 'year': '2005'},\n",
       " 15: {'paper': [], 'year': '2000'},\n",
       " 16749: {'paper': [22356, 47539, 38951], 'year': '2006'},\n",
       " 34664: {'paper': [14023], 'year': '2003'},\n",
       " 33968: {'paper': [47055], 'year': '2004'},\n",
       " 7338: {'paper': [50283, 17], 'year': '2005'},\n",
       " 38331: {'paper': [31733, 47545, 31802, 62106], 'year': '2005'},\n",
       " 6766: {'paper': [22356, 46804, 10790, 949, 66329], 'year': '2006'},\n",
       " 43082: {'paper': [21778], 'year': '2002'},\n",
       " 21262: {'paper': [29864, 23331, 15568], 'year': '2006'},\n",
       " 29045: {'paper': [22356, 11, 35510, 56600, 11582, 39545, 65897],\n",
       "  'year': '2006'},\n",
       " 25794: {'paper': [31534], 'year': '2004'},\n",
       " 35978: {'paper': [25794], 'year': '2004'},\n",
       " 51525: {'paper': [50942, 20629, 45777, 953], 'year': '2004'},\n",
       " 32941: {'paper': [64791, 65703, 27795, 10210, 47267], 'year': '2004'},\n",
       " 58537: {'paper': [65703, 41089, 10210, 47267, 32941], 'year': '2005'},\n",
       " 16: {'paper': [], 'year': '2000'},\n",
       " 19606: {'paper': [14888, 64251, 13440], 'year': '2006'},\n",
       " 17: {'paper': [1244], 'year': '2000'},\n",
       " 56665: {'paper': [62831, 43629], 'year': '2005'},\n",
       " 31064: {'paper': [190], 'year': '2006'},\n",
       " 30664: {'paper': [36590, 38247, 30275, 47545, 3857, 65996], 'year': '2006'},\n",
       " 61334: {'paper': [64251, 190, 26899], 'year': '2006'},\n",
       " 28117: {'paper': [37662, 18448], 'year': '2006'},\n",
       " 58292: {'paper': [36900, 23344, 19576, 13525, 64956, 6883], 'year': '2003'},\n",
       " 35297: {'paper': [], 'year': '2006'},\n",
       " 63068: {'paper': [6031, 27539, 66329], 'year': '2006'},\n",
       " 26757: {'paper': [4749, 23344, 15], 'year': '2002'},\n",
       " 29376: {'paper': [], 'year': '2001'},\n",
       " 16359: {'paper': [36900, 24174, 48375], 'year': '2005'},\n",
       " 3762: {'paper': [23934, 16359, 24519, 62835], 'year': '2006'},\n",
       " 22608: {'paper': [], 'year': '2006'},\n",
       " 56323: {'paper': [30275, 23934], 'year': '2006'},\n",
       " 60750: {'paper': [7684], 'year': '2006'},\n",
       " 15214: {'paper': [11, 19576, 65897, 13525], 'year': '2003'},\n",
       " 60706: {'paper': [587, 36821], 'year': '2004'},\n",
       " 7882: {'paper': [3366, 45586, 36900, 23344], 'year': '2006'},\n",
       " 59703: {'paper': [113,\n",
       "   47545,\n",
       "   2214,\n",
       "   43890,\n",
       "   18354,\n",
       "   20350,\n",
       "   55677,\n",
       "   30275,\n",
       "   24669,\n",
       "   3857],\n",
       "  'year': '2006'},\n",
       " 46445: {'paper': [190, 57006, 38222, 32063, 4983, 53008], 'year': '2006'},\n",
       " 37404: {'paper': [970, 27], 'year': '2001'},\n",
       " 57821: {'paper': [6031, 61903, 19985, 39042], 'year': '2006'},\n",
       " 11384: {'paper': [65703, 40318, 31759, 55504, 32941], 'year': '2006'},\n",
       " 44168: {'paper': [52303], 'year': '2004'},\n",
       " 2453: {'paper': [22356, 16905, 4644], 'year': '2006'},\n",
       " 38708: {'paper': [46804], 'year': '2006'},\n",
       " 2517: {'paper': [15214, 57366, 61867], 'year': '2005'},\n",
       " 6031: {'paper': [47545, 949], 'year': '2001'},\n",
       " 60573: {'paper': [50942, 5369, 45777], 'year': '2005'},\n",
       " 56811: {'paper': [47545, 5369, 39082, 45777, 22135, 53008], 'year': '2006'},\n",
       " 22135: {'paper': [50942, 45777, 11894], 'year': '2004'},\n",
       " 50942: {'paper': [20629, 45777, 11894], 'year': '2003'},\n",
       " 59619: {'paper': [33743, 37362, 45008, 46804, 66329, 6031], 'year': '2007'},\n",
       " 13629: {'paper': [47539, 18356, 6031], 'year': '2006'},\n",
       " 62310: {'paper': [17124, 30275], 'year': '2006'},\n",
       " 34638: {'paper': [31534, 36821, 53008], 'year': '2006'},\n",
       " 5874: {'paper': [34018, 15611, 21779, 37202], 'year': '2007'},\n",
       " 2236: {'paper': [9310, 65672, 11513, 50233, 23772], 'year': '2007'},\n",
       " 39086: {'paper': [2214,\n",
       "   47545,\n",
       "   31910,\n",
       "   36590,\n",
       "   23344,\n",
       "   65996,\n",
       "   62835,\n",
       "   30664,\n",
       "   31303],\n",
       "  'year': '2007'},\n",
       " 18: {'paper': [], 'year': '2000'},\n",
       " 19: {'paper': [26134], 'year': '2000'},\n",
       " 20: {'paper': [], 'year': '2000'},\n",
       " 12167: {'paper': [46884, 4770], 'year': '2005'},\n",
       " 24878: {'paper': [34353], 'year': '2008'},\n",
       " 58216: {'paper': [], 'year': '2008'},\n",
       " 50334: {'paper': [8365, 31684, 9073], 'year': '2008'},\n",
       " 55542: {'paper': [24898, 33893, 2178, 41541], 'year': '2008'},\n",
       " 48123: {'paper': [23229], 'year': '2008'},\n",
       " 55462: {'paper': [1413, 60205], 'year': '2008'},\n",
       " 6913: {'paper': [20468, 29345, 63011, 37941, 34607], 'year': '2008'},\n",
       " 57467: {'paper': [32722], 'year': '2008'},\n",
       " 23798: {'paper': [44866, 31418], 'year': '2008'},\n",
       " 28504: {'paper': [55408, 53294, 50674, 12399, 37232], 'year': '2008'},\n",
       " 41851: {'paper': [604, 41851], 'year': '2005'},\n",
       " 7496: {'paper': [], 'year': '2002'},\n",
       " 35091: {'paper': [11388], 'year': '2005'},\n",
       " 58825: {'paper': [], 'year': '2005'},\n",
       " 11388: {'paper': [], 'year': '2002'},\n",
       " 63357: {'paper': [], 'year': '2004'},\n",
       " 54124: {'paper': [6740, 56299, 19534, 36632], 'year': '2004'},\n",
       " 38996: {'paper': [], 'year': '2004'},\n",
       " 29387: {'paper': [66470, 18897, 46901], 'year': '2005'},\n",
       " 23019: {'paper': [4259], 'year': '2005'},\n",
       " 63942: {'paper': [60336], 'year': '2003'},\n",
       " 47996: {'paper': [], 'year': '2001'},\n",
       " 65371: {'paper': [9295, 941], 'year': '2005'},\n",
       " 45116: {'paper': [18040, 43425], 'year': '2003'},\n",
       " 41342: {'paper': [740, 63734], 'year': '2003'},\n",
       " 62478: {'paper': [], 'year': '2001'},\n",
       " 52961: {'paper': [], 'year': '2003'},\n",
       " 47876: {'paper': [1522, 62478], 'year': '2002'},\n",
       " 44742: {'paper': [1563], 'year': '2003'},\n",
       " 40444: {'paper': [], 'year': '2003'},\n",
       " 16085: {'paper': [20235, 13658, 42437, 56913, 16178], 'year': '2005'},\n",
       " 53543: {'paper': [], 'year': '2002'},\n",
       " 61170: {'paper': [51514, 39768, 52941], 'year': '2004'},\n",
       " 3735: {'paper': [740, 53245], 'year': '2005'},\n",
       " 15475: {'paper': [673, 54834, 48158, 11326, 38252, 28165, 10584],\n",
       "  'year': '2005'},\n",
       " 24214: {'paper': [65848, 19387, 728, 30052], 'year': '2004'},\n",
       " 2569: {'paper': [], 'year': '2001'},\n",
       " 63028: {'paper': [], 'year': '2003'},\n",
       " 23070: {'paper': [28854, 24790, 68, 30034, 496, 503], 'year': '2004'},\n",
       " 20919: {'paper': [4535,\n",
       "   24790,\n",
       "   68,\n",
       "   556,\n",
       "   550,\n",
       "   33278,\n",
       "   63172,\n",
       "   9440,\n",
       "   26530,\n",
       "   37408],\n",
       "  'year': '2005'},\n",
       " 55765: {'paper': [], 'year': '2002'},\n",
       " 11338: {'paper': [470, 17027], 'year': '2005'},\n",
       " 7706: {'paper': [], 'year': '2001'},\n",
       " 10590: {'paper': [50568, 39768, 3109, 34227], 'year': '2003'},\n",
       " 29414: {'paper': [40106, 60427, 498], 'year': '2005'},\n",
       " 27491: {'paper': [376, 62427], 'year': '2004'},\n",
       " 41836: {'paper': [51297, 54, 23294, 59771], 'year': '2005'},\n",
       " 59406: {'paper': [28604, 10292, 17399, 21753, 3173, 8578, 3702],\n",
       "  'year': '2007'},\n",
       " 46933: {'paper': [32133, 28990], 'year': '2003'},\n",
       " 45775: {'paper': [], 'year': '2003'},\n",
       " 28610: {'paper': [46933, 32133, 48570], 'year': '2004'},\n",
       " 7701: {'paper': [1712, 61047, 28610, 48570], 'year': '2005'},\n",
       " 50704: {'paper': [36777], 'year': '2003'},\n",
       " 26741: {'paper': [], 'year': '2003'},\n",
       " 45353: {'paper': [45721], 'year': '2006'},\n",
       " 28990: {'paper': [32133], 'year': '2002'},\n",
       " 29648: {'paper': [32831, 43838, 10152, 30404], 'year': '2008'},\n",
       " 49066: {'paper': [12928], 'year': '2006'},\n",
       " 50559: {'paper': [1887, 35551, 4259, 64861, 41786], 'year': '2006'},\n",
       " 15350: {'paper': [707, 24034, 1140, 43147], 'year': '2006'},\n",
       " 34790: {'paper': [], 'year': '2008'},\n",
       " 43373: {'paper': [28476, 6675, 3906, 26202, 28610, 7701, 48570, 66467],\n",
       "  'year': '2006'},\n",
       " 32133: {'paper': [], 'year': '2001'},\n",
       " 64775: {'paper': [29864,\n",
       "   54374,\n",
       "   52619,\n",
       "   24599,\n",
       "   40643,\n",
       "   14534,\n",
       "   1917,\n",
       "   3422,\n",
       "   50714,\n",
       "   28277,\n",
       "   55198,\n",
       "   43134,\n",
       "   61484],\n",
       "  'year': '2008'},\n",
       " 43003: {'paper': [9433,\n",
       "   62249,\n",
       "   36007,\n",
       "   62690,\n",
       "   32600,\n",
       "   36775,\n",
       "   52241,\n",
       "   19591,\n",
       "   1356,\n",
       "   28915,\n",
       "   36069,\n",
       "   30430,\n",
       "   44260,\n",
       "   62235,\n",
       "   7499,\n",
       "   4436],\n",
       "  'year': '2008'},\n",
       " 10580: {'paper': [1360, 3200, 2342, 31577], 'year': '2006'},\n",
       " 16385: {'paper': [11827, 55132], 'year': '2008'},\n",
       " 39626: {'paper': [], 'year': '2006'},\n",
       " 45662: {'paper': [63584, 44394, 4564, 4571, 25242, 62118], 'year': '2007'},\n",
       " 40062: {'paper': [50704, 36777, 138, 21368], 'year': '2007'},\n",
       " 33240: {'paper': [731, 41911, 1357], 'year': '2006'},\n",
       " 38276: {'paper': [59597], 'year': '2008'},\n",
       " 30446: {'paper': [19982,\n",
       "   756,\n",
       "   14571,\n",
       "   39256,\n",
       "   32296,\n",
       "   33259,\n",
       "   23626,\n",
       "   64850,\n",
       "   41042,\n",
       "   56866,\n",
       "   30356,\n",
       "   37226,\n",
       "   15445],\n",
       "  'year': '2007'},\n",
       " 65104: {'paper': [], 'year': '2001'},\n",
       " 9821: {'paper': [13637, 2384], 'year': '2005'},\n",
       " 58961: {'paper': [], 'year': '2004'},\n",
       " 19659: {'paper': [7257], 'year': '2007'},\n",
       " 34118: {'paper': [51678], 'year': '2008'},\n",
       " 19212: {'paper': [19212], 'year': '2006'},\n",
       " 66023: {'paper': [51678, 26292, 42212, 66023], 'year': '2006'},\n",
       " 38297: {'paper': [44758, 1523, 45950, 54070], 'year': '2006'},\n",
       " 1865: {'paper': [12601, 48696, 123], 'year': '2003'},\n",
       " 64277: {'paper': [23288, 47424, 14620, 55464], 'year': '2006'},\n",
       " 14102: {'paper': [58887, 58197, 52449], 'year': '2006'},\n",
       " 40579: {'paper': [1647, 11037, 19934, 40909, 18897, 20256, 37951, 46901],\n",
       "  'year': '2006'},\n",
       " 28783: {'paper': [26588, 53464, 4656], 'year': '2006'},\n",
       " 65925: {'paper': [1332], 'year': '2004'},\n",
       " 8403: {'paper': [34146], 'year': '2003'},\n",
       " 34146: {'paper': [8403], 'year': '2004'},\n",
       " 32500: {'paper': [36214], 'year': '2005'},\n",
       " 47386: {'paper': [30052, 16299, 1057, 1503, 24214], 'year': '2004'},\n",
       " 35370: {'paper': [], 'year': '2006'},\n",
       " 13009: {'paper': [33845,\n",
       "   39375,\n",
       "   18866,\n",
       "   25845,\n",
       "   6617,\n",
       "   62342,\n",
       "   18079,\n",
       "   48970,\n",
       "   13087,\n",
       "   65091,\n",
       "   53384],\n",
       "  'year': '2004'},\n",
       " 57905: {'paper': [], 'year': '2003'},\n",
       " 56079: {'paper': [], 'year': '2003'},\n",
       " 66129: {'paper': [24214, 56094, 60010], 'year': '2007'},\n",
       " 15940: {'paper': [28595, 1613], 'year': '2005'},\n",
       " 56006: {'paper': [7073, 1079], 'year': '2003'},\n",
       " 65807: {'paper': [64732, 56751], 'year': '2004'},\n",
       " 29097: {'paper': [728, 16700, 19729, 56094], 'year': '2007'},\n",
       " 57393: {'paper': [], 'year': '2003'},\n",
       " 36625: {'paper': [4731, 50443, 19025, 60071, 15057, 4235], 'year': '2007'},\n",
       " 15715: {'paper': [65348, 7933, 19604, 976], 'year': '2007'},\n",
       " 9418: {'paper': [], 'year': '2007'},\n",
       " 36097: {'paper': [9073, 18897, 46901], 'year': '2007'},\n",
       " 56673: {'paper': [740], 'year': '2004'},\n",
       " 20194: {'paper': [1351, 538, 555, 41053], 'year': '2003'},\n",
       " 43043: {'paper': [], 'year': '2003'},\n",
       " 17719: {'paper': [338, 61048], 'year': '2003'},\n",
       " 16060: {'paper': [1626], 'year': '2003'},\n",
       " 64260: {'paper': [41246, 31610, 36379], 'year': '2003'},\n",
       " 3886: {'paper': [39339], 'year': '2007'},\n",
       " 31785: {'paper': [24899, 338], 'year': '2007'},\n",
       " 39186: {'paper': [], 'year': '2007'},\n",
       " 3225: {'paper': [6938, 17782, 47742, 22973], 'year': '2007'},\n",
       " 17397: {'paper': [61595], 'year': '2007'},\n",
       " 56641: {'paper': [552, 956, 28895], 'year': '2007'},\n",
       " 10551: {'paper': [66499, 32753], 'year': '2003'},\n",
       " 3361: {'paper': [61515, 61153, 25867, 15477, 60898], 'year': '2007'},\n",
       " 29096: {'paper': [18816, 61833, 64656], 'year': '2007'},\n",
       " 50323: {'paper': [64861, 3897, 43181, 24958, 46181, 60747, 21210],\n",
       "  'year': '2007'},\n",
       " 18060: {'paper': [24247, 748, 53884, 18188], 'year': '2007'},\n",
       " 33791: {'paper': [14495, 51336, 34640], 'year': '2007'},\n",
       " 40702: {'paper': [51829, 20836, 46591], 'year': '2007'},\n",
       " 51172: {'paper': [40381, 39878, 34356, 49810], 'year': '2007'},\n",
       " 9969: {'paper': [65374, 8424, 26849], 'year': '2007'},\n",
       " 31458: {'paper': [35849, 31655, 25863, 24410, 47014], 'year': '2007'},\n",
       " 30424: {'paper': [22881, 55969, 55109, 8099, 14171, 5433, 41651, 51752],\n",
       "  'year': '2007'},\n",
       " 9037: {'paper': [36442, 54170, 43844, 28351, 31865, 26666, 3792, 28188, 3300],\n",
       "  'year': '2007'},\n",
       " 61573: {'paper': [12987, 14626, 25052, 16572, 58614], 'year': '2007'},\n",
       " 19885: {'paper': [59964], 'year': '2007'},\n",
       " 34637: {'paper': [49531, 35524, 60639, 16946, 47692, 759, 39427],\n",
       "  'year': '2007'},\n",
       " 43329: {'paper': [], 'year': '2001'},\n",
       " 22898: {'paper': [19934], 'year': '2004'},\n",
       " 35597: {'paper': [], 'year': '2002'},\n",
       " 63515: {'paper': [50568, 698], 'year': '2002'},\n",
       " 61557: {'paper': [10864, 61961, 12939], 'year': '2005'},\n",
       " 49505: {'paper': [1647], 'year': '2005'},\n",
       " 60967: {'paper': [], 'year': '2001'},\n",
       " 4321: {'paper': [829,\n",
       "   8424,\n",
       "   585,\n",
       "   30308,\n",
       "   62716,\n",
       "   25451,\n",
       "   66499,\n",
       "   17973,\n",
       "   32753,\n",
       "   52423],\n",
       "  'year': '2005'},\n",
       " 4717: {'paper': [], 'year': '2001'},\n",
       " 63906: {'paper': [12612], 'year': '2003'},\n",
       " 25742: {'paper': [40643, 699, 56695], 'year': '2004'},\n",
       " 39849: {'paper': [40643, 20107, 53817, 32434], 'year': '2004'},\n",
       " 51948: {'paper': [], 'year': '2003'},\n",
       " 59397: {'paper': [26329], 'year': '2004'},\n",
       " 37359: {'paper': [14453, 740], 'year': '2005'},\n",
       " 38388: {'paper': [], 'year': '2002'},\n",
       " 28891: {'paper': [1357], 'year': '2002'},\n",
       " 65931: {'paper': [], 'year': '2001'},\n",
       " 55799: {'paper': [740, 25802, 37284], 'year': '2003'},\n",
       " 45435: {'paper': [46569, 31370, 866, 22802], 'year': '2005'},\n",
       " 23879: {'paper': [16892, 34227, 5384], 'year': '2005'},\n",
       " 49917: {'paper': [866], 'year': '2003'},\n",
       " 64409: {'paper': [731, 51184, 62342, 7922, 17413], 'year': '2004'},\n",
       " 34880: {'paper': [], 'year': '2002'},\n",
       " 44708: {'paper': [11326], 'year': '2003'},\n",
       " 50043: {'paper': [26329], 'year': '2003'},\n",
       " 65334: {'paper': [192, 34499, 63156, 53246], 'year': '2003'},\n",
       " 29939: {'paper': [756, 26329, 33290], 'year': '2005'},\n",
       " 66132: {'paper': [], 'year': '2002'},\n",
       " 44974: {'paper': [31865, 23778, 25685], 'year': '2004'},\n",
       " 63406: {'paper': [60731, 44206, 54266, 57279, 33674], 'year': '2004'},\n",
       " 7608: {'paper': [18699], 'year': '2003'},\n",
       " 20749: {'paper': [59588, 28553], 'year': '2004'},\n",
       " 25129: {'paper': [62118, 15919], 'year': '2004'},\n",
       " 20116: {'paper': [733], 'year': '2002'},\n",
       " 42046: {'paper': [], 'year': '2003'},\n",
       " 29479: {'paper': [44167, 64208], 'year': '2004'},\n",
       " 4715: {'paper': [122], 'year': '2002'},\n",
       " 7867: {'paper': [], 'year': '2001'},\n",
       " 19422: {'paper': [], 'year': '2001'},\n",
       " 55622: {'paper': [], 'year': '2001'},\n",
       " 5368: {'paper': [], 'year': '2001'},\n",
       " 17102: {'paper': [], 'year': '2002'},\n",
       " 28700: {'paper': [], 'year': '2005'},\n",
       " 3796: {'paper': [19272, 51948, 1356, 59529], 'year': '2005'},\n",
       " 7815: {'paper': [740], 'year': '2003'},\n",
       " 7013: {'paper': [756], 'year': '2002'},\n",
       " 58594: {'paper': [], 'year': '2001'},\n",
       " 47702: {'paper': [], 'year': '2002'},\n",
       " 37061: {'paper': [579, 27892, 20893, 12340, 866], 'year': '2003'},\n",
       " 6142: {'paper': [22802, 24851, 4005], 'year': '2008'},\n",
       " 31938: {'paper': [], 'year': '2002'},\n",
       " 34331: {'paper': [], 'year': '2001'},\n",
       " 56977: {'paper': [740, 34227], 'year': '2002'},\n",
       " 16337: {'paper': [38837, 1451, 44708, 854], 'year': '2004'},\n",
       " 50517: {'paper': [736, 759], 'year': '2004'},\n",
       " ...}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_citation_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ed64793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "years = ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010']\n",
    "graph_list = []\n",
    "for year in years:\n",
    "    G_author = nx.Graph()\n",
    "    G_citation = nx.Graph()\n",
    "    G_author.add_nodes_from(list(idx_2_paper_index.keys()))\n",
    "    G_author.add_nodes_from(list(idx_2_author_name.keys()))\n",
    "    G_citation.add_nodes_from(list(idx_2_paper_index.keys()))\n",
    "    G_citation.add_nodes_from(list(idx_2_author_name.keys()))\n",
    "\n",
    "    for k,v in paper_author_idx.items():\n",
    "        if v['year'] == year:\n",
    "            for author in v['author']:\n",
    "                G_author.add_edge(k, author)\n",
    "\n",
    "    for k,v in paper_citation_idx.items():\n",
    "        if v['year'] == year:\n",
    "            for paper in v['paper']:\n",
    "                G_citation.add_edge(k, paper)\n",
    "    graph_list.append([G_author, G_citation, idx_2_paper_index, idx_2_author_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a4756aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000  Author edges added this year: 5591  Citation edges added this year: 700\n",
      "2001  Author edges added this year: 7289  Citation edges added this year: 2409\n",
      "2002  Author edges added this year: 11369  Citation edges added this year: 5863\n",
      "2003  Author edges added this year: 14973  Citation edges added this year: 11031\n",
      "2004  Author edges added this year: 15720  Citation edges added this year: 15673\n",
      "2005  Author edges added this year: 18381  Citation edges added this year: 22307\n",
      "2006  Author edges added this year: 21897  Citation edges added this year: 29890\n",
      "2007  Author edges added this year: 31517  Citation edges added this year: 46847\n",
      "2008  Author edges added this year: 31171  Citation edges added this year: 50386\n",
      "2009  Author edges added this year: 32528  Citation edges added this year: 56900\n",
      "2010  Author edges added this year: 8247  Citation edges added this year: 15060\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(graph_list)):\n",
    "    print(years[i] + '  Author edges added this year: ' + str(nx.number_of_edges(graph_list[i][0])) + '  Citation edges added this year: ' + str(nx.number_of_edges(graph_list[i][1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "396d0753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000  Snapshot nodes this year: 140896\n",
      "2001  Snapshot nodes this year: 140896\n",
      "2002  Snapshot nodes this year: 140896\n",
      "2003  Snapshot nodes this year: 140896\n",
      "2004  Snapshot nodes this year: 140896\n",
      "2005  Snapshot nodes this year: 140896\n",
      "2006  Snapshot nodes this year: 140896\n",
      "2007  Snapshot nodes this year: 140896\n",
      "2008  Snapshot nodes this year: 140896\n",
      "2009  Snapshot nodes this year: 140896\n",
      "2010  Snapshot nodes this year: 140896\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(graph_list)):\n",
    "    print(years[i] + '  Snapshot nodes this year: ' + str(nx.number_of_nodes(graph_list[i][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0065bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/dblp_paper/graph_input.pkl', 'wb') as f:\n",
    "    pickle.dump(graph_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fe5e0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_num = {}\n",
    "for year in years:\n",
    "    tmp = []\n",
    "    for k,v in paper_citation_idx.items():\n",
    "        if v['year'] == year:\n",
    "            tmp.append(k)\n",
    "    paper_num[year] = len(list(set(tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3e022a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000  Papers published this year: 2102\n",
      "2001  Papers published this year: 2703\n",
      "2002  Papers published this year: 4155\n",
      "2003  Papers published this year: 5333\n",
      "2004  Papers published this year: 5390\n",
      "2005  Papers published this year: 6208\n",
      "2006  Papers published this year: 7299\n",
      "2007  Papers published this year: 10377\n",
      "2008  Papers published this year: 10081\n",
      "2009  Papers published this year: 10290\n",
      "2010  Papers published this year: 2563\n"
     ]
    }
   ],
   "source": [
    "for k,v in paper_num.items():\n",
    "    print(k + '  Papers published this year: ' + str(v))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19805eac",
   "metadata": {},
   "source": [
    "## DBLP paper text content (doc_input.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ac8958b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_index_list = []\n",
    "for item in citation_count_list:\n",
    "    if item[0] not in paper_index_list:\n",
    "        paper_index_list.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8a3d65a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1749"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "58d0b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_input = []\n",
    "for paper_index in paper_index_list:\n",
    "    tmp_item = []\n",
    "    for paper in paper_list_filter:\n",
    "        if paper['index'] == paper_index:\n",
    "            tmp_item.append([[paper['title'], paper['abstract']], paper['year']])\n",
    "    doc_input.append(tmp_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f2c98b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['Video Artifacts for Design: Bridging the Gap Between Abstraction and Detail.',\n",
       "    'Video artifacts help bridge the gap between abstraction and detail in the design process. This paper describes how our use and re-use of video artifacts affected the re-design of a graphical editor for building, simulating, and analyzing Coloured Petri Nets. The two primary goals of the project were to create design abstractions that integrate recent advances in graphical interaction techniques and to explicitly support specific patterns of use of Petri nets in real-world settings.Using a participatory design process, we organized a series of video-based design activities that helped us manage the tension between finding useful design abstractions and specifying the details of the user interface. Video artifacts resulting from one activity became the basis for the next, facilitating communication among members of the multi-disciplinary design team. The video artifacts provided an efficient way of capturing and incorporating subtle aspects of Petri Nets In Use into our design and ensured that the implementation of our design principles was grounded in real-world work practices.'],\n",
       "   '2000']],\n",
       " [[['PaperButtons: Expanding a Tangible User Interface.',\n",
       "    'Expanding the functionality of a successful system is always a challenge; the initial simplicity and ease-of-use is easily lost in the process. Experience indicates that this problem is worsened in systems with tangible interfaces: while it might be relatively easy to suggest a single successful tangible interaction component, it is notoriously hard to preserve the success when expanding with more components or more manipulation using the same component. This paper describes our approach to creating and expanding tangible interfaces. The approach consist of adherence to a set of guidelines for tangible interfaces, derived from practical tangible design and general object-oriented design, and solicitation of user requirements to the particular interaction method in question. Finally the paper describes a prototype of PaperButtons built in response to these requirements and designed in accordance to the guidelines for tangible interfaces.'],\n",
       "   '2000']],\n",
       " [[['Constraint animation using an object-oriented declarative language.',\n",
       "    \"Prototypes can be an effective way of interacting with an end-user to validate that the user's requirements have been correctly captured. In the formal methods community, specification animation has been investigated as a way of creating a kind of prototype that is generated from a formal specification. Enriching UML diagrams with OCL constraints can provide the formality that is needed to animate the diagrams without the need for a more rigorous formal specification language. This paper provides an overview of issues concerning specification animation and describes an initial attempt at an animation environment for UML/OCL. We translate the UML/OCL into an object-oriented declarative language, Prolog++, and utilize a primitive animation environment that allows both a developer and client to explore the validity of the specification. In particular, in this paper we focus on animating the effect of constraints.\"],\n",
       "   '2000']],\n",
       " [[['Data Relation Vectors: A New Abstraction for Data Optimizations.',\n",
       "    'This paper presents an abstraction called data relation vectors to improve the data access characteristics and memory layouts in a given regular computation. The key idea is to define a relation between the data elements accessed by close-by iterations and use this relation to guide to a number of optimizations for array-based computations. The specific optimizations studied in this paper include enhancing group-spatial and self-spatial reuses, improving intra-tile and inter-tile reuses, and reducing unnecessary communication on message-passing architectures. In addition, this abstraction works well with other known abstractions such as data reuse vectors. The data relation vector abstraction has been implemented in the SUIF compilation framework and has been tested using a set of twelve benchmarks from image processing and scientific computation domains. Preliminary results on a super-scalar processor show that it is successful in reducing compilation time and outperforms two previously proposed techniques, one that uses only loop transformations and one that uses both loop and data transformations. Our experiments also show that the proposed abstraction helps one to select good data tile shapes, which can subsequently be used to determine iteration space tiles.'],\n",
       "   '2000']],\n",
       " [[['Local Search with Constraint Propagation and Conflict-Based Heuristics.',\n",
       "    'Search algorithms for solving CSP (Constraint Satisfaction Problems) usually fall into one of two main families: local search algorithms and systematic algorithms. Both families have their advantages. Designing hybrid approaches seems promising since those advantages may be combined into a single approach. In this paper, we present a new hybrid technique. It performs a local search over partial assignments instead of complete assignments, and uses filtering techniques and conflict-based techniques to efficiently guide the search. This new technique benefits from both classical approaches: a priori pruning of the search space from filtering-based search and possible repair of early mistakes from local search. We focus on a specific version of this technique: tabu decision-repair. Experiments done on open-shop scheduling problems show that our approach competes well with the best highly specialized algorithms.'],\n",
       "   '2000']],\n",
       " [[['Exact and Approximate Testing/Correcting of Algebraic Functions: A Survey.',\n",
       "    \"In the late 80's Blum, Luby, Rubinfeld, Kannan et al. pioneered the theory of self-testing as an alternative way of dealing with the problem of software reliability. Over the last decade this theory played a crucial role in the construction of probabilistically checkable proofs and the derivation of hardness of approximation results. Applications in areas like computer vision, machine learning, and self-correcting programs were also established.In the self-testing problem one is interested in determining (maybe probabilistically) whether a function to which one has oracle access satisfies a given property. We consider the problem of testing algebraic functions and survey over a decade of research in the area. Special emphasis is given to illustrate the scenario where the problem takes place and to the main techniques used in the analysis of tests. A novel aspect of this work is the separation it advocates between the mathematical and algorithmic issues that arise in the theory of self-testing.\"],\n",
       "   '2000']],\n",
       " [[['The Regularity Lemma and Its Applications in Graph Theory.',\n",
       "    \"Szemerédi's Regularity Lemma is an important tool in discrete mathematics. It says that, in some sense, all graphs can be approximated by random-looking graphs. Therefore the lemma helps in proving theorems for arbitrary graphs whenever the corresponding result is easy for random graphs. In the last few years more and more new results were obtained by using the Regularity Lemma, and Mso some new variants and generalizations appeared. Komlós and Simonovits have written a survey on the topic [96]. The present survey is, in a sense, a continuation of the earlier survey. Here we describe some sample applications and generalizations. To keep the paper self-contained we decided to repeat (sometimes in a shortened form) parts of the first survey, but the emphasis is on new results.\"],\n",
       "   '2000']],\n",
       " [[['Graph-Theoretical Methods in Computer Vision.',\n",
       "    'The management of large databases of hierarchical (e.g., multi-scale or multilevel) image features is a common problem in object recognition. Such structures are often represented as trees or directed acyclic graphs (DAGs), where nodes represent image feature abstractions and arcs represent spatial relations, mappings across resolution levels, component parts, etc. Object recognition consists of two processes: indexing and verification. In the indexing process, a collection of one or more extracted image features belonging to an object is used to select, from a large database of object models, a small set of candidates likely to contain the object. Given this relatively small set of candidates, a verification, or matching procedure is used to select the most promising candidate. Such matching problems can be formulated as largest isomorphic subgraph or largest isomorphic subtree problems, for which a wealth of literature exists in the graph algorithms community. However, the nature of the vision instantiation of this problem often precludes the direct application of these methods. Due to occlusion and noise, no significant isomorphisms may exists between two graphs or trees. In this paper, we review our application of spectral encoding of a graph for indexing to large database of image features represented as DAGs. We will also review a more general class of matching methods, called bipartite matching, to two problems in object recognition.'],\n",
       "   '2000']],\n",
       " [[['Headline Generation Based on Statistical Translation.',\n",
       "    'Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required. An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding. A more practical approach to this problem results in the use of an approximation: viewing summarization as a problem analogous to statistical machine translation. The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language. This paper presents results on experiments using this approach, in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus.'],\n",
       "   '2000']],\n",
       " [[['Statistical Parsing with an Automatically-Extracted Tree Adjoining Grammar.',\n",
       "    'We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance. We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.'],\n",
       "   '2000']],\n",
       " [[['Language Independent, Minimally Supervised Induction of Lexical Probabilities.',\n",
       "    'A central problem in part-of-speech tagging, especially for new languages for which limited annotated resources are available, is estimating the distribution of lexical probabilities for unknown words. This paper introduces a new paradigmatic similarity measure and presents a minimally supervised learning approach combining effective selection and weighting methods based on paradigmatic and contextual similarity measures populated from large quantities of inexpensive raw text data. This approach is highly language independent and requires no modification to the algorithm or implementation to shift between languages such as French and English.'],\n",
       "   '2000']],\n",
       " [[['Automatic Labeling of Semantic Roles.',\n",
       "    'We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data.'],\n",
       "   '2000']],\n",
       " [[['Independence and Commitment: Assumptions for Rapid Training and Execution of Rule-based POS Taggers.',\n",
       "    \"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.\"],\n",
       "   '2000']],\n",
       " [[['Chinese-Korean Word Alignment Based on Linguistic Comparison.',\n",
       "    'Word alignment problem between parallel corpora is based on the similar characteristics of two aligned words in two languages. We investigate linguistic-knowledge-based word similarity measures while other previous works heavily rely on statistical information, and their limits will be discussed. Linguistic knowledge is acquired from linguistic comparison of all layers between two languages, for Chinese and Korean in this paper.'],\n",
       "   '2000']],\n",
       " [[['Incorporating Compositional Evidence in Memory-Based Partial Parsing.',\n",
       "    'In this paper, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked.'],\n",
       "   '2000']],\n",
       " [[['Rule Writing or Annotation: Cost-efficient Resource Usage for Base Noun Phrase Chunking.',\n",
       "    'This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive real-time human annotation. Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored. Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.'],\n",
       "   '2000']],\n",
       " [[['An Unsupervised Approach to Prepositional Phrase Attachment using Contextually Similar Words.',\n",
       "    'Prepositional phrase attachment is a common source of ambiguity in natural language processing. We present an unsupervised corpus-based approach to prepositional phrase attachment that achieves similar performance to supervised methods. Unlike previous unsupervised approaches in which training data is obtained by heuristic extraction of unambiguous examples from a corpus, we use an iterative process to extract training data from an automatically parsed corpus. Attachment decisions are made using a linear combination of features and low frequency events are approximated using contextually similar words.'],\n",
       "   '2000']],\n",
       " [[['Word Sense Disambiguation by Learning from Unlabeled Data.',\n",
       "    'Most corpus-based approaches to natural language processing suffer from lack of training data. This is because acquiring a large number of labeled data is expensive. This paper describes a learning method that exploits unlabeled data to tackle data sparseness problem. The method uses committee learning to predict the labels of unlabeled data that augment the existing training data. Our experiments on word sense disambiguation show that predictive accuracy is significantly improved by using additional unlabeled data.'],\n",
       "   '2000']],\n",
       " [[['Calculating Functional Programs.',\n",
       "    'Functional programs are merely equations; they may be manipulated by straightforward equational reasoning. In particular, one can use this style of reasoning to calculate programs, in the same way that one calculates numeric values in arithmetic. Many useful theorems for such reasoning derive from an algebraic view of programs, built around datatypes and their operations. Traditional algebraic methods concentrate on initial algebras, constructors, and values; dual co-algebraic methods concentrate on final co-algebras, destructors, and processes. Both methods are elegant and powerful; they deserve to be combined.'],\n",
       "   '2000']],\n",
       " [[['Exercises in Coalgebraic Specification.',\n",
       "    \"An introduction to coalgebraic specification is presented via examples. A coalgebralc specification describes a collection of coalgebras satisfying certain assertions. It is thus an axiomatic description of a particular class of mathematical structures. Such specifications are especially suitable for state-based dynamical systems in general, and for classes in object-oriented programming languages in particular. This chapter will gradually introduce the notions of bisimilarity, invariance, component classes, temporal logic and refinement in a coalgebraic setting. Besides the running example of the coalgebraic specification of (possibly infinite) binary trees, a specification of Peterson's mutual exclusion algorithm is elaborated in detail.\"],\n",
       "   '2000']],\n",
       " [[['Temporal Algebra.',\n",
       "    \"We develop temporal logic from the theory of complete lattices, Galois connections and fixed points. In particular, we prove that all seventeen axioms of Manna and Pnueli's sound and complete proof system for linear temporal logic can be derived from just two postulates, namely that (&oplus;, &&ominus;tilde;) is a Galois connection and that (&ominus;, &oplus;) is a perfect Galois connection. We also obtain a similar result for the branching time logic CTL.A surprising insight is that most of the theory can be developed without the use of negation. In effect, we are studying intuitionistic temporal logic. Several examples of such structures occurring in computer science are given. Finally, we show temporal algebra at work in the derivation of a simple graph-theoretic algorithm.This paper is tutorial in style and there are no difficult technical results. To the experts in temporal logics, we hope to convey the simplicity and beauty of algebraic reasoning as opposed to the machine-orientedness of logical deduction. To those familiar with the calculational approach to programming, we want to show that their methods extend easily and smoothly to temporal reasoning. For anybody else, this text may serve as a gentle introduction to both areas.\"],\n",
       "   '2000']],\n",
       " [[['Self-Duality of Bounded Monotone Boolean Functions and Related Problems.',\n",
       "    'In this paper we examine the problem of determining the self-duality of a monotone boolean function in disjunctive normal form (DNF). We show that the self-duality of monotone boolean functions with n disjuncts such that each disjunct has at most k literals can be determined in O(2^k^^^2k^2n) time. This implies an O(n^2logn) algorithm for determining the self-duality of logn-DNF functions. We also consider the version where any two disjuncts have at most c literals in common. For this case we give an O(n^4^(^c^+^1^)) algorithm for determining self-duality.'],\n",
       "   '2000']],\n",
       " [[['On Approximate Learning by Multi-layered Feedforward Circuits.',\n",
       "    'We deal with the problem of efficient learning of feedforward neural networks. First, we consider the objective to maximize the ratio of correctly classified points compared to the size of the training set. We show that it is NP-hard to approximate the ratio within some constant relative error if architectures with varying input dimension, one hidden layer, and two hidden neurons are considered where the activation function in the hidden layer is the sigmoid function, and the situation of epsilon-separation is assumed, or the activation function is the semilinear function. For single hidden layer threshold networks with varying input dimension and n hidden neurons, approximation within a relative error depending on n is NP-hard even if restricted to situations where the number of examples is limited with respect to n.Afterwards, we consider the objective to minimize the failure ratio in the presence of misclassification errors. We show that it is NP-hard to approximate the failure ratio within any positive constant for a multilayered threshold network with varying input dimension and a fixed number of neurons in the hidden layer if the thresholds of the neurons in the first hidden layer are zero. Furthermore, even obtaining weak approximations is almost NP-hard in the same situation.'],\n",
       "   '2000']],\n",
       " [[['Identification of Function Distinguishable Languages.',\n",
       "    \"We show how appropriately chosen functions f which we call distinguishing can be used to make deterministic finite automata backward deterministic. This idea can be exploited to design regular language classes called f-distinguishable which are identifiable in the limit from positive samples. Special cases of this approach are the k-reversible and terminal distinguishable languages, as discussed in Angluin (J. Assoc. Comput. Mach. 29 (3) (1982) 741), Fernau (Technical Report WSI-99-23, Universität Tübingen (Germany), Wilhelm-Schickard-Institut für Informatik, 1999, Short version published in the proceedings of AMAI 2000, see http://rutcot. rutgers. edu/~amai/aimath00/AcceptedCont.htm, Proc. 15th Internat. Conf. on Pattern Recognition (ICPR 2000), Vol. 2, IEEE Press, New York, 2000, pp. 125-128), Radhakrishnan (Ph.D. Thesis, Department of Computer Science and Engineering, Indian Institute of Technology, Bombay, India, 1987), Radhakrishnan and Nagaraja (IEEE Trans. Systems, Man Cybernet. 17 (6) (1987) 982). Moreover, we show that all regular languages may be approximated in the setting introduced by Kobayashi and Yokomori (in: K. P. Jantke, T. Shinohara, Th. Zeugmann (Eds.), Proc. Sixth Internat. Conf. Algorithmic Learning Theory (ALT'95), Lecture Notes in Computer Science/Lecture Notes in Artificial Intelligence, Vol. 997, Springer, Berlin, 1995, pp. 298-312), (Theoret. Comput. Sci. 174 (1997) 251-257) by any class of f-distinguishable languages. Observe that the class of all function-distinguishable languages is equal to the class of regular languages.\"],\n",
       "   '2000']],\n",
       " [[['Learning Erasing Pattern Languages with Queries.',\n",
       "    \"A pattern is a finite string of constant and variable symbols. The non-erasing language generated by a pattern is the set of all strings of constant symbols that can be obtained by substituting non-empty strings for variables. In order to build the erasing language generated by a pattern, it is also admissible to substitute the empty string.The present paper deals with the problem of learning erasing pattern languages within Angluin's model of learning with queries. Moreover, the learnability of erasing pattern languages with queries is studied when additional information is available. The results obtained are compared with previously known results in case non-erasing pattern languages have to be learned.First, when regular pattern languages have to be learned, it is shown that the learnability results for the non-erasing case remain valid, if the proper superclass of all erasing regular pattern languages is the object of learning. Second, in the general case, serious differences have been observed. For instance, it turns out that arbitrary erasing pattern languages cannot be learned in settings in which, in the non-erasing case, even polynomially many queries will suffice.\"],\n",
       "   '2000']],\n",
       " [[['A New Tractable Class of Constraint Satisfaction Problems.',\n",
       "    'In this paper we consider constraint satisfaction problems where the set of constraint relations is fixed. Feder and Vardi (1998) identified three families of constraint satisfaction problems containing all known polynomially solvable problems. We introduce a new class of problems called para-primal problems, incomparable with the families identified by Feder and Vardi (1998) and we prove that any constraint problem in this class is decidable in polynomial time. As an application of this result we prove a complete classification for the complexity of constraint satisfaction problems under the assumption that the basis contains all the permutation relations. In the proofs, we make an intensive use of algebraic results from clone theory about the structure of para-primal and homogeneous algebras.'],\n",
       "   '2000']],\n",
       " [[['A New Logic for Electronic Commerce Protocols.',\n",
       "    'The primary objective of this paper is to present the definition of a new dynamic, linear and modal logic for security protocols. The logic is compact, expressive and formal. It allows the specification of classical security properties (authentication, secrecy and integrity) and also electronic commerce properties (non-repudiation, anonymity, good atomicity, money atomicity, certified delivery, etc.). The logic constructs are interpreted over a trace-based model. Traces reflect valid protocol executions in the presence of a malicious smart intruder. The logic is endowed with a tableau-based proof system that leads to a modular denotational semantics and local model checking.'],\n",
       "   '2000']],\n",
       " [[['Advances in domain independent linear text segmentation.',\n",
       "    'This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering.'],\n",
       "   '2000']],\n",
       " [[['MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for Information Queries.',\n",
       "    \"This paper describes MIMIC, an adaptive mixed initiative spoken dialogue system that provides movie showtime information. MIMIC improves upon previous dialogue systems in two respects. First, it employs initiative-oriented strategy adaptation to automatically adapt response generation strategies based on the cumulative effect of information dynamically extracted from user utterances during the dialogue. Second, MIMIC's dialogue management architecture decouples its initiative module from the goal and response strategy selection processes, providing a general framework for developing spoken dialogue systems with different adaptation behavior.\"],\n",
       "   '2000']],\n",
       " [[['Mostly-Unsupervised Statistical Segmentation of Japanese: Applications to Kanji.',\n",
       "    'Given the lack of word delimiters in written Japanese, word segmentation is generally considered a crucial first step in processing Japanese texts. Typical Japanese segmentation algorithms rely either on a lexicon and grammar or on pre-segmented data. In contrast, we introduce a novel statistical method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyzers over a variety of error metrics.'],\n",
       "   '2000']],\n",
       " [[['A Classification Approach to Word Prediction.',\n",
       "    'The eventual goal of a language model is to curately predict the value of a missing word given its context. We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context. This approach raises a few new questions that we address. First, in order to learn good word representations it is necessary to use an expressive representation of the context. We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction. Second, since the number of words \"competing\" for each prediction is large, there is a need to \"focus the attention\" on a smaller subset of these. We exhibit the contribution of a \"focus of attention\" mechanism to the performance of the word predictor. Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks.'],\n",
       "   '2000']],\n",
       " [[['Corpus-Based Syntactic Error Detection Using Syntactic Patterns.',\n",
       "    'This paper presents a parsing system for the detection of syntactic errors. It combines a robust partial parser which obtains the main sentence components and a finite-state parser used for the description of syntactic error patterns. The system has been tested on a corpus of real texts, containing both correct and incorrect sentences, with promising results.'],\n",
       "   '2000']],\n",
       " [[['Multilingual Coreference Resolution.',\n",
       "    'In this paper we present a new, multilingual data-driven method for coreference resolution as implemented in the SWIZZLE system. The results obtained after training this system on a bilingual corpus of English and Romanian tagged texts, outperformed coreference resolution in each of the individual languages.'],\n",
       "   '2000']],\n",
       " [[['The Efficiency of Multimodal Interaction for a Map-based Task.',\n",
       "    'This paper compares the efficiency of using a standard direct-manipulation graphical user interface (GUI) with that of using the QuickSet pen/voice multimodal interface for supporting a military task. In this task, a user places military units and control measures (e.g., various types of lines, obstacles, objectives) on a map. Four military personnel designed and entered their own simulation scenarios via both interfaces. Analyses revealed that the multimodal interface led to an average 3.5-fold speed improvement in the average entity creation time, including all error handling. The mean time to repair errors also was 4.3 times faster when interacting multimodally. Finally, all subjects reported a strong preference for multimodal interaction. These results indicate a substantial efficiency advantage for multimodal over GUI-based interaction during map-based tasks.'],\n",
       "   '2000']],\n",
       " [[['Unit Completion for a Computer-aided Translation Typing System.',\n",
       "    'This work is in the context of TRANSTYPE, a system that observes its user as he or she types a translation and repeatedly suggests completions for the text already entered. The user may either accept, modify, or ignore these suggestions. We describe the design, implementation, and performance of a prototype which suggests completions of units of texts that are longer than one word.'],\n",
       "   '2000']],\n",
       " [[['The Automatic Translation of Discourse Structures.',\n",
       "    'We empirically show that there are significant differences between the discourse structure of Japanese texts and the discourse structure of their corresponding English translations. To improve translation quality, we propose a computational model for rewriting discourse structures. When we train our model on a parallel corpus of manually built Japanese and English discourse structure trees, we learn to rewrite Japanese trees as trees that are closer to the natural English rendering than the original ones.'],\n",
       "   '2000']],\n",
       " [[['Using Semantic Preferences to Identify Verbal Participation in Role Switching Alternations.',\n",
       "    'We propose a method for identifying diathesis alternations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms. The method uses selectional preferences acquired as probability distributions over WordNet. Preferences for the target slots are compared using a measure of distributional similarity. The method is evaluated on the causative and conative alternations, but is generally applicable and does not require a priori knowledge specific to the alternation.'],\n",
       "   '2000']],\n",
       " [[['Extracting Molecular Binding Relationships from Biomedical Text.',\n",
       "    'ARBITER is a Prolog program that extracts assertions about macromolecular binding relationships from biomedical text. We describe the domain knowledge and the underspecified linguistic analyses that support the identification of these predications. After discussing a formal evaluation of ARBITER, we report on its application to 491, 000 MEDLINE&reg; abstracts, during which almost 25, 000 binding relationships suitable for entry into a database of macromolecular function were extracted.'],\n",
       "   '2000']],\n",
       " [[['An Empirical Assessment of Semantic Interpretation.',\n",
       "    'We introduce a framework for semantic interpretation in which dependency structures are mapped to conceptual representations based on a parsimonious set of interpretation schemata. Our focus is on the empirical evaluation of this approach to semantic interpretation, i.e., its quality in terms of recall and precision. Measurements are taken with respect to two real-world domains, viz. information technology test reports and medical finding reports.'],\n",
       "   '2000']],\n",
       " [[['Generic Validation of Structural Content with Parametric Modules.',\n",
       "    'We demonstrate a natural mapping from XML element types to ML module expressions. The mapping is inductive and definitions of common XML operations can be derived as the module expressions are composed. We show how to derive, in a generic way, the validation function, which checks an XML document for conformance to its DTD (Document Type Definition). One can view validation as assigning ML types to XML elements and the validation procedure a pre-requisite for typeful XML programming in ML. Our mapping uses the parametric module facility of ML in some contrived way. For example, in validating WML (WAP Markup Language) documents, we need to use 36ary type constructors, as well as higher-order modules that take in as many as 17 modules as input. That one can systematically model XML DTDs at the module level suggests ML-like languages are suitable for type-safe prototyping of DTD-aware XML applications.'],\n",
       "   '2000']],\n",
       " [[['Performance-Scalable Array Architectures for Modular Multiplication.',\n",
       "    'Modular multiplication is a fundamental operation in numerous public-key cryptosystems including the RSA method. Increasing popularity of internet e-commerce and other security applications translate into a demand for a scalable performance hardware design framework. Previous scalable hardware methodologies either were not systolic and thus involved performance-degrading, full-word-length broadcasts or were not scalable beyond linear array size. In this paper, these limitations are overcome with the introduction of three classes of scalable-performance modular multiplication architectures based on systolic arrays. Very high clock rates are feasible, since the cells composing the architectures are of bit-level complexity. Architectural methods based on both binary and high-radix modular multiplication are derived. All techniques are constructed to allow additional flexibility for the impact of interconnect delay within the design environment.'],\n",
       "   '2000']],\n",
       " [[['Architecture and design of AlphaServer GS320.',\n",
       "    'This paper describes the architecture and implementation of the AlphaServer GS320, a cache-coherent non-uniform memory access multiprocessor developed at Compaq. The AlphaServer GS320 architecture is specifically targeted at medium-scale multiprocessing with 32 to 64 processors. Each node in the design consists of four Alpha 21264 processors, up to 32GB of coherent memory, and an aggressive IO subsystem. The current implementation supports up to 8 such nodes for a total of 32 processors. While snoopy-based designs have been stretched to medium-scale multiprocessors by some vendors, providing sufficient snoop bandwidth remains a major challenge especially in systems with aggressive processors. At the same time, directory protocols targeted at larger scale designs lead to a number of inherent inefficiencies relative to snoopy designs. A key goal of the AlphaServer GS320 architecture has been to achieve the best-of-both-worlds, partly by exploiting the bounded scale of the target systems.This paper focuses on the unique design features used in the AlphaServer GS320 to efficiently implement coherence and consistency. The guiding principle for our directory-based protocol is to address correctness issues related to rare protocol races without burdening the common transaction flows. Our protocol exhibits lower occupancy and lower message counts compared to previous designs, and provides more efficient handling of 3-hop transactions. Furthermore, our design naturally lends itself to elegant solutions for deadlock, livelock, starvation, and fairness. The AlphaServer GS320 architecture also incorporates a couple of innovative techniques that extend previous approaches for efficiently implementing memory consistency models. These techniques allow us to generate commit events (which are used for ordering purposes) well in advance of formulating the reply to a transaction. Furthermore, the separation of the commit event allows time-critical replies to bypass inbound requests without violating ordering properties. Even though our design specifically targets medium-scale servers, many of the same techniques can be applied to larger-scale directory-based and smaller-scale snoopy-based designs. Finally, we evaluate the performance impact of some of the above optimizations and present a few competitive benchmark results.'],\n",
       "   '2000']],\n",
       " [[['Evaluating Design Alternatives for Reliable Communication on High-Speed Networks.',\n",
       "    'We systematically evaluate the performance of five implementations of a single, user-level communication interface. Each implementation makes different architectural assumptions about the reliability of the network hardware and the capabilities of the network interface. The implementations differ accordingly in their division of protocol tasks between host software, network-interface firmware, and network hardware. Using microbenchmarks, parallel-programming systems, and parallel applications, we assess the performance impact of different protocol decompositions. We show how moving protocol tasks to a relatively slow network interface yields both performance advantages and disadvantages, depending on the characteristics of the application and the underlying parallel-programming system. In particular, we show that a communication system that assumes highly reliable network hardware and that uses network-interface support to process multicast traffic performs best for all applications.'],\n",
       "   '2000']],\n",
       " [[['Software Profiling for Hot Path Prediction: Less is More.',\n",
       "    'Recently, there has been a growing interest in exploiting profile information in adaptive systems such as just-in-time compilers, dynamic optimizers and, binary translators. In this paper, we show that sophisticated software profiling schemes that provide highly accurate information in an offline setting are ill-suited for these dynamic code generation systems. We experimentally demonstrate that hot path predictions must be made early in order to control the rising cost of missed opportunity that result from the prediction delay. We also show that existing sophisticated path profiling schemes, if used in an online setting, offer no prediction advantages over simpler schemes that exhibit much lower runtime overheads.Based on these observation we developed a new low-overhead software profiling scheme for hot path prediction. Using an abstract metric we compare our scheme to path profile based prediction and show that our scheme achieves comparable prediction quality. In our second set of experiments we include runtime overhead and evaluate the performance of our scheme in a realistic application: Dynamo, a dynamic optimization system. The results show that our prediction scheme clearly outperforms path profile based prediction and thus confirm that less profiling as exhibited in our scheme will actually lead to more effective hot path prediction.'],\n",
       "   '2000']],\n",
       " [[['Efficient and Flexible Value Sampling.',\n",
       "    'This paper presents novel sampling-based techniques for collecting statistical profiles of register contents, data values, and other information associated with instructions, such as memory latencies. Values of interest are sampled in response to periodic interrupts. The resulting value profiles can be analyzed by programmers and optimizers to improve the performance of production uniprocessor and multiprocessor systems.Our value sampling system extends the DCPI continuous profiling infrastructure, and inherits many of its desirable properties: our value profiler has low overhead (approximately 10% slowdown); it profiles all the code in the system, including the operating system kernel; and it operates transparently, without requiring any modifications to the profiled code.'],\n",
       "   '2000']],\n",
       " [[['Using Meta-level Compilation to Check FLASH Protocol Code.',\n",
       "    'Building systems such as OS kernels and embedded software is difficult. An important source of this difficulty is the numerous rules they must obey: interrupts cannot be disabled for \"too long,\" global variables must be protected by locks, user pointers passed to OS code must be checked for safety before use, etc. A single violation can crash the system, yet typically these invariants are unchecked, existing only on paper or in the implementor\\'s mind.This paper is a case study in how system implementors can use a new programming methodology, metalevel compilation (MC), to easily check such invariants. It focuses on using MC to check for errors in the code used to manage cache coherence on the FLASH shared memory multiprocessor. The only real practical method known for verifying such code is testing and simulation. We show that simple, system-specific checkers can dramatically improve this situation by statically pinpointing errors in the program source. These checkers can be written by implementors themselves and, by exploiting the system-specific information this allows, can detect errors unreachable with other methods. The checkers in this paper found 34 bugs in FLASH code despite the care used in building it and the years of testing it has undergone. Many of these errors fall in the worst category of systems bugs: those that show up sporadically only after days of continuous use. The case study is interesting because it shows that the MC approach finds serious errors in well-tested, non-toy systems code. Further, the code to find such bugs is usually 10-100 lines long, written in a few hours, and exactly locates errors that, if discovered during testing, would require several days of investigation by an experienced implementor.The paper presents 8 checkers we wrote, their application to five different protocol implementations, and a discussion of the errors that we found.'],\n",
       "   '2000']],\n",
       " [[['OceanStore: An Architecture for Global-Scale Persistent Storage.',\n",
       "    'OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development.'],\n",
       "   '2000']],\n",
       " [[['Power Aware Page Allocation.',\n",
       "    'One of the major challenges of post-PC computing is the need to reduce energy consumption, thereby extending the lifetime of the batteries that power these mobile devices. Memory is a particularly important target for efforts to improve energy efficiency. Memory technology is becoming available that offers power management features such as the ability to put individual chips in any one of several different power modes. In this paper we explore the interaction of page placement with static and dynamic hardware policies to exploit these emerging hardware features. In particular, we consider page allocation policies that can be employed by an informed operating system to complement the hardware power management strategies. We perform experiments using two complementary simulation environments: a trace-driven simulator with workload traces that are representative of mobile computing and an execution-driven simulator with a detailed processor/memory model and a more memory-intensive set of benchmarks (SPEC2000). Our results make a compelling case for a cooperative hardware/software approach for exploiting power-aware memory, with down to as little as 45% of the Energy&bull; Delay for the best static policy and 1% to 20% of the Energy&bull; Delay for a traditional full-power memory.'],\n",
       "   '2000']],\n",
       " [[['Symbiotic Jobscheduling for a Simultaneous Multithreading Processor.',\n",
       "    \"Simultaneous Multithreading machines fetch and execute instructions from multiple instruction streams to increase system utilization and speedup the execution of jobs. When there are more jobs in the system than there is hardware to support simultaneous execution, the operating system scheduler must choose the set of jobs to coscheduleThis paper demonstrates that performance on a hardware multithreaded processor is sensitive to the set of jobs that are coscheduled by the operating system jobscheduler. Thus, the full benefits of SMT hardware can only be achieved if the scheduler is aware of thread interactions. Here, a mechanism is presented that allows the scheduler to significantly raise the performance of SMT architectures. This is done without any advance knowledge of a workload's characteristics, using sampling to identify jobs which run well together.We demonstrate an SMT jobscheduler called SOS. SOS combines an overhead-free sample phase which collects information about various possible schedules, and a symbiosis phase which uses that information to predict which schedule will provide the best performance. We show that a small sample of the possible schedules is sufficient to identify a good schedule quickly. On a system with random job arrivals and departures, response time is improved as much as 17% over a schedule which does not incorporate symbiosis.\"],\n",
       "   '2000']],\n",
       " [[['Timestamp snooping: an approach for extending SMPs.',\n",
       "    \"Symmetric multiprocessor (SMP) servers provide superior performance for the commercial workloads that dominate the Internet. Our simulation results show that over one-third of cache misses by these applications result in cache-to-cache transfers, where the data is found in another processor's cache rather than in memory. SMPs are optimized for this case by using snooping protocols that broadcast address transactions to all processors. Conversely, directory-based shared-memory systems must indirectly locate the owner and sharers through a directory, resulting in larger average miss latencies.This paper proposes timestamp snooping, a technique that allows SMPs to i) utilize high-speed switched interconnection networks and ii) exploit physical locality by delivering address transactions to processors and memories without regard to order. Traditional snooping requires physical ordering of transactions. Timestamp snooping works by processing address transactions in a logical order. Logical time is maintained by adding a few bits per address transaction and having network switches perform a handshake to ensure on-time delivery. Processors and memories then reorder transactions based on their timestamps to establish a total order.We evaluate timestamp snooping with commercial workloads on a 16-processor SPARC system using the Simics full-system simulator. We simulate both an indirect (butterfly) and a direct (torus) network design. For OLTP, DSS, web serving, web searching, and one scientific application, timestamp snooping with the butterfly network runs 6-28% faster than directories, at a cost of 13-43% more link traffic. Similarly, with the torus network, timestamp snooping runs 6-29% faster for 17-37% more link traffic. Thus, timestamp snooping is worth considering when buying more interconnect bandwidth is easier than reducing interconnect latency.\"],\n",
       "   '2000']],\n",
       " [[['Slipstream Processors: Improving both Performance and Fault Tolerance.',\n",
       "    'Processors execute the full dynamic instruction stream to arrive at the final output of a program, yet there exist shorter instruction streams that produce the same overall effect. We propose creating a shorter but otherwise equivalent version of the original program by removing ineffectual computation and computation related to highly-predictable control flow. The shortened program is run concurrently with the full program on a chip multiprocessor simultaneous multithreaded processor, with two key advantages:1) Improved single-program performance. The shorter program speculatively runs ahead of the full program and supplies the full program with control and data flow outcomes. The full program executes efficiently due to the communicated outcomes, at the same time validating the speculative, shorter program. The two programs combined run faster than the original program alone. Detailed simulations of an example implementation show an average improvement of 7% for the SPEC95 integer benchmarks.2) Fault tolerance. The shorter program is a subset of the full program and this partial-redundancy is transparently leveraged for detecting and recovering from transient hardware faults.'],\n",
       "   '2000']],\n",
       " [[['Communication Scheduling.',\n",
       "    'The high arithmetic rates of media processing applications require architectures with tens to hundreds of functional units, multiple register files, and explicit interconnect between functional units and register files. Communication scheduling enables scheduling to these emerging architectures, including those that use shared buses and register file ports. Scheduling to these shared interconnect architectures is difficult because it requires simultaneously allocating functional units to operations and buses and register file ports to the communications between operations. Prior VLIW scheduling algorithms are limited to clustered register file architectures with no shared buses or register file ports. Communication scheduling extends the range of target architectures by making each communication explicit and decomposing it into three components: a write stub, zero or more copy operations, and a read stub. Communication scheduling allows media processing kernels to achieve 98% of the performance of a central register file architecture on a distributed register file architecture with only 9% of the area, 6% of the power consumption, and 37% of the access delay, and 120% of the performance of a clustered register file architecture on a distributed register file architecture with 56% of the area and 50% of the power consumption.'],\n",
       "   '2000']],\n",
       " [[['MemorIES: A Programmable, Real-Time Hardware Emulation Tool for Multiprocessor Server Design.',\n",
       "    \"Modern system design often requires multiple levels of simulation for design validation and performance debugging. However, while machines have gotten faster, and simulators have become more detailed, simulation speeds have not tracked machine speeds. As a result, it is difficult to simulate realistic problem sizes and hardware configurations for a target machine. Instead, researchers have focussed on developing scaling methodologies and running smaller problem sizes and configurations that attempt to represent the behavior of the real problem. Given the increasing size of problems today, it is unclear whether such an approach yields accurate results. Moreover, although commercial workloads are prevalent and important in today's marketplace, many simulation tools are unable to adequately profile such applications, let alone for realistic sizes.In this paper we present a hardware-based emulation tool that can be used to aid memory system designers. Our focus is on the memory system because the ever-widening gap between processor and memory speeds means that optimizing the memory subsystem is critical for performance. We present the design of the Memory Instrumentation and Emulation System (MemorIES). MemorIES is a programmable tool designed using FPGAs and SDRAMs. It plugs into an SMP bus to perform on-line emulation of several cache configurations, structures and protocols while the system is running real-life workloads in real-time, without any slowdown in application execution speed. We demonstrate its usefulness in several case studies, and find several important results. First, using traces to perform system evaluation can lead to incorrect results (off by 100% or more in some cases) if the trace size is not sufficiently large. Second, MemorIES is able to detect performance problems by profiling miss behavior over the entire course of a run, rather than relying on a small interval of time. Finally, we observe that previous studies of SPLASH2 applications using scaled application sizes can result in optimistic miss rates relative to real sizes on real machines, providing potentially misleading data when used for design evaluation.\"],\n",
       "   '2000']],\n",
       " [[['An Analysis of Operating System Behavior on a Simultaneous Multithreaded Architecture.',\n",
       "    \"This paper presents the first analysis of operating system execution on a simultaneous multithreaded (SMT) processor. While SMT has been studied extensively over the past 6 years, previous research has focused entirely on user-mode execution. However, many of the applications most amenable to multithreading technologies spend a significant fraction of their time in kernel code. A full understanding of the behavior of such workloads therefore requires execution and measurement of the operating system, as well as the application itself.To carry out this study, we (1) modified the Digital Unix 4.0d operating system to run on an SMT CPU, and (2) integrated our SMT Alpha instruction set simulator into the SimOS simulator to provide an execution environment. For an OS-intensive workload, we ran the multithreaded Apache Web server on an 8-context SMT. We compared Apache's user- and kernel-mode behavior to a standard multiprogrammed SPECInt workload, and compared the SMT processor to an out-of-order superscalar running both workloads. Overall, our results demonstrate the microarchitectural impact of an OS-intensive workload on an SMT processor and provide insight into the OS demands of the Apache Web server. The synergy between the SMT processor and Web and OS software produced a greater throughput gain over superscalar execution than seen on any previously examined workloads, including commercial databases and explicitly parallel programs.\"],\n",
       "   '2000']],\n",
       " [[['OS and Compiler Considerations in the Design of the IA-64 Architecture.',\n",
       "    \"Increasing demands for processor performance have outstripped the pace of process and frequency improvements, pushing designers to find ways of increasing the amount of work that can be processed in parallel. Traditional RISC architectures use hardware approaches to obtain more instruction-level parallelism, with the compiler and the operating system (OS) having only indirect visibility into the mechanisms used.The IA-64 architecture [14] was specifically designed to enable systems which create and exploit high levels of instruction-level parallelism by explicitly encoding a program's parallelism in the instruction set [25]. This paper provides a qualitative summary of the IA-64 architecture features that support control and data speculation, and register stacking. The paper focusses on the functional synergy between these architectural elements (rather than their individual performance merits), and emphasizes how they were designed for cooperation between processor hardware, compilers and the OS.\"],\n",
       "   '2000']],\n",
       " [[['Hierarchical Flip Zooming: Enabling Parallel Exploration of Hierarchical Visualization.',\n",
       "    'This paper describes hierarchical Flip Zooming, a focus+context visualization technique for hierarchical information sets. It allows for independent focus+context views at each node of the hierarchy and enables parallel exploration of different branches of the hierarchy. Visualization, navigation and interaction in the Flip Zooming technique is described as well as how the technique fits into existing models of information visualization. Examples of applications using the technique are given.'],\n",
       "   '2000']],\n",
       " [[['Guidelines for Using Multiple Views in Information Visualization.',\n",
       "    'A multiple view system uses two or more distinct views to support the investigation of a single conceptual entity. Many such systems exist, ranging from computer-aided design (CAD) systems for chip design that display both the logical structure and the actual geometry of the integrated circuit to overview-plus-detail systems that show both an overview for context and a zoomed-in-view for detail. Designers of these systems must make a variety of design decisions, ranging from determining layout to constructing sophisticated coordination mechanisms. Surprisingly, little work has been done to characterize these systems or to express guidelines for their design. Based on a workshop discussion of multiple views, and based on our own design and implementation experience with these systems, we present eight guidelines for the design of multiple view systems.'],\n",
       "   '2000']],\n",
       " [[['Reification, Polymorphism and Reuse: Three Principles for Designing Visual Interfaces.',\n",
       "    'This paper presents three design principles to support the development of large-scale applications and take advantage of recent research in new interaction techniques: Reification turns concepts into first class objects, polymorphism permits commands to be applied to objects of different types, and reuse makes both user input and system output accessible for later use. We show that the power of these principles lies in their combination. Reification creates new objects that can be acted upon by a small set of polymorphic commands, creating more opportunities for reuse. The result is a simpler yet more powerful interface. To validate these principles, we describe their application in the redesign of a complex interface for editing and simulating Coloured Petri Nets. The cpn2000 interface integrates floating palettes, toolglasses and marking menus in a consistent manner with a new metaphor for managing the workspace. It challenges traditional ideas about user interfaces, getting rid of pull-down menus, scrollbars, and even selection, while providing the same or greater functionality. Preliminary tests with users show that they find the new system both easier to use and more efficient.'],\n",
       "   '2000']],\n",
       " [[['A Comparision of Set-Based and Graph-Based Visualisations of Overlapping Classification Hierarchies.',\n",
       "    \"The visualisation of hierarchical information sets has been a staple of Information Visualisation since the field came into being in the early 1990's. However, at present, support for visualising the correlations between multiple, overlapping sets of hierarchical information has been lacking. This is despite the realisation that for certain tasks this information is as important as the information that forms the individual hierarchies. In response to this, we have produced two early visualisation prototypes, one based on a graph visualisation, and the other on a set-based metaphor, that endeavour to display such information in a readily perceived form to potential users. The science of botanical taxonomy is used as an example of a field where such a visualisation would be useful, and also as a resource for example information sets that the prototypes can act upon. Technical and perceptual issues involved in the design and implementation of both prototypes are discussed. Following this, informal user testing on both prototypes is described, which utilised user observation techniques to elicit qualitative feedback from the taxonomists. These findings are then used to emphasise the shortcomings and advantages of each prototype, and from these probable issues for future prototyping and development are drawn.\"],\n",
       "   '2000']],\n",
       " [[['An Architecture for Pen-based Interaction on Electronic Whiteboards.',\n",
       "    'This paper describes the software architecture for our pen-based electronic whiteboard system, called Flatland. The design goal of Flatland is to support various activities on personal office whiteboards, while maintaining the outstanding ease of use and informal appearance of conventional whiteboards. The GUI framework of existing window systems is too complicated and heavy-weight to achieve this goal, and so we designed a new architecture that works as a kind of window system for pen-based applications. Our architecture is characterized by its use of freeform strokes as the basic primitive for both input and output, flexible screen space segmentation, pluggable applications that can operate on each segment, and built-in history management mechanisms. This architecture is carefully designed to achieve simple, unified coding and high extensibility, which was essential to the iterative prototyping of the Flatland interface. While the current implementation is optimized for large office whiteboards, this architecture is useful for the implementation of a range of various pen-based systems.'],\n",
       "   '2000']],\n",
       " [[['Device Independant Text Input: A Rationale and an Example.',\n",
       "    'Individual characters and text are the main inputs in many computing devices. Currently there is a growing trend in developing small portable devices like mobile phones, personal digital assistants, GPS-navigators, and two-way pagers. Unfortunately these portable computing devices have different user interfaces and therefore the task of text input takes many forms. The user, who in the future is likely to have several of these devices, has to learn several text input methods. We argue that there is a need for a universal text input method. A method like this would work on a wide range of interface technologies and allow the user to transfer his or her writing skill without device-specific training. To show that device independent text input is possible, we present a candidate for a device independent text entry method that supports skill transfer between different devices. A limited longitudinal study was conducted to achieve a proof of concept evaluation of our Minimal Device Independent Text Input Method (MDITIM). We found MDITIM writing skill acquired with a touchpad to work almost equally well on mouse, trackball, joystick and keyboard without any additional training. Our test group reached on average 41% of their handwriting speed by the end of the tenth 30-minute training session.'],\n",
       "   '2000']],\n",
       " [[['Multidimensional Information Visualization through Sliding Rods.',\n",
       "    'In this paper we propose new visual interface technology to address multidimensional data exploration and browsing tasks. MultiNav, a prototype from GTE Laboratories, is based upon a multidimensional information model that affords new data exploration and semantically structured browsing interactions. The primary visual metaphor is based on sliding rods, each of which is associated with an information dimension from the underlying model. Users can interactively select value ranges along the rods in order to reveal hidden relationships as well as query and restrict the set through direct manipulation. A novel focus+context view is afforded in which detail about individual items is revealed within the context of the global multidimensional attribute space. We propose a novel interaction technique to change focus, which is based on dragging rods from side to side. We relate this work on multidimensional information visualization to other research in the area, including Parallel Coordinates, Dynamic Histograms, Dynamic Queries, and focus+context tables.*.'],\n",
       "   '2000']],\n",
       " [[['Snap-Together Visualization: A User Interface for Coodinating Visualizations via Relational Schemata.',\n",
       "    \"Multiple coordinated visualizations enable users to rapidly explore complex information. However, users often need unforeseen combinations of coordinated visualizations that are appropriate for their data. Snap-Together Visualization enables data users to rapidly and dynamically mix and match visualizations and coordinations to construct custom exploration interfaces without programming. Snap's conceptual model is based on the relational database model. Users load relations into visualizations then coordinate them based on the relational joins between them. Users can create different types of coordinations such as: brushing, drill down, overview and detail view, and synchronized scrolling. Visualization developers can make their independent visualizations snap-able with a simple API. Evaluation of Snap revealed benefits, cognitive issues, and usability concerns. Data savvy users were very capable and thrilled to rapidly construct powerful coordinated visualizations. A snapped overview and detail-view coordination improved user performance by 30-80%, depending on task.\"],\n",
       "   '2000']],\n",
       " [[['Incremental Mining of Sequential Patterns in Large Databases.',\n",
       "    'In this paper, we consider the problem of the incremental mining of sequential patterns when new transactions or new customers are added to an original database. We present a new algorithm for mining frequent sequences that uses information collected during an earlier mining process to cut down the cost of finding new sequential patterns in the updated database. Our test shows that the algorithm performs significantly faster than the naive approach of mining the whole updated database from scratch. The difference is so pronounced that this algorithm could also be useful for mining sequential patterns, since in many cases it is faster to apply our algorithm than to mine sequential patterns using a standard algorithm, by breaking down the database into an original database plus an increment.'],\n",
       "   '2000']],\n",
       " [[['Simulation Based Minimization.',\n",
       "    'We present a minimization algorithm that receives a Kripke structure M and returns the smallest structure that is simulation equivalent to M. The simulation equivalence relation is weaker than bisimulation but stronger than the simulation preorder. It strongly preserves ACTL and LTL (as sublogics of ACTL*).We show that every structure M has a unique-up-to-isomorphism reduced structure that is simulation equivalent to M and smallest in size. Our Minimizing Algorithm constructs this reduced structure. It first constructs the quotient structure for M, then eliminates transitions to little brothers, and finally deletes unreachable states.Since the first step of the algorithm is based on the simulation preorder over M, it has maximal space requirements. To reduce them, we present the Partitioning Algorithm, which constructs the quotient structure for M without ever building the simulation preorder. The Partitioning Algorithm has improved space complexity, but its time complexity might have worse.'],\n",
       "   '2000']],\n",
       " [[['On Unification for Bonded Distributive Lattices.',\n",
       "    'We give a method for deciding unifiability in the variety of bounded distributive lattices. For this, we reduce the problem of deciding whether a unification problem S has a solution to the problem of checking the satisfiability of a set &Phi;S of ground clauses. This is achieved by using a structure-preserving translation to clause form. The satisfiability check can then be performed by either a resolution-based theorem prover or a SAT checker. We apply the method to unification with free constants and to unification with linear constant restrictions, and show that, in fact, it yields a decision procedure for the positive theory of the variety of bounded distributive lattices. We also consider the problem of unification over (i.e., in an algebraic extension of) the free lattice. Complexity issues are also addressed.'],\n",
       "   '2000']],\n",
       " [[['A multi-perspective software visualization environment.',\n",
       "    'This paper describes a multi-perspective software visualization environment, SHriMP, which combines single view and multi-view techniques to support software exploration at both the architectural and source code levels. SHriMP provides three different views: a primary nested view and two subsidiary views. The primary nested view employs fisheye views of nested graphs, provides contextual cues, and supports general exploration activities. In SHriMP, subsidiary views exist as a searching tool and a relation tracer. These views complement each other and allow programmers to examine a software system from multiple perspectives.'],\n",
       "   '2000']],\n",
       " [[['A framework for optimizing Java using attributes.',\n",
       "    \"This paper presents a framework for supporting the optimization of Java programs using attributes in Java class files. We show how class file attributes may be used to convey both optimization opportunities and profile information to a variety of Java virtual machines including ahead-of-time compilers and just-in-time compilers.We present our work in the context of Soot, a framework that supports the analysis and transformation of Java bytecode (class files)[21, 25, 26]. We demonstrate the framework with attributes for elimination of array bounds and null pointer checks, and we provide experimental results for the Kaffe just-in-time compiler, and IBM's High Performance Compiler for Java ahead-of-time compiler.\"],\n",
       "   '2000']],\n",
       " [[['Automatic detection of immutable fields in Java.',\n",
       "    'This paper introduces techniques to detect mutability of fields and classes in Java. A variable is considered to be mutable if a new value is stored into it, as well as if any of its reachable variables is mutable. We present a static flow-sensitive analysis algorithm which can be applied to any Java component. The analysis classifies fields and classes as either mutable or immutable. In order to facilitate openworld analysis, the algorithm identifies situations that expose variables to potential modification by code outside the component, as well as situations where variables are modified by the analyzed code. We also present an implementation of the analysis which focuses on detecting mutability of class variables, so as to avoid isolation problems. The implementation incorporates intra- and inter-procedural data-flow analyses and is shown to be highly scalable. Experimental results demonstrate the effectiveness of the algorithms.'],\n",
       "   '2000']],\n",
       " [[['Supporting maintenance of legacy software with data mining techniques.',\n",
       "    'Software maintenance is a very costly and time consuming part of the software life cycle. The problems with software maintenance are even more pressing in the case of legacy software systems. This paper describes our research towards application of inductive methods to the data extracted from source code, software maintenance records, and software developers activities to learn a Maintenance Relevance Relation among files in a software system. We discuss the methodology employed, and some of the encountered problems and our solutions for them. The paper will also present some of the results that we have obtained.'],\n",
       "   '2000']],\n",
       " [[['Efficient mapping of software system traces to architectural views.',\n",
       "    \"Information about a software system''s execution can help a developer with many tasks, including software testing, performance tuning, and program understanding. In almost all cases, this dynamic information is reported in terms of source-level constructs, such as procedures and methods. For some software engineering tasks, source-level information is not optimal because there is a wide gap between the information presented (i.e., procedures) and the concepts of interest to the software developer (i.e., subsystems). One way to close this gap is to allow developers to investigate the execution information in terms of a higher-level, typically architectural, view. In this paper, we present a straightforward encoding technique for dynamic trace information that makes it tractable and efficient to manipulate a trace from a variety of different architecture-level viewpoints. We also describe how this encoding technique has been used to support the development of two tools: a visualization tool and a path query tool. We present this technique to enable the development of additional tools that manipulate dynamic information at a higher-level than source.\"],\n",
       "   '2000']],\n",
       " [[['A cognitive and user centric based approach for reverse engineering tool design.',\n",
       "    'Reverse engineering tools aimed at facilitating software maintenance suffer from low adoption. Many are developed, but few are used by software engineers in performing their maintenance work. We introduce an approach for tool design that is aimed at increasing the adoptability potential of tools.Our approach is based on applying cognitive analysis to identify cognitively difficult aspects of maintenance work, then deriving cognitive requirements to address these difficulties. The approach is described in the context of the implementation of a reverse engineering tool we call DynaSee, which we have used to for the visualization of traces generated by a large telecommunications system. We describe how DynaSee addresses a specific set of cognitive difficulties.'],\n",
       "   '2000']],\n",
       " [[['Boolean Satisfiability with Transitivity Constraints.',\n",
       "    'We consider a variant of the Boolean satisfiability problem where a subset &epsiv; of the propositional variables appearing in formula Fsat encode a symmetric, transitive, binary relation over N elements. Each of these relational variables, ei,j, for 1 &le; i < j &le; N, expresses whether or not the relation holds between elements i and j. The task is to either find a satisfying assignment to Fsat that also satisfies all transitivity constraints over the relational variables (e.g., e1,2 &wedge; e2,3 &rArr; e1,3), or to prove that no such assignment exists. Solving this satisfiability problem is the final and most difficult step in our decision procedure for a logic of equality with uninterpreted functions. This procedure forms the core of our tool for verifying pipelined microprocessors.To use a conventional Boolean satisfiability checker, we augment the set of clauses expressing Fsat with clauses expressing the transitivity constraints. We consider methods to reduce the number of such clauses based on the sparse structure of the relational variables.To use Ordered Binary Decision Diagrams (OBDDs), we show that for some sets &epsiv;, the OBDD representation of the transitivity constraints has exponential size for all possible variable orderings. By considering only those relational variables that occur in the OBDD representation of Fsat, our experiments show that we can readily construct an OBDD representation of the relevant transitivity constraints and thus solve the constrained satisfiability problem.'],\n",
       "   '2000']],\n",
       " [[['Power Analysis Attacks and Algorithmic Approaches to their Countermeasures for Koblitz Curve Cryptosystems.',\n",
       "    'Because of their shorter key sizes, cryptosystems based on elliptic curves are being increasingly used in practical applications. A special class of elliptic curves, namely, Koblitz curves, offers an additional, but crucial, advantage of considerably reduced processing time. In this article, power analysis attacks are applied to cryptosystems that use scalar multiplication on Koblitz curves. Both the simple and the differential power analysis attacks are considered and a number of countermeasures are suggested. While the proposed countermeasures against the simple power analysis attacks rely on making the power consumption for the elliptic curve scalar multiplication independent of the secret key, those for the differential power analysis attacks depend on randomizing the secret key prior to each execution of the scalar multiplication. These countermeasures are computationally efficient and suitable for hardware implementation.'],\n",
       "   '2000']],\n",
       " [[['Symmetric bimanual interaction.',\n",
       "    'We present experimental work that explores the factors governing symmetric bimanual interaction in a two-handed task that requires the user to track a pair of targets, one target with each hand. A symmetric bimanual task is a two-handed task in which each hand is assigned an identical role. In this context, we explore three main experimental factors. We vary the distance between the pair of targets to track: as the targets become further apart, visual diversion increases, forcing the user to divide attention between the two targets. We also vary the demands of the task by using both a slow and a fast tracking speed. Finally, we explore visual integration of sub-tasks: in one condition, the two targets to track are connected by a line segment which visually links the targets, while in the other condition there is no connecting line. Our results indicate that all three experimental factors affect the degree of parallelism, which we quantify using a new metric of bimanual parallelism. However, differences in tracking error between the two hands are affected only by the visual integration factor.'],\n",
       "   '2000']],\n",
       " [[['Instrumental interaction: an interaction model for designing post-WIMP user interfaces.',\n",
       "    'This article introduces a new interaction model called Instrumental Interaction that extends and generalizes the principles of direct manipulation. It covers existing interaction styles, including traditional WIMP interfaces, as well as new interaction styles such as two-handed input and augmented reality. It defines a design space for new interaction techniques and a set of properties for comparing them. Instrumental Interaction describes graphical user interfaces in terms of domain objects and interaction instruments. Interaction between users and domain objects is mediated by interaction instruments, similar to the tools and instruments we use in the real world to interact with physical objects. The article presents the model, applies it to describe and compare a number of interaction techniques, and shows how it was used to create a new interface for searching and replacing text.'],\n",
       "   '2000']],\n",
       " [[['Designing storytelling technologies to encouraging collaboration between young children.',\n",
       "    'We describe the iterative design of two collaborative storytelling technologies for young children, KidPad and the Klump. We focus on the idea of designing interfaces to subtly encourage collaboration so that children are invited to discover the added benefits of working together. This idea has been motivated by our experiences of using early versions of our technologies in schools in Sweden and the UK. We compare the approach of encouraging collaboration with other approaches to synchronizing shared interfaces. We describe how we have revised the technologies to encourage collaboration and to reflect design suggestions made by the children themselves.'],\n",
       "   '2000']],\n",
       " [[['Power browser: efficient Web browsing for PDAs.',\n",
       "    \"We have designed and implemented new Web browsing facilities to support effective navigation on Personal Digital Assistants (PDAs) with limited capabilities: low bandwidth, small display, and slow CPU. The implementation supports wireless browsing from 3Com's Palm Pilot. An HTTP proxy fetches web pages on the client's behalf and dynamically generates summary views to be transmitted to the client. These summaries represent both the link structure and contents of a set of web pages, using information about link importance. We discuss the architecture, user interface facilities, and the results of comparative performance evaluations. We measured a 45% gain in browsing speed, and a 42% reduction in required pen movements.\"],\n",
       "   '2000']],\n",
       " [[['Bringing order to the Web: automatically categorizing search results.',\n",
       "    'We developed a user interface that organizes Web search results into hierarchical categories. Text classification algorithms were used to automatically classify arbitrary search results into an existing category structure on-the-fly. A user study compared our new category interface with the typical ranked list interface of search results. The study showed that the category interface is superior both in objective and subjective measures. Subjects liked the category interface much better than the list interface, and they were 50% faster at finding information that was organized into categories. Organizing search results allows users to focus on items in categories of interest rather than having to browse through all the results sequentially.'],\n",
       "   '2000']],\n",
       " [[['Tool support for cooperative object-oriented design: gesture based modelling on an electronic whiteboard.',\n",
       "    'Modeling is important in object-oriented software development. Although a number of Computer Aided Software Engineering (CASE) tools are available, and even though some are technically advanced, few developers use them. This paper describes our attempt to examine the requirements needed to provide tool support for the development process, and describes and evaluates a tool, Knight, which has been developed based on these requirements. The tool is based on a direct, whiteboard-like interaction achieved using gesture input on a large electronic whiteboard. So far the evaluations have been successful and the tool shows the potential of greatly enhancing current support for object-oriented modeling.'],\n",
       "   '2000']],\n",
       " [[['Comparing presentation summaries: slides vs. reading vs. listening.',\n",
       "    'As more audio and video technical presentations go online, it becomes imperative to give users effective summarization and skimming tools so that they can find the presentation they want and browse through it quickly. In a previous study, we reported three automated methods for generating audio-video summaries and a user evaluation of those methods. An open question remained about how well various text/image only techniques will compare to the audio-video summarizations. This study attempts to fill that gap. This paper reports a user study that compares four possible ways of allowing a user to skim a presentation: 1) PowerPoint slides used by the speaker during the presentation, 2) the text transcript created by professional transcribers from the presentation, 3) the transcript with important points highlighted by the speaker, and 4) a audio-video summary created by the speaker. Results show that although some text-only conditions can match the audio-video summary, users have a marginal preference for audio-video (ANOVA f=3.067, p=0.087). Furthermore, different styles of slide-authoring (e.g., detailed vs. big-points only) can have a big impact on their effectiveness as summaries, raising a dilemma for some speakers in authoring for on-demand previewing versus that for live audiences.'],\n",
       "   '2000']],\n",
       " [[['Interactive textbook and interactive Venn diagram: natural and intuitive interfaces on augmented desk system.',\n",
       "    'This paper describes two interface prototypes which we have developed on our augmented desk interface system, EnhancedDesk. The first application is Interactive Textbook, which is aimed at providing an effective learning environment. When a student opens a page which describes experiments or simulations, Interactive Textbook automatically retrieves digital contents from its database and projects them onto the desk. Interactive Textbook also allows the student hands-on ability to interact with the digital contents. The second application is the Interactive Venn Diagram, which is aimed at supporting effective information retrieval. Instead of keywords, the system uses real objects such as books or CDs as keys for retrieval. The system projects a circle around each book; data corresponding the book are then retrieved and projected inside the circle. By moving two or more circles so that the circles intersect each other, the user can compose a Venn diagram interactively on the desk. We also describe the new technologies introduced in EnhancedDesk which enable us to implement these applications.'],\n",
       "   '2000']],\n",
       " [[['Browsing digital video.',\n",
       "    \"Video in digital format played on programmable devices presents opportunities for significantly enhancing the user's viewing experience. For example, time compression and pause removal can shorten the viewing time for a video, textual and visual indices can allow personalized navigation through the content, and random-access digital storage allows instantaneous seeks into the content. To understand user behavior when such capabilities are available, we built a software video browsing application that combines many such features. We present results from a user study where users browsed video in six different categories: classroom lectures, conference presentations, entertainment shows, news, sports, and travel. Our results show that the most frequently used features were time compression, pause removal, and navigation using shot boundaries. Also, the behavior was different depending on the content type, and we present a classification. Finally, the users found the browser to be very useful. Two main reasons were: i) the ability to save time and ii) the feeling of control over what content they watched.\"],\n",
       "   '2000']],\n",
       " [[['DENIM: finding a tighter fit between tools and practice for Web site design.',\n",
       "    'Through a study of web site design practice, we observed that web site designers design sites at different levels of refinement&mdash;site map, storyboard, and individual page&mdash;and that designers sketch at all levels during the early stages of design. However, existing web design tools do not support these tasks very well. Informed by these observations, we created DENIM, a system that helps web site designers in the early stages of design. DENIM supports sketching input, allows design at different refinement levels, and unifies the levels through zooming. We performed an informal evaluation with seven professional designers and found that they reacted positively to the concept and were interested in using such a system in their work.'],\n",
       "   '2000']],\n",
       " [[['Visual similarity of pen gestures.',\n",
       "    'Pen-based user interfaces are becoming ever more popular. Gestures (i.e., marks made with a pen to invoke a command) are a valuable aspect of pen-based UIs, but they also have drawbacks. The challenge in designing good gestures is to make them easy for people to learn and remember. With the goal of better gesture design, we performed a pair of experiments to determine why users find gestures similar. From these experiments, we have derived a computational model for predicting perceived gesture similarity that correlates 0.56 with observation. We will incorporate the results of these experiments into a gesture design tool, which will aid the pen-based UI designer in creating gesture sets that are easier to learn and more memorable.'],\n",
       "   '2000']],\n",
       " [[['Providing integrated toolkit-level support for ambiguity in recognition-based interfaces.',\n",
       "    'Interfaces based on recognition technologies are used extensively in both the commercial and research worlds. But recognizers are still error-prone, and this results in human performance problems, brittle dialogues, and other barriers to acceptance and utility of recognition systems. Interface techniques specialized to recognition systems can help reduce the burden of recognition errors, but building these interfaces depends on knowledge about the ambiguity inherent in recognition. We have extended a user interface toolkit in order to model and to provide structured support for ambiguity at the input event level. This makes it possible to build re-usable interface components for resolving ambiguity and dealing with recognition errors. These interfaces can help to reduce the negative effects of recognition errors. By providing these components at a toolkit level, we make it easier for application writers to provide good support for error handling. Further, with this robust support, we are able to explore new types of interfaces for resolving a more varied range of ambiguity.'],\n",
       "   '2000']],\n",
       " [[['Non-isomorphic 3D rotational techniques.',\n",
       "    'This paper demonstrates how non-isomorphic rotational mappings and interaction techniques can be designed and used to build effective spatial 3D user interfaces. In this paper, we develop a mathematical framework allowing us to design non-isomorphic 3D rotational mappings and techniques, investigate their usability properties, and evaluate their user performance characteristics. The results suggest that non-isomorphic rotational mappings can be an effective tool in building high-quality manipulation dialogs in 3D interfaces, allowing our subjects to accomplish experimental tasks 13% faster without a statistically detectable loss in accuracy. The current paper will help interface designers to use non-isomorphic rotational mappings effectively.'],\n",
       "   '2000']],\n",
       " [[['The Task Gallery: a 3D window manager.',\n",
       "    'The Task Gallery is a window manager that uses interactive 3D graphics to provide direct support for task management and document comparison, lacking from many systems implementing the desktop metaphor. User tasks appear as artwork hung on the walls of a virtual art gallery, with the selected task on a stage. Multiple documents can be selected and displayed side-by-side using 3D space to provide uniform and intuitive scaling. The Task Gallery hosts any Windows application, using a novel redirection mechanism that routes input and output between the 3D environment and unmodified 2D Windows applications. User studies suggest that the Task Gallery helps with task management, is enjoyable to use, and that the 3D metaphor evokes spatial memory and cognition.'],\n",
       "   '2000']],\n",
       " [[['Talking in circles: designing a spatially-grounded audioconferencing environment.',\n",
       "    \"This paper presents Talking in Circles, a multimodal audioconferencing environment whose novel design emphasizes spatial grounding with the aim of supporting naturalistic group interaction behaviors. Participants communicate primarily by speech and are represented as colored circles in a two-dimensional space. Behaviors such as subgroup conversations and social navigation are supported through circle mobility as mediated by the environment and the crowd and distance-based attenuation of the audio. The circles serve as platforms for the display of identity, presence and activity: graphics are synchronized to participants' speech to aid in speech-source identification and participants can sketch in their circle, allowing a pictorial and gestural channel to complement the audio. We note user experiences through informal studies as well as design challenges we have faced in the creation of a rich environment for computer-mediated communication.\"],\n",
       "   '2000']],\n",
       " [[['Interacting with eye movements in virtual environments.',\n",
       "    \"Eye movement-based interaction offers the potential of easy, natural, and fast ways of interacting in virtual environments. However, there is little empirical evidence about the advantages or disadvantages of this approach. We developed a new interaction technique for eye movement interaction in a virtual environment and compared it to more conventional 3-D pointing. We conducted an experiment to compare performance of the two interaction types and to assess their impacts on spatial memory of subjects and to explore subjects' satisfaction with the two types of interactions. We found that the eye movement-based interaction was faster than pointing, especially for distant objects. However, subjects' ability to recall spatial information was weaker in the eye condition than the pointing one. Subjects reported equal satisfaction with both types of interactions, despite the technology limitations of current eye tracking equipment.\"],\n",
       "   '2000']],\n",
       " [[['Using naming time to evaluate quality predictors for model simplification.',\n",
       "    'Model simplification researchers require quality heuristics to guide simplification, and quality predictors to allow comparison of different simplification algorithms. However, there has been little evaluation of these heuristics or predictors. We present an evaluation of quality predictors. Our standard of comparison is naming time, a well established measure of recognition from cognitive psychology. Thirty participants named models of familiar objects at three levels of simplification. Results confirm that naming time is sensitive to model simplification. Correlations indicate that view-dependent image quality predictors are most effective for drastic simplifications, while view-independent three-dimensional predictors are better for more moderate simplifications.'],\n",
       "   '2000']],\n",
       " [[['The impact of fluid documents on reading and browsing: an observational study.',\n",
       "    'Fluid Documents incorporate additional information into a page by adjusting typography using interactive animation. One application is to support hypertext browsing by providing glosses for link anchors. This paper describes an observational study of the impact of Fluid Documents on reading and browsing. The study involved six conditions that differ along several dimensions, including the degree of typographic adjustment and the distance glosses are placed from anchors. Six subjects read and answered questions about two hypertext corpora while being monitored by an eyetracker. The eyetracking data revealed no substantial differenccs in eye behavior between conditions. Gloss placement was significant: subjects required less time to use nearby glosses. Finally, the reaction to the conditions was highly varied, with several conditions receiving both a best and worst rating on the subjective questionnaires. These results suggest implications for the design of dynamic reading environments.'],\n",
       "   '2000']],\n",
       " [[['Maintaining Views in Object-relational Databases.',\n",
       "    'View materialization is an important way of improving the performance of query processing. When an update occurs to the source data from which a materialized view is derived, the materialized view has to be updated so that it is consistent with the source data. This update process is called view maintenance . The incremental method of view maintenance, which computes the new view using the old view and the update to the source data, is widely preferred to full view recomputation when the update is small in size. In this paper we investigate how to incrementally maintain views in object-relational (OR) databases. The investigation focuses on maintaining views defined in OR-SQL, a language containing the features of object referencing, inheritance, collection, and aggregate functions including user-defined set aggregate functions. We propose an architecture and algorithms for incremental OR view maintenance. We implement all algorithms and analyze the performance of them in comparison with full view recomputation. The analysis shows that the algorithms significantly reduce the cost of updating a view when the size of an update to the source data is relatively small.'],\n",
       "   '2000']],\n",
       " [[['Sampling from Databases Using B+-Trees.',\n",
       "    'Sampling techniques are becoming increasingly important for very large databases. However, the problem of obtaining a random sample from index structures has not received much attention. In this paper, we examine sampling techniques for B^+-tree. As the fanout of each node varies, a random walk through the index structure does not produce a good representative sample of the data set. We propose a new technique, called B^+-Tree based Weighted Random Sampling (BTWRS), that alters the inclusion probabilities of records accordingly to allow more records from leaves, along the paths with higher fanouts, to be extracted. We extensively evaluated our method, and the results show that there is an improvement in BTWRS over the existing schemes in terms of the quality of the samples obtained and the efficiency of the sampling process. The proposed method can be readily adopted in existing commercial systems.'],\n",
       "   '2000']],\n",
       " [[['On the Problem of Computing the Well-Founded Semantics.',\n",
       "    'The well-founded semantics is one of the most widely studied and used semantics of logic programs with negation. In the case of finite propositional programs, it can be computed in polynomial time, more specifically, in O(&#x2223;At(P)&#x2223; &times; size(P)) steps, where size(P) denotes the total number of occurrences of atoms in a logic program P. This bound is achieved by an algorithm introduced by Van Gelder and known as the alternating-fixpoint algorithm. Improving on the alternating-fixpoint algorithm turned out to be difficult. In this paper we study extensions and modifications of the alternating-fixpoint approach. We then restrict our attention to the class of programs whose rules have no more than one positive occurrence of an atom in their bodies. For programs in that class we propose a new implementation of the alternating-fixpoint method in which false atoms are computed in a top-down fashion. We show that our algorithm is faster than other known algorithms and that for a wide class of programs it is linear and so, asymptotically optimal.'],\n",
       "   '2000']],\n",
       " [[['The Complexity of Tensor Calculus.',\n",
       "    \"Tensor calculus over semirings is shown relevant to complexity theory in unexpected ways. First, evaluating well-formed tensor formulas with explicit tensor entries is shown complete for ⊗P, for NP, and for #P as the semiring varies. Indeed the permanent of a matrix is shown expressible as the value of a tensor formula in much the same way that Berkowitz's theorem expresses its determinant. Second, restricted tensor formulas are shown to capture the classes LOGCFL and NL, their parity counterparts ⊗LOGCFL and ⊗L, and several other counting classes. Finally, the known inclusions NP/poly ⊆ ⊗P/poly, LOGCFL/poly ⊆ ⊗LOGCFL/poly, and NL/poly ⊆ ⊗L/poly, which have scattered proofs in the literature (Valiant & Vazirani 1986; Gál & Wigderson 1996), are shown to follow from the new characterizations in a single blow. As an intermediate tool, we define and make use of the natural notion of an algebraic Turing machine over a semiring S.\"],\n",
       "   '2000']],\n",
       " [[['Parameterized system design.',\n",
       "    \"Continued growth in chip capacity has led to new methodologies stressing reuse, not only of pre-designed processing components, but even of entire pre-designed architectures. To be used across a variety of applications, such architectures must be heavily parameterized, so they can adapt to those applications' differing constraints by trading off power, performance and size. We describe several parameterized system design issues, and provide results showing how a single architecture with easily configurable parameters can support a wide range of tradeoffs.\"],\n",
       "   '2000']],\n",
       " [[['Co-design of interleaved memory systems.',\n",
       "    'Memory interleaving is a cost-efficient approach to increase bandwidth. Improving data access locality and reducing memory access conflicts are two important aspects to achieve high efficiency for interleaved memory. In this paper, we introduce a design framework that integrates these two optimizations, in order to find out minimal memory banks and channels required in the embedded system under performance restriction. Several important techniques, loop and data layout transformations for data access locality, extracting data streams, conflict cache miss reduction as well as data placement and optimally reordered access for interleaved memories, are incorporated in the design framework. Experiments show that our co-design method results in substantially less hardware requirement compared to the implementations without optimization.'],\n",
       "   '2000']],\n",
       " [[['Code compression as a variable in hardware/software co-design.',\n",
       "    'We present a new way to practice and view handware/software co-design: rather than raising the level of abstraction in order to exploit the highest possible degree of optimization, we use code compression i.e. we practice co-design at the bit-level. Through our novel architecture combined with our compression methodology this results in optimization of all major design goals/constraints. In particular, we present a compression methodology that deploys what we call a &ldquo;post-cache architecture&rdquo; (i.e. the detached decompression unit is located between the CPU and the instruction cache). We present a design methodology that allows the designer to control parameters like speed, power, and area through the choice of compression parameters. In addition we show that our compression methodology (using a Markov Model) is more efficient than the widely used Huffman compression scheme.'],\n",
       "   '2000']],\n",
       " [[['An Empirical Evaluation of LFG-DOP.',\n",
       "    \"This paper presents an empirical assessment of the LFG-DOP model introduced by Bod & Kaplan (1998). The parser we describe uses fragments from LFG-annotated sentences to parse new sentences and Monte Carlo techniques to compute the most probable parse. While our main goal is to test Bod & Kaplan's model, we will also test a version of LFG-DOP which treats generalized fragments as previously unseen events. Experiments with the Verbmobil and Homecentre corpora show that our version of LFG-DOP outperforms Bod & Kaplan's model, and that LFG's functional information improves the parse accuracy of tree structures.\"],\n",
       "   '2000']],\n",
       " [[['Automatic Text Categorization by Unsupervised Learning.',\n",
       "    'The goal of text categorization is to classify documents into a certain number of predefined categories. The previous works in this area have used a large number of labeled training documents for supervised learning. One problem is that it is difficult to create the labeled training documents. While it is easy to collect the unlabeled documents, it is not so easy to manually categorize them for creating training documents. In this paper, we propose an unsupervised learning method to overcome these difficulties. The proposed method divides the documents into sentences, and categorizes each sentence using keyword lists of each category and sentence similarity measure. And then, it uses the categorized sentences for training. The proposed method shows a similar degree of performance, compared with the traditional supervised learning methods. Therefore, this method can be used in areas where low-cost text categorization is needed. It also can be used for creating training documents.'],\n",
       "   '2000']],\n",
       " [[['Parsing with the Shortest Derivation.',\n",
       "    'Common wisdom has it that the bias of stochastic grammars in favor of shorter derivations of a sentence is harmful and should be redressed. We show that the common wisdom is wrong for stochastic grammars that use elementary trees instead of context-free rules, such as Stochastic Tree-Substitution Grammars used by Data-Oriented Parsing models. For such grammars a non-probabilistic metric based on the shortest derivation outperforms a probabilistic metric on the ATIS and OVIS corpora, while it obtains competitive results on the Wall Street Journal (WSJ) corpus. This paper also contains the first published experiments with DOP on the WSJ.'],\n",
       "   '2000']],\n",
       " [[['A Class-based Probabilistic approach to Structural Disambiguation.',\n",
       "    'Knowledge of which words are able to fill particular argument slots of a predicate can be used for structural disambiguation. This paper describes a proposal for acquiring such knowledge, and in line with much of the recent work in this area, a probabilistic approach is taken. We develop a novel way of using a semantic hierarchy to estimate the probabilities, and demonstrate the general approach using a prepositional phrase attachment experiment.'],\n",
       "   '2000']],\n",
       " [[['Automatic Thesaurus Generation through Multiple Filtering.',\n",
       "    'In this paper, we propose a method of generating bilingual keyword clusters or thesauri from parallel or comparable bilingual corpora. The method combines morphological and lexical processing, bilingual word aligmnent, and graph-theoretic cluster generation. An experiment shows that the method is promising.'],\n",
       "   '2000']],\n",
       " [[['Structural Feature Selection For English-Korean Statistical Machine Translation.',\n",
       "    'When aligning texts in very different languages such as Korean and English, structural features beyond word or phrase give useful information. In this paper, we present a method for selecting structural features of two languages, from which we construct a model that assigns the conditional probabilities to corresponding tag sequences in bilingual English-Korean corpora. For tag sequence mapping between two languages, we first define a structural feature function which represents statistical properties of empirical distribution of a set of training samples. The system, based on maximum entropy concept, selects only features that produce high increases in loglikelihood of training samples. These structurally mapped features are more informative knowledge for statistical machine translation between English and Korean. Also, the information can help to reduce the parameter space of statistical alignment by eliminating syntactically unlikely alignments.'],\n",
       "   '2000']],\n",
       " [[['Tagging and Chunking with Bigrams.',\n",
       "    'In this paper we present an integrated system for tagging and chunking texts from a certain language. The approach is based on stochastic finite-state models that are learnt automatically. This includes biagram models or finite-state automata learnt using grammatical inference techniques. As the models involved in our system are learnt automatically, this is a very flexible and portable system.In order to show the viability of our approach we present results for tagging and chunking using bigram models on the Wall Street Journal corpus. We have achieved an accuracy rate for tagging of 96.8%, and a precision rate for NP chunks of 94.6% with a recall rate of 93.6%.'],\n",
       "   '2000']],\n",
       " [[['Automatic Extraction of Subcategorization Frames for Czech.',\n",
       "    'We present some novel machine learning techniques for the identification of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we are able to achieve 88% precision on unseen parsed text.'],\n",
       "   '2000']],\n",
       " [[['Experiments in Automated Lexicon Building for Text Searching.',\n",
       "    'This paper describes experiments in the automatic construction of lexicons that would be useful in searching large document collections for text fragments that address a specific information need, such as an answer to a question.'],\n",
       "   '2000']],\n",
       " [[['Robust German Noun Chunking With a Probabilistic Context-Free Grammar.',\n",
       "    'We present a noun chunker for German which is based on a head-lexicalised probabilistic context-free grammar. A manually developed grammar was semi-automatically extended with robustness rules in order to allow parsing of unrestricted text. The model parameters were learned from unlabelled training data by a probabilistic context-free parser. For extracting noun chunks, the parser generates all possible noun chunk analyses, scores them with a novel algorithm which maximizes the best chunk sequence criterion, and chooses the most probable chunk sequence. An evaluation of the chunker on 2,140 hand-annotated noun chunks yielded 92% recall and 93% precision.'],\n",
       "   '2000']],\n",
       " [[['A Statistical Approach to the Processing of Metonymy.',\n",
       "    'This paper describes a statistical approach to the interpretation of metonymy. A metonymy is received as an input, then its possible interpretations are ranked by applying a statistical measure. The method has been tested experimentally. It correctly interpreted 53 out of 75 metonymies in Japanese.'],\n",
       "   '2000']],\n",
       " [[['Finding Structural Correspondences from Bilingual Parsed Corpus for Corpus-based Translation.',\n",
       "    'In this paper, we describe a system and methods for finding structural correspondences from the paired dependency structures of a source sentence and its translation in a target language. The system we have developed finds word correspondences first, then finds phrasal correspondences based on word correspondences. We have also developed a GUI system with which a user can check and correct the correspondences retrieved by the system. These structural correspondences will be used as raw translation patterns in a corpus-based translation system.'],\n",
       "   '2000']],\n",
       " [[['DIASUMM: Flexible Summarization of Spontaneous Dialogues in Unrestricted Domains.',\n",
       "    'In this paper, we present a summarization system for spontaneous dialogues which consists of a novel multi-stage architecture. It is specifically aimed at addressing issues related to the nature of the texts being spoken vs. written and being dialogical vs. monological. The system is embedded in a graphical user interface and was developed and tested on transcripts of recorded telephone conversations in English and Spanish (CALLHOME).'],\n",
       "   '2000']],\n",
       " [[['On the Difficulty of Approximately Maximizing Agreements.',\n",
       "    'We address the computational complexity of learning in the agnostic framework. For a variety of common concept classes we prove that, unless P = NP, there is no polynomial time approximation scheme for finding a member in the class that approximately maximizes the agreement with a given training sample. In particular our results apply to the classes of monomials, axis-aligned hyper-rectangles, closed balls and monotone monomials. For each of these classes, we prove the NP-hardness of approximating maximal agreement to within some fixed constant (independent of the sample size and of the dimensionality of the sample space). For the class of half-spaces, we prove that, for any ε > 0, it is NP-hard to approximately maximize agreements to within a factor of (418/415 - ε), improving on the best previously known constant for this problem, and using a simpler proof. An interesting feature of our proofs is that, for each of the classes we discuss, we find patterns of training examples that, while being hard for approximating agreement within that concept class, allow efficient agreement maximization within other concept classes. These results bring up a new aspect of the model selection problem--they imply that the choice of hypothesis class for agnostic learning from among those considered in this paper can drastically effect the computational complexity of the learning process.'],\n",
       "   '2000']],\n",
       " [[['Logistic Regression, AdaBoost and Bregman Distances.',\n",
       "    'We give a unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances. The striking similarity of the two problems in this framework allows us to design and analyze algorithms for both simultaneously, and to easily adapt algorithms designed for one problem to the other. For both problems, we give new algorithms and explain their potential advantages over existing methods. These algorithms are iterative and can be divided into two types based on whether the parameters are updated sequentially (one at a time) or in parallel (all at once). We also describe a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as special cases, thus showing how the sequential and parallel approaches can themselves be unified. For all of the algorithms, we give convergence proofs using a general formalization of the auxiliary-function proof technique. As one of our sequential-update algorithms is equivalent to AdaBoost, this provides the first general proof of convergence for AdaBoost. We show that all of our algorithms generalize easily to the multiclass case, and we contrast the new algorithms with the iterative scaling algorithm. We conclude with a few experimental results with synthetic data that highlight the behavior of the old and newly proposed algorithms in different settings.'],\n",
       "   '2000']],\n",
       " [[['On the Efficiency of Noise-Tolerant PAC Algorithms Derived from Statistical Queries.',\n",
       "    'The Statistical Query (SQ) model provides an elegant means for generating noise-tolerant PAC learning algorithms that run in time inverse polynomial in the noise rate. Whether or not there is an SQ algorithm for every noise-tolerant PAC algorithm that is efficient in this sense remains an open question. However, we show that PAC algorithms derived from the Statistical Query model are not always the most efficient possible. Specifically, we give a general definition of SQ-based algorithm and show that there is a subclass of parity functions for which there is an efficient PAC algorithm requiring asymptotically less running time than any SQ-based algorithm. While it turns out that this result can be derived fairly easily by combining a recent algorithm of Blum, Kalai, and Wasserman with an older lower bound, we also provide alternate, Fourier-based approaches to both the upper and lower bounds that strengthen the results in various ways. The lower bound in particular is stronger than might be expected, and the amortized technique used in deriving this bound may be of independent interest.'],\n",
       "   '2000']],\n",
       " [[['PAC Analogues of Perceptron and Winnow via Boosting the Margin.',\n",
       "    \"We describe a novel family of PAC model algorithms for learning linear threshold functions. The new algorithms work by boosting a simple weak learner and exhibit sample complexity bounds remarkably similar to those of known online algorithms such as Perceptron and Winnow, thus suggesting that these well-studied online algorithms in some sense correspond to instances of boosting. We show that the new algorithms can be viewed as natural PAC analogues of the online p-norm algorithms which have recently been studied by Grove, Littlestone, and Schuurmans (1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory (pp. 171&ndash;183) and Gentile and Littlestone (1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory (pp. 1&ndash;11). As special cases of the algorithm, by taking p &equals; 2 and p &equals; &infin; we obtain natural boosting-based PAC analogues of Perceptron and Winnow respectively. The p &equals; &infin; case of our algorithm can also be viewed as a generalization (with an improved sample complexity bound) of Jackson and Craven's PAC-model boosting-based algorithm for learning &ldquo;sparse perceptrons&rdquo; (Jackson & Craven, 1996, Advances in neural information processing systems 8, MIT Press). The analysis of the generalization error of the new algorithms relies on techniques from the theory of large margin classification.\"],\n",
       "   '2000']],\n",
       " [[['The 2-center problem with obstacles.',\n",
       "    'Given a set S of n points in the plane and a set O of pairwise disjoint simple polygons with a total of m edges, we wish to find two congruent disks of smallest radius whose union covers S and whose centers lie outside the polygons in O (referred to as locational constraints in facility location theory). We present an algorithm to solve this problem in randomized expected time O(m log2(mn) + mn log2 n log(mn)). We also present an efficient approximation scheme that constructs, for a given ε > 0, two disks as above of radius at most (1 + ε)r*, where r* is the optimal radius, in time O(1/ε log(1/ε)(m log2 m + n log2 n)) or in randomized expected time O(1/ε log(1/ε)((m + n log n) log(mn))).'],\n",
       "   '2000']],\n",
       " [[['Secrecy and Group Creation.',\n",
       "    'We add an operation of group creation to the typed π-calculus, where a group is a type for channels. Creation of fresh groups has the effect of statically preventing certain communications, and can block the accidental or malicious leakage of secrets. Intuitively, no channel belonging to a fresh group can be received by processes outside the initial scope of the group, even if those processes are untyped. We formalize this intuition by adapting a notion of secrecy introduced by Abadi, and proving a preservation of secrecy property.'],\n",
       "   '2000']],\n",
       " [[['Random 3-SAT: The Plot Thickens.',\n",
       "    'This paper presents an experimental investigation of the following questions: how does the average-case complexity of random 3-SAT, understood as a function of the order (number of variables) for fixed density (ratio of number of clauses to order) instances, depend on the density? Is there a phase transition in which the complexity shifts from polynomial to exponential in the order? Is the transition dependent or independent of the solver? Our experiment design uses three complete SAT solvers embodying different algorithms: GRASP, CPLEX, and CUDD. We observe new phase transitions for all three solvers, where the median running time shifts from polynomial in the order to exponential. The location of the phase transition appears to be solver-dependent. GRASP shifts from polynomial to exponential complexity near the density of 3.8, CPLEX shifts near density 3, while CUDD exhibits this transition between densities of 0.1 and 0.5. This experimental result underscores the dependence between the solver and the complexity phase transition, and challenges the widely held belief that random 3-SAT exhibits a phase transition in computational complexity very close to the crossover point.'],\n",
       "   '2000']],\n",
       " [[['New Tractable Classes from Old.',\n",
       "    'The constraint satisfaction problem is known to be NP-hard in general, but a number of restrictions of the problem have been identified over the years which ensure tractability. This paper introduces two simple methods of combining two or more tractable classes over disjoint domains, in order to synthesise larger, more expressive tractable classes. We demonstrate that the classes so obtained are genuinely novel, and have not been previously identified. In addition, we use algebraic techniques to extend the tractable classes which we identify, and to show that the algorithms for solving these extended classes can be less than obvious.'],\n",
       "   '2000']],\n",
       " [[['Approximate String Matching over Ziv-Lempel Compressed Text.',\n",
       "    'We present the first nontrivial algorithm for approximate pattern matching on compressed text. The format we choose is the Ziv-Lempel family. Given a text of length u compressed into length n, and a pattern of length m, we report all the R occurrences of the pattern in the text allowing up to k insertions, deletions and substitutions. On LZ78/LZW we need O(mkn + R) time in the worst case and O(k2n + mk min(n, (mσ)k) + R) on average where σ is the alphabet size. The experimental results show a practical speedup over the basic approach of up to 2X for moderate m and small k. We extend the algorithms to more general compression formats and approximate matching models.'],\n",
       "   '2000']],\n",
       " [[['Using Web annotations for asynchronous collaboration around documents.',\n",
       "    'Digital web-accessible annotations are a compelling medium for personal comments and shared discussions around documents. Only recently supported by widely used products, \"in-context\" digital annotation is a relatively unexamined phenomenon. This paper presents a case study of annotations created by members of a large development team using Microsoft Office 2000-approximately 450 people created 9,000 shared annotations on about 1,250 documents over 10 months. We present quantitative data on use, supported by interviews with users, identifying strengths and weaknesses of the existing capabilities and possibilities for improvement.'],\n",
       "   '2000']],\n",
       " [[['How can cooperative work tools support dynamic group process? bridging the specificity frontier.',\n",
       "    'In the past, most collaboration support systems have focused on either automating fixed work processes or simply supporting communication in ad-hoc processes. This results in systems that are usually inflexible and difficult to change or that provide no specific support to help users decide what to do next.This paper describes a new kind of tool that bridges the gap between these two approaches by flexibly supporting processes at many points along the spectrum: from highly specified to highly unspecified. The development of this approach was strongly based on social science theory about collaborative work.'],\n",
       "   '2000']],\n",
       " [[['FieldWise: a mobile knowledge management architecture.',\n",
       "    'The paper presents results of a research project that has aimed at developing a knowledge management architecture for mobile work domains. The architecture developed, called FieldWise, was based on fieldwork in two organisations and feedback from users of prototype systems. This paper describes the empirically grounded requirements of FieldWise, how these have been realised in the architecture, and how the architecture has been implemented in the news journalism domain. FieldWise adds to the field of CSCW by offering an empirically grounded architecture with a set of novel features that have not been previously reported in the literature.'],\n",
       "   '2000']],\n",
       " [[['Coordination of communication: effects of shared visual context on collaborative work.',\n",
       "    \"We outline some of the benefits of shared visual information for collaborative repair tasks and report on a study comparing collaborative performance on a manual task by workers and helpers who are located side-by-side or connected via audio-video or audio-only links. Results show that the dyads complete the task more quickly and accurately when helpers are co-located than when they are connected via an audio link. However, they didn't achieve similar efficiency gains when they communicated through an audio/video link. These results demonstrate the value of a shared visual work space, but raise questions about the adequacy of current video communication technology for implementing it.\"],\n",
       "   '2000']],\n",
       " [[['The effects of filtered video on awareness and privacy.',\n",
       "    'Video-based media spaces are designed to support casual interaction between intimate collaborators. Yet transmitting video is fraught with privacy concerns. Some researchers suggest that the video stream be filtered to mask out potentially sensitive information. While a variety of filtering techniques exist, they have not been evaluated for how well they safeguard privacy. In this paper, we analyze how a blur and a pixelize video filter might impact both awareness and privacy in a media space. Each filter is considered at nine different levels of fidelity, ranging from heavily applied filter levels that mask almost all information, to lightly applied filters that reveal almost everything. We examined how well observers of several filtered video scenes extract particular awareness cues: the number of actors; their posture (moving, standing, seated); their gender; the visible objects (basic to detailed); and how available people look (their busyness, seriousness and approachability). We also examined the privacy-preserving potential of each filter level in the context of common workplace activities. Our results suggest that the blur filter, and to a lesser extent the pixelize filter, have a level suitable for providing awareness information while safeguarding privacy.'],\n",
       "   '2000']],\n",
       " [[['Ensuring privacy in presence awareness: an automated verification approach.',\n",
       "    'Providing information about other users and their activites is a central function of many collaborative applications. The data that provide this \"presence awareness\" are usually automatically generated and highly dynamic. For example, services such as AOL Instant Messenger allow users to observe the status of one another and to initiate and participate in chat sessions. As such services become more powerful, privacy and security issues regarding access to sensitive user data become critical. Two key software engineering challenges arise in this context:Policies regarding access to data in collaborative applications have subtle complexities, and must be easily modifiable during a collaboration. Users must be able to have a high degree of confidence that the implementations of these policies are correct. In this paper, we propose a framework that uses an automated verification approach to ensure that such systems conform to complex policies. Our approach takes advantage of VeriSoft, a recent tool for systematically testing implementations of concurrent systems, and is applicable to a wide variety of specification and development platforms for collaborative applications. we illustrate the key features of our framework by applying it to the development of a presence awareness system.'],\n",
       "   '2000']],\n",
       " [[['Supporting collaborative interpretation in distributed Groupware.',\n",
       "    \"Collaborative interpretationoccurs when a group interprets and transforms a diverse set of information fragments into a coherent set of meaningful descriptions. This activity is characterized byemergence, where the participants' shared understanding develops gradually as they interact with each other and the source material. Our goal is to support collaborative interpretation by small, distributed groups. To achieve this, we first observed how face-to-face groups perform collaborative interpretation in a particular work context. We then synthesized design principles from two relevant areas: the key behaviors of people engaged in activities where emergence occurs, and how distributed groups work together over visual surfaces. We built and evaluated a system that supports a specific collaborative interpretation task. This system provides a large workspace and several objects that encourages emergence in interpretation. People manipulatecardsthat contain the raw information fragments. They reduce complexity by placing duplicate cards intopiles. They suggest groupings as they manipulate the spatial layout of cards and piles. They enrich spatial layouts throughnotes, textandfreehand annotations. They record their understanding of their final groupings asreportscontaining coherent descriptions.\"],\n",
       "   '2000']],\n",
       " [[[\"GestureMan: a mobile robot that embodies a remote instructor's actions.\",\n",
       "    'When designing systems that support remote instruction on physical tasks, one must consider four requirements: 1) participants should be able to use non-verbal expressions, 2) they must be able to take an appropriate body arrangement to see and show gestures, 3) the instructor should be able to monitor operators and objects, 4) they must be able to organize the arrangement of bodies and tools and gestural expression sequentially and interactively. GestureMan was developed to satisfy these four requirements by using a mobile robot that embodies a remote instructors actions. The mobile robot mounts a camera and a remote control laser pointer on it. Based on the experiments with the system we discuss the advantage and disadvantage of the current implementation. Also, some implications to improve the system are described.'],\n",
       "   '2000']],\n",
       " [[['Developing adaptive groupware applications using a mobile component framework.',\n",
       "    'A need exists to develop groupware systems that adapt to available resources and support user mobility. This paper presents DACIA, a system that provides mechanisms for building such groupware applications. Using DACIA, components of a groupware application can be moved to different hosts during execution, while maintaining communication connectivity with groupware services and other users. DACIA provides mechanisms that simplify building groupware for domains where users are mobile. New collaboration features can be also more easily implemented. DACIA is also applicable to non-mobile environments. We show its applicability to building groupware applications that can be reconfigured at run-time to adapt to changing user demands and resource constraints, for example, by relocating services or introducing new services. This paper describes the architecture of DACIA and its use in building adaptable groupware systems.'],\n",
       "   '2000']],\n",
       " [[['Designing presentations for on-demand viewing.',\n",
       "    'Increasingly often, presentations are given before a live audience, while simultaneously being viewed remotely and recorded for subsequent viewing on-demand over the Web. How should video presentations be designed for web access? How is video accessed and used online? Does optimal design for live and on-demand audiences conflict? We examined detailed behavior patterns of more than 9000 on-demand users of a large corpus of professionally prepared presentations. We find that as many people access these talks on-demand as attend live. Online access patterns differ markedly from live attendance. People watch less overall and skip to different parts of a talk. Speakers designing presentations for viewing on-demand should emphasize key points early in the talk and early within each slide, use slide titles that reveal the talk structure and are meaningful outside the flow of the talk. In some cases the recommendations conflict with optimal design for live audiences. The results also provide guidance in developing tools for on-demand multimedia authoring and use.'],\n",
       "   '2000']],\n",
       " [[['Distance, dependencies, and delay in a global collaboration.',\n",
       "    'Collaborations over distance must contend with the loss of the rich, subtle interactions that co-located teams use to coordinate their work. Previous research has suggested that one consequence of this loss is that cross-site work will take longer than comparable single-site work. We use both survey data and data from the change management system to measure the extent of delay in a multi-site software development organization. We also measure site interdependence, differences in same-site and cross-site communication patterns, and analyze the relationship of these variables to delay. Our results show a significant relationship between delay in cross-site work and the degree to which remote colleagues are perceived to help out when workloads are heavy. This result is particularly troubling in light of the finding that workers generally believed they were as helpful to their remote colleagues as to their local colleagues. We discuss implications of our findings for collaboration technology for distributed organizations.'],\n",
       "   '2000']],\n",
       " [[['Consistency in replicated continuous interactive media.',\n",
       "    'In this paper we investigate how consistency can be ensured for replicated continuous interactive media, i.e., replicated media which change their state in reaction to user initiated operations as well as because of the passing of time. Typical examples for this media class are networked computer games and distributed VR applications. Existing approaches to reach consistency for replicated discrete interactive media are briefly outlined and it is shown that these fail in the continuous domain. In order to allow a thorough discussion of the problem, a formal definition of the term consistency in the continuous domain is given. Based on this definition we show that an important tradeoff relationship exists between the responsiveness of the medium and the appearance of short-term inconsistencies. Until now this tradeoff was not taken into consideration for consistency in the continuous domain, thereby severely limiting the consistency related fidelity for a large number of applications. We show that for those applications the fidelity can be significantly raised by voluntarily decreasing the responsiveness of the medium. This concept is called local lag. It enables the distribution of continuous interactive media that are more vulnerable to short-term inconsistencies than, e.g., battlefield simulations. We prove that the concept of local lag is valid by describing how local lag was successfully used to ensure consistency in a 3D telecooperation application.'],\n",
       "   '2000']],\n",
       " [[['Expertise recommender: a flexible recommendation system and architecture.',\n",
       "    'Locating the expertise necessary to solve difficult problems is a nuanced social and collaborative problem. In organizations, some people assist others in locating expertise by making referrals. People who make referrals fill key organizational roles that have been identified by CSCW and affiliated research. Expertise locating systems are not designed to replace people who fill these key organizational roles. Instead, expertise locating systems attempt to decrease workload and support people who have no other options. Recommendation systems are collaborative software that can be applied to expertise locating. This work describes a general recommendation architecture that is grounded in a field study of expertise locating. Our expertise recommendation system details the work necessary to fit expertise recommendation to a work setting. The architecture and implementation begin to tease apart the technical aspects of providing good recommendations from social and collaborative concerns.'],\n",
       "   '2000']],\n",
       " [[['Interaction and outeraction: instant messaging in action.',\n",
       "    'We discuss findings from an ethnographic study of instant messaging (IM) in the workplace and its implications for media theory. We describe how instant messaging supports a variety of informal communication tasks. We document the affordances of IM that support flexible, expressive communication. We describe some unexpected uses of IM that highlight aspects of communication which are not part of current media theorizing. They pertain to communicative processes people use to connect with each other and to manage communication, rather than to information exchange. We call these processes \"outeraction\". We discuss how outeractional aspects of communication affect media choice and patterns of media use.'],\n",
       "   '2000']],\n",
       " [[['Data management support for asynchronous groupware.',\n",
       "    'In asynchronous collaborative applications, users usually collaborate accessing and modifying shared information independently. We have designed and implemented a replicated object store to support such applications in distributed environments that include mobile computers. Unlike most data management systems, awareness support is integrated in the system. To improve the chance for new contributions, the system provides high data availability. The development of applications is supported by an object framework that decomposes objects in several components, each one managing a different aspect of object \"execution\". New data types may be created relying on pre-defined components to handle concurrent updates, awareness information, etc.'],\n",
       "   '2000']],\n",
       " [[['How does radical collocation help a team succeed?',\n",
       "    'Companies are experimenting with putting teams into warrooms, hoping for some productivity enhancement. We conducted a field study of six such teams, tracking their activity, attitudes, use of technology and productivity. Teams in these warrooms showed a doubling of productivity. Why? Among other things, teams had easy access to each other for both coordination of their work and for learning, and the work artifacts they posted on the walls remained visible to all. These results imply that if we are to truly support remote teams, we should provide constant awareness and easy transitions in and out of spontaneous meetings.'],\n",
       "   '2000']],\n",
       " [[['Coping with errors: the importance of process data in robust sociotechnical systems.',\n",
       "    'This paper presents an analysis of written and electronic records that document the collaborative process of packing museum artifacts in preparation for a move. The majority of data recorded detailed the process of packing, while only a small amount of the data concerned which artifacts were packed in which boxes. Museum staff members were able to use these process data to solve the numerous errors that occurred during packing. We explore the design implications for collaborative systems which focus on supporting error recovery rather than error prevention.'],\n",
       "   '2000']],\n",
       " [[['Copies convergence in a distributed real-time collaborative environment.',\n",
       "    \"In real-time collaborative systems, replicated objects, shared by users, are subject to concurrency constraints. In order to satisfy these, various algorithms, qualified as opðtimistic, [3, 5, 13, 17, 14, 15, 18], have been proposed that exploit the semantic properties of operations to serialize concurrent operations and achieve copy conðvergence of replicated objects. Their drawback is that they either reðquire a condition on user's operations which is hard to verify when possible to ensure, or they need undoðing then redoing operations in some situations. The main purpose of this paper is to present two new algorithms that overðcome these drawbacks. They are based upon the impleðmentation of a continuous global order which enables that condition to be released, and simplifies the operation inteðgration process. In the second algorithm, thanks to deðferred broadcast of operations to other sites, this process becomes even more simplified.\"],\n",
       "   '2000']],\n",
       " [[['Limits on Super-Resolution and How to Break Them.',\n",
       "    'Nearly all super-resolution algorithms are based on the fundamental constraints that the super-resolution image should generate the low resolution input images when appropriately warped and down-sampled to model the image formation process. (These reconstruction constraints are normally combined with some form of smoothness prior to regularize their solution.) In the first part of this paper, we derive a sequence of analytical results which show that the reconstruction constraints provide less and less useful information as the magnification factor increases. We also validate these results empirically and show that, for large enough magnification factors, any smoothness prior leads to overly smooth results with very little high-frequency content (however, many low resolution input images are used). In the second part of this paper, we propose a super-resolution algorithm that uses a different kind of constraint, in addition to the reconstruction constraints. The algorithm attempts to recognize local features in the low-resolution images and then enhances their resolution in an appropriate manner. We call such a super-resolution algorithm a hallucination or recogstruction algorithm. We tried our hallucination algorithm on two different data sets, frontal images of faces and printed Roman text. We obtained significantly better results than existing reconstruction-based algorithms, both qualitatively and in terms of RMS pixel error.'],\n",
       "   '2000']],\n",
       " [[['A Formal Classification of 3D Medial Axis Points and Their Local Geometry.',\n",
       "    'Abstract--This paper proposes a novel hypergraph skeletal representation for 3D shape based on a formal derivation ofthe generic structure of its medial axis. By classifying each skeletal point by its order of contact, we show that, generically, the medial axis consists of five types of points, which are then organized into sheets, curves, and points: 1) sheets (manifolds with boundary) which are the locus of bitangent spheres with regular tangency A_1^2 (A_k^n notation means n distinct k{\\\\hbox{-}}{\\\\rm{fold}} tangencies of the sphere of contact, as explained in the text); two types of curves, 2) the intersection curve of three sheets and the locus of centers of tritangent spheres, A_1^3, and 3) the boundary of sheets, which are the locus of centers of spheres whose radius equals the larger principal curvature, i.e., higher order contact A_3 points; and two types of points, 4) centers of quad-tangent spheres, A_1^4, and 5) centers of spheres with one regular tangency and one higher order tangency, A_1A_3. The geometry of the 3D medial axis thus consists of sheets (A_1^2) bounded by one type of curve (A_3) on their free end, which corresponds to ridges on the surface, and attached to two other sheets at another type of curve (A_1^3), which support a generalized cylinder description. The A_3 curves can only end in A_1A_3 points where they must meet an A_1^3 curve. The A_1^3 curves meet together in fours at an A_1^4 point. This formal result leads to a compact representation for 3D shape, referred to as the medial axis hypergraph representation consisting of nodes (A_1^4 and A_1 A_3 points), links between pairs of nodes (A_1^3 and A_3 curves) and hyperlinks between groups of links (A_1^2 sheets). The description of the local geometry at nodes by itself is sufficient to capture qualitative aspects of shapes, in analogy to 2D. We derive a pointwise reconstruction formula to reconstruct a surface from this medial axis hypergraph together with the radius function. Thus, this information completely characterizes 3D shape and lays the theoretical foundation for its use in recognition, morphing, design, and manipulation of shapes.'],\n",
       "   '2000']],\n",
       " [[['Reconstruction of a Scene with Multiple Linearly Moving Objects.',\n",
       "    'In this paper we describe an algorithm to recover the scene structure, the trajectories of the moving objects and the camera motion simultaneously given a monocular image sequence. The number of the moving objects is automatically detected without prior motion segmentation. Assuming that the objects are moving linearly with constant speeds, we propose a unified geometrical representation of the static scene and the moving objects. This representation enables the embedding of the motion constraints into the scene structure, which leads to a factorization-based algorithm. We also discuss solutions to the degenerate cases which can be automatically detected by the algorithm. Extension of the algorithm to weak perspective projections is presented as well. Experimental results on synthetic and real images show that the algorithm is reliable under noise.'],\n",
       "   '2000']],\n",
       " [[['Computing Optical Flow with Physical Models of Brightness Variation.',\n",
       "    'Although most optical flow techniques presume brightness constancy, it is well-known that this constraint is often violated, producing poor estimates of image motion. This paper describes a generalized formulation of optical flow estimation based on models of brightness variations that are caused by time-dependent physical processes. These include changing surface orientation with respect to a directional illuminant, motion of the illuminant, and physical models of heat transport in infrared images. With these models, we simultaneously estimate the 2D image motion and the relevant physical parameters of the brightness change model. The estimation problem is formulated using total least squares (TLS), with confidence bounds on the parameters. Experiments in four domains, with both synthetic and natural inputs, show how this formulation produces superior estimates of the 2D image motion.'],\n",
       "   '2000']],\n",
       " [[['Boosting Image Retrieval.',\n",
       "    'We present an approach for image retrieval using a very large number of highly selective features and efficient learning of queries. Our approach is predicated on the assumption that each image is generated by a sparse set of visual &ldquo;causes&rdquo; and that images which are visually similar share causes. We propose a mechanism for computing a very large number of highly selective features which capture some aspects of this causal structure (in our implementation there are over 46,000 highly selective features). At query time a user selects a few example images, and the AdaBoost algorithm is used to learn a classification function which depends on a small number of the most appropriate features. This yields a highly efficient classification function. In addition we show that the AdaBoost framework provides a natural mechanism for the incorporation of relevance feedback. Finally we show results on a wide variety of image queries.'],\n",
       "   '2000']],\n",
       " [[['Formal verification of iterative algorithms in microprocessors.',\n",
       "    'Contemporary microprocessors implement many iterative algorithms. For example, the front-end of a microprocessor repeatedly fetches and decodes instructions while updating internal state such as the program counter; floating-point circuits perform divide and square root computations iteratively. Iterative algorithms often have complex implementations because of performance optimizations like result speculation, re-timing and circuit redundancies. Verifying these iterative circuits against high-level specifications requires two steps: reasoning about the algorithm itself and verifying the implementation against the algorithm. In this paper we discuss the verification of four iterative circuits from Intel microprocessor designs. These verifications were performed using Forte, a custom-built verification system; we discuss the Forte features necessary for our approach. Finally, we discuss how we maintained these proofs in the face of evolving design implementations.'],\n",
       "   '2000']],\n",
       " [[['Symbolic guided search for CTL model checking.',\n",
       "    'CTL model checking of complex systems often suffers from the state-explosion problem. We propose using Symbolic Guided Search to avoid difficult-to-represent sections of the state space and prevent state explosion from occurring. Symbolic Guided Search applies hints to guide the exploration of the state space. In this way, the size of the BDDs involved in the computation is controlled, and the truth of a property may be decided before all states have been explored. In this work, we show how hints can be used in the computation of nested fixpoints. We show how to use hints to obtain overapproximations useful for greatest fixpoints, and we present the first results for backward search. Our experiments demonstrate the effectiveness of our approach.'],\n",
       "   '2000']],\n",
       " [[['Can recursive bisection alone produce routable placements?',\n",
       "    'This work focuses on congestion-driven placement of standard cells into rows in the fixed-die context. We summarize the state-of-the-art after two decades of research in recursive bisection placement and implement a new placer, called Capo, to empirically study the achievable limits of the approach. From among recently proposed improvements to recursive bisection, Capo incorporates a leading-edge multilevel min-cut partitioner [7], techniques for partitioning with small tolerance [8], optimal min-cut partitioners and end-case min-wirelength placers [5], previously unpublished partitioning tolerance computations, and block splitting heuristics. On the other hand, our &ldquo;good enough&rdquo; implementation does not use &ldquo;overlapping&rdquo; [17], multi-way partitioners [17, 20], analytical placement, or congestion estimation [24, 35]. In order to run on recent industrial placement instances, Capo must take into account fixed macros, power stripes and rows with different allowed cell orientations. Capo reads industry-standard LEF/DEF, as well as formats of the GSRC bookshelf for VLSI CAD algorithms [6], to enable comparisons on available placement instances in the fixed-die regime.Capo clearly demonstrates that despite a potential mismatch of objectives, improved mincut bisection can still lead to improved placement wirelength and congestion. Our experiments on recent industrial benchmarks fail to give a clear answer to the question in the title of this paper. However, they validate a series of improvements to recursive bisection and point out a need for transparent congestion management techniques that do not worsen the wirelength of already routable placements. Our experimental flow, which validates fixed-die placement results by violation-free detailed auto-routability, provides a new norm for comparison of VLSI placement implementations.'],\n",
       "   '2000']],\n",
       " [[['An architecture-driven metric for simultaneous placement and global routing for FPGAs.',\n",
       "    'FPGA routing resources typically consist of segments of various lengths. Due to the segmented routing architectures, the traditional measure of wiring cost (wirelength, delay, congestion, etc) based on geometric distance and/or channel density is no longer accurate for FPGAs. Researchers have shown that the number of segments, instead of geometric (Manhattan) distance, traveled by a net is the most crucial factor in controlling the routing delay and cost in an FPGA. Further, the congestion information of a routing channel shall be measured by the available segments of specific lengths, instead of the density in a channel alone. In this paper, we propose an architecture-driven metric for simultaneous FPGA placement and global routing. The new metric considers the available segments and their lengths to optimize the wiring cost for placement and global routing. Experiments by employing a cluster growth placement and maze routing to demonstrate the new metric show respective average reductions of 8%, 20%, and 19% in the number of tracks used (area), maximum net delay, and average net delay based on the Lucent Technologies ORCA2C-like and the Xilinx XC4000EX-like architectures, compared with the traditional metric of geometric distance and channel density.'],\n",
       "   '2000']],\n",
       " [[['B*-Trees: a new representation for non-slicing floorplans.',\n",
       "    'We present in this paper an efficient, flexible, and effective data structure, B*-trees for non-slicing floorplans. B*-trees are based on ordered binary trees and the admissible placement presented in [1]. Inheriting from the nice properties of ordered binary trees, B*-trees are very easy for implementation and can perform the respective primitive tree, operations search, insertion, and deletion in only O(1), O(1), and O(n) times while existing representations for non-slicing floorplans need at least O(n) time for each of these operations, where n is the number of modules. The correspondence between an admissible placement and its induced B*-tree is 1-to-1 (i.e., no redundancy); further, the transformation between them takes only linear time. Unlike other representations for non-slicing floorplans that need to construct constraint graphs for cost evaluation, in particular, the evaluation can be performed on B*-trees and their corresponding placements directly and incrementally. We further show the flexibility of B*-trees by exploring how to handle rotated, pre-placed, soft, and rectilinear modules. Experimental results on MCNC benchmarks show that the B*-tree representation runs about 4.5 times faster, consumes about 60% less memory, and results in smaller silicon area than the O-tree one [1]. We also develop a B*-tree based simulated annealing scheme for floorplan design; the scheme achieves near optimum area utilization even for rectilinear modules.'],\n",
       "   '2000']],\n",
       " [[['Performance driven multi-level and multiway partitioning with retiming.',\n",
       "    'In this paper, we study the performance driven multiw ay circuit partitioning problem with consideration of the significant difference of local and global interconnect delay induced by the partitioning. We develop an efficient algorithm HPM (Hierarc hicalP erformance driven Multi-level partitioning) that simultaneously considers cutsize and delay minimization with retiming. HPM builds a multi-lev el cluster hierarc hy and performs various refinement while gradually decomposing the clusters for simultaneous cutsize and delay minimization. We provide comprehensive experimental justification for each step involv ed in HPM and in-depth analysis of cutsize and delay tradeoff existing in the performance driven partitioning problem. HPM obtains (i) 7% to 23% better delay compared to the state-of-the-art cutsize driven hMetis [11] at the expense of 19% increase in cutsize, and (ii) 81% better cutsize compared to the state-of-the-art delay driven PRIME [2] at the expense of 6% increase in delay.'],\n",
       "   '2000']],\n",
       " [[['Unifying behavioral synthesis and physical design.',\n",
       "    'Our methodology unifies behavioral synthesis and physical design, allowing scheduling, allocation, binding, and placement to occur simultaneously. This is accomplished via set of defined transformation from both domains acting as forces in a single behavioral/physical system. Experiments show results with 50% less area and 10% lower critical path delay than the best results from a commercial behavioral synthesis tool. Our behavioral level area, delay, and individual component location estimates closely match results produced by physical design tools given only pin locations as a starting point.'],\n",
       "   '2000']],\n",
       " [[['Interconnect testing in cluster-based FPGA architectures.',\n",
       "    'As IC densities are increasing, cluster-based FPGA architectures are becoming the architecture of choice for major FPGA manufacturers. A cluster-based architecture is one in which several logic blocks are grouped together into a coarse-grained logic block. While the high density local interconnect often found within clusters serves to improve FPGA utilization, it also greatly complicates the FPGA interconnect testing problem. To address this issue, we have developed a hierarchical approach to define a set of FPGA configurations which enable interconnect faults to be detected. This technique enables the detection of bridging faults involving intra-cluster interconnect and extra-cluster interconnect. The hierarchical structure of a cluster-based tile is exploited to define intra-cluster configurations separately from extra-cluster configurations, thereby improving the efficiency of the configuration definition process. By guaranteeing that both intra-cluster and extra-cluster configurations have several test transparency properties, hierarchical fault detectability is ensured.'],\n",
       "   '2000']],\n",
       " [[['Distance driven finite state machine traversal.',\n",
       "    'Symbolic techniques have revolutionized reachability analysis in the last years. Extending their applicability to handle large, industrial designs is a key issue, involving the need to focus on memory consumption for BDD representation as well as time consumption to perform symbolic traversals of Finite State Machines (FSMs). We address the problem of reachability analysis for large FSMs, introducing a novel technique that performs reachability analysis using a sequence of &ldquo;distance driven&rdquo; partial traversals based on dynamically chosen prunings of the transition relation. Experiments are given to demonstrate the efficiency and robustness of our approach: We succeed in completing reachability problems with significantly smaller memory requirements and improved time performance.'],\n",
       "   '2000']],\n",
       " [[['Analysis of composition complexity and how to obtain smaller canonical graphs.',\n",
       "    'We discuss an open problem in construction of Reduced Ordered Binary Decision Diagrams (ROBDDs) using composition, and prove that the worst case complexity of the construction is truly cubic. With this insight we show that the process of composition naturally leads to the construction of (even exponentially) compact partitioned-OBDDs (POBDDs) [12]. Our algorithm which incorporates dynamic partitioning, leads to the most general (and compact) form of POBDDs - graphs with multiple root variables. To show that our algorithm is robust and practical, we have analyzed some well known problems in Boolean function representation, verification and finite state machine analysis where our approach generates graphs which are even orders of magnitude smaller.'],\n",
       "   '2000']],\n",
       " [[['Domino logic synthesis minimizing crosstalk.',\n",
       "    'Based on the new concept of crosstalk immunity set (CIS), procedures to minimize capacitive cross-coupling effects are developed for domino logic circuit. The nets in a crosstalk immunity set are free from crosstalk effects for any combination of input vectors. New algorithms for CIS identification and minimization are proposed to maximize the chance of crosstalk minimization in routing step. Our routing algorithm augmented with CIS information searches for optimal solution to minimize maximum crosstalk the circuit would experience. Experimental results show that the reduction of maximum crosstalk is up to 30% with CIS-augmented routing.'],\n",
       "   '2000']],\n",
       " [[['Forensic engineering techniques for VLSI CAD tools.',\n",
       "    'The proliferation of the Internet has affected the business model of almost all semiconductor and VLSI CAD companies that rely on intellectual property (IP) as their main source of revenues. The fact that IP has become more accessible and easily transferable, has influenced the emergence of copyright infringement as one of the most common obstructions to e-commerce of IP. In this paper, we propose a generic forensic engineering technique that addresses a number of copyright infringement scenarios. Given a solution SP to a particular optimization problem instance P and a finite set of algorithms A applicable to P, the goal is to identify with a certain degree of confidence the algorithm Ai which has been applied to P in order to obtain SP. We have applied forensic analysis principles to two problem instances commonly encountered in VLSI CAD: graph coloring and boolean satisfiability. We have demonstrated that solutions produced by strategically different algorithms can be associated with their corresponding algorithms with high accuracy.'],\n",
       "   '2000']],\n",
       " [[['Communication architecture tuners: a methodology for the design of high-performance communication architectures for systems-on-chips.',\n",
       "    \"In this chapter, we present a general methodology for the design of custom system-on-chip communication architectures. Our technique is based on the addition of a layer of circuitry, called the Communication Architecture Tuner (CAT), around any existing communication architecture topology. The added layer enhances the ability of the system to adapt to changing communication needs of its constituent components. For example, more critical data may be handled differently, leading to lower communication latencies. The CAT monitors the internal state and communication transactions of each component, and &ldquo;predicts&rdquo; the relative importance of each communication transaction in terms of its potential impact on different system-level performance metrics. It then configures the protocol parameters of the underlying communication architecture (e.g., priorities, DMA modes,etc.) to best suit the system's changing communication needs. We illustrate issues and tradeoffs involved in the design of CAT-based communication architectures, and present algorithms to automate the key steps. Experimental results indicate that performance metrics (e.g. number of missed deadlines, average processing time) for systems with CAT-based communication architectures are significantly (sometimes, over an order of magnitude) better than those with conventional communication architectures.\"],\n",
       "   '2000']],\n",
       " [[['Code compression for low power embedded system design.',\n",
       "    'We propose instruction code compression as an efficient method for reducing power on an embedded system. Our approach is the first one to measure and optimize the power consumption of a complete SOC (System--On--a--Chip) comprising a CPU, instruction cache, data cache, main memory, data buses and address bus through code compression. We compare the pre-cache architecture (decompressor between main memory and cache) to a novel post-cache architecture (decompressor between cache and CPU). Our simulations and synthesis results show that our methodology results in large energy savings between 22% and 82% compared to the same system without code compression. Furthermore, we demonstrate that power savings come with reduced chip area and the same or even improved performance.'],\n",
       "   '2000']],\n",
       " [[['To split or to conjoin: the question in image computation.',\n",
       "    'Image computation is the key step in fixpoint computations that are extensively used in model checking. Two techniques have been used for this step: one based on conjunction of the terms of the transition relation, and the other based on recursive case splitting. We discuss when one technique outperforms the other, and consequently formulate a hybrid approach to image computation. Experimental results show that the hybrid algorithm is much more robust than the &ldquo;pure&rdquo; algorithms and outperforms both of them in most cases. Our findings also shed light on the remark of several researchers that splitting is especially effective in approximate reachability analysis.'],\n",
       "   '2000']],\n",
       " [[['Schedulability-driven performance analysis of multiple mode embedded real-time systems.',\n",
       "    'Providing multiple modes to support dynamically changing environments, standards, and new services is prevalent in embedded systems, especially in mobile radio systems. Because such a system frequently contains time-constrained tasks, it is important to analyze the temporal requirements as well as the functional correctness. This paper presents a method to analyze temporal requirements imposed on an embedded real-time system supporting multiple modes. While most performance analysis methods focus only on testing the feasibility of a task or a system, our method goes further by addressing the problem of locating hot spots of a system thereby helping the designer to choose among alternative designs or architectures. We formally define the analysis problem and show that it is very unlikely to be solved efficiently. We present a heuristic algorithm, which is accurate and fast enough to be used in iterative processes in system-level analysis and design. The analysis problem is extended to accommodate probabilistic behavior exhibited by soft real-time tasks.'],\n",
       "   '2000']],\n",
       " [[['Boolean satisfiability in electronic design automation.',\n",
       "    'Boolean Satisfiability (SAT) is often used as the underlying model for a significant and increasing number of applications in Electronic Design Automation (EDA) as well as in many other fields of Computer Science and Engineering. In recent years, new and efficient algorithms for SAT have been developed, allowing much larger problem instances to be solved. SAT &ldquo;packages&rdquo; are currently expected to have an impact on EDA applications similar to that of BDD packages since their introduction more than a decade ago. This tutorial paper is aimed at introducing the EDA professional to the Boolean satisfiability problem. Specifically, we highlight the use of SAT models to formulate a number of EDA problems in such diverse areas as test pattern generation, circuit delay computation, logic optimization, combinational equivalence checking, bounded model checking and functional test vector generation, among others. In addition, we provide an overview of the algorithmic techniques commonly used for solving SAT, including those that have seen widespread use in specific EDA applications. We categorize these algorithmic techniques, indicating which have been shown to be best suited for which tasks.'],\n",
       "   '2000']],\n",
       " [[['Formal verification of superscale microprocessors with multicycle functional units, exception, and branch prediction.',\n",
       "    'We extend the Burch and Dill flushing technique [6] for formal verification of microprocessors to be applicable to designs where the functional units and memories have multicycle and possibly arbitrary latency. We also show ways to incorporate exceptions and branch prediction by exploiting the properties of the logic of Positive Equality with Uninterpreted Functions [4][5]. We study the modeling of the above features in different versions of dual-issue superscalar processors.'],\n",
       "   '2000']],\n",
       " [[['The design and use of simplepower: a cycle-accurate energy estimation tool.',\n",
       "    'In this paper, we presen t the design and use of a comprehensiv e framework, SimplePower, for ev aluating the effect of high-level algorithmic, architectural, and compilation trade-offs on energy. An execution-driven, cycle-accurate RT lev el energy estimation tool that uses transition sensitive energy models forms the cornerstone of this framework. SimplePower also pro vides the energy consumed in the memory system and on-chip buses using analytical energy models. We presen t the use of SimplePower to evaluate the impact of a new selective gated pipeline register optimization, a high-level data transformation and a pow er-conscious post compilation optimization (register relabeling) on the datapath, memory and on-chip bus energy, respectively. We find that these three optimizations reduce the energy by 18-36% in the datapath, 62% in the memory system and 12% in the instruction cache data bus, respectively.'],\n",
       "   '2000']],\n",
       " [[['System chip test: how will it impact your design?',\n",
       "    'A major challenge in realizing core-based system chips is the adoption and design-in of adequate test and diagnosis strategies. This tutorial paper discusses the specific challenges that come with testing deeply embedded reusable cores supplied by diverse providers, who often use different hardware description levels and mixed technologies. The paper describes a general test access architecture for embedded cores, and covers the current standardization efforts in this domain. In addition, we give an overview of the emerging EDA developments in SOC test, and illustrate the current industrial practices by means of two case studies.'],\n",
       "   '2000']],\n",
       " [[['SpaMod: Design of a Spatial Modeling Tool.',\n",
       "    'The aim of this paper is to present the design of a spatial modeling tool, called SpaMod, that is currently developed in Poitiers (France). SpaMod will allow users to represent and manipulate both discrete and continuous representations of geometrical objects. It is a topology based geometric modeling tool with three types of embeddings: the classical Euclidean embedding, the discrete matrix embedding (voxel or inter-pixel) and the discrete analytical embedding (discrete objects defined by inequations). In order for such a tool to htlfill its role, all three embeddings have to be available together. Conversions between embeddings is thus an important however, in 3D, still partially open question.'],\n",
       "   '2000']],\n",
       " [[['Representing Vertex-Based Simplicial Multi-complexes.',\n",
       "    'In this paper, we consider the problem of representing a multiresolution geometric model, called a Simplicial Multi-Complex (SMC), in a compact way. We present encoding schemes for both two- and three-dimensional SMCs built through a vertex insertion (removal) simplification strategy. We show that a good compression ratio is achieved not only with respect to a general-purpose data structure for a SMC, but also with respect to just encoding the complex at the maximum resolution.'],\n",
       "   '2000']],\n",
       " [[['Visualization in Algorithm Engineering: Tools and Techniques.',\n",
       "    'The process of implementing, debugging, testing, engineering and experimentally analyzing algorithmic codes is a complex and delicate task, fraught with many difficulties and pitfalls. In this context, traditional low-level textual debuggers or industrial-strength development environments can be of little help for algorithm engineers, who are mainly interested in high-level algorithmic ideas and not particularly in the language and platform-dependent details of actual implementations. Algorithm visualization environments provide tools for abstracting irrelevant program details and for conveying into still or animated images the high-level algorithmic behavior of a piece of software.In this paper we address the role of visualization in algorithm engineering. We survey the main approaches and existing tools and we discuss difficulties and relevant examples where visualization systems have helped developers gain insight about algorithms, test implementation weaknesses, and tune suitable heuristics for improving the practical performances of algorithmic codes.'],\n",
       "   '2000']],\n",
       " [[['Parameterized Complexity: The Main Ideas and Connections to Practical Computing.',\n",
       "    'The purposes of this paper are two: (1) to give an exposition of the main ideas of parameterized complexity, and (2) to discuss the connections of parameterized complexity to the systematic design of heuristics and approximation algorithms.'],\n",
       "   '2000']],\n",
       " [[['Using Finite Experiments to Study Asymptotic Performance.',\n",
       "    'In the analysis of algorithms we are interested in obtaining closed form expressions for algorithmic complexity, or at least asymptotic expressions in O(ċ)-notation. It is often possible to use experimental results to make significant progress towards this goal, although there are fundamental reasons why we cannot guarantee to obtain such expressions from experiments alone. This paper investigates two approaches relating to problems of developing theoretical analyses based on experimental data.We first consider the scientific method, which views experimentation as part of a cycle alternating with theoretical analysis. This approach has been very successful in the natural sciences. Besides supplying preliminary ideas for theoretical analysis, experiments can test falsifiable hypotheses obtained by incomplete theoretical analysis. Asymptotic behavior can also sometimes be deduced from stronger hypotheses which have been induced from experiments. As long as complete mathematical analyses remains elusive, well tested hypotheses may have to take their place. Several examples are given where average complexity can be tested experimentally so that support for hypotheses is quite strong.A second question is how to approach systematically the problem of inferring asymptotic bounds from experimental data. Five heuristic rules for \"empirical curve bounding\" are presented, ogether with analytical results guaranteeing correctness for certain families of functions. Experimental evaluations of the correctness and tightness of bounds obtained by the rules for several constructed functions and real datasets are described.'],\n",
       "   '2000']],\n",
       " [[['Reconstructing Optimal Phylogenetic Trees: A Challenge in Experimental Algorithmics.',\n",
       "    \"The benefits of experimental algorithmics and algorithm engineering need to be extended to applications in the computational sciences. In this paper, we present on one such application: the reconstruction of evolutionary histories (phylogenies) from molecular data such as DNA sequences. Our presentation is not a survey of past and current work in the area, but rather a discussion of what we see as some of the important challenges in experimental algorithmics that arise from computational phylogenetics. As motivational examples or examples of possible approaches, we briefly discuss two specific uses of algorithm engineering and of experimental algorithmics from our recent research. The first such use focused on speed: we reimplemented Sankoff and Blanchette's breakpoint analysis and obtained a 200, 000-fold speedup for serial code and 108-fold speedup on a 512-processor supercluster. We report here on the techniques used in obtaining such a speedup. The second use focused on experimentation: we conducted an extensive study of quartet-based reconstruction algorithms within a parameter-rich simulation space, using several hundred CPU-years of computation. We report here on the challenges involved in designing, conducting, and assessing such a study.\"],\n",
       "   '2000']],\n",
       " [[['Distributed Algorithm Engineering.',\n",
       "    'When one engineers distributed algorithms, some special characteristics arise that are different from conventional (sequential or parallel) computing paradigms. These characteristics include: the need for either a scalable real network environment or a platform supporting a simulated distributed environment; the need to incorporate asynchrony, where arbitrary asynchrony is hard, if not impossible, to implement; and the generation of \"difficult\" input instances which is a particular challenge. In this work, we identify some of the methodological issues required to address the above characteristics in distributed algorithm engineering and illustrate certain approaches to tackle them via case studies. Our discussion begins by addressing the need of a simulation environment and how asynchrony is incorporated when experimenting with distributed algorithms. We then proceed by suggesting two methods for generating difficult input instances for distributed experiments, namely a game-theoretic one and another based on simulations of adversarial arguments or lower bound proofs. We give examples of the experimental analysis of a pursuit-evasion protocol and of a shared memory problem in order to demonstrate these ideas. We then address a particularly interesting case of conducting experiments with algorithms for mobile computing and tackle the important issue of motion of processes in this context. We discuss the two-tier principle as well as a concurrent random walks approach on an explicit representation of motions in ad-hoc mobile networks, which allow at least for averagecase analysis and measurements and may give worst-case inputs in some cases. Finally, we discuss a useful interplay between theory and practice that arise in modeling attack propagation in networks.'],\n",
       "   '2000']],\n",
       " [[['Trace-Driven Memory Simulation: A Survey.',\n",
       "    'As the gap between processor and memory speeds continues to widen, methods for evaluating memory system designs before they are implemented in hardware are becoming increasingly important. One such method, trace-driven memory simulation, has been the subject of intense interest among researchers and has, as a result, enjoyed rapid development and substantial improvements during the past decade. This article surveys and analyzes these developments by establishing criteria for evaluating trace-driven methods, and then applies these criteria to describe, categorize, and compare over 50 trace-driven simulation tools. We discuss the strengths and weaknesses of different approaches and show that no single method is best when all criteria, including accuracy, speed, memory, flexibility, portability, expense, and ease of use are considered. In a concluding section, we examine fundamental limitations to trace-driven simulation, and survey some recent developments in memory simulation that may overcome these bottlenecks.'],\n",
       "   '2000']],\n",
       " [[['Implementations and Experimental Studies of Dynamic Graph Algorithms.',\n",
       "    'Dynamic graph algorithms have been extensively studied in the last two decades due to their wide applicability in many contexts. Recently, several implementations and experimental studies have been conducted investigating the practical merits of fundamental techniques and algorithms. In most cases, these algorithms required sophisticated engineering and fine-tuning to be turned into efficient implementations. In this paper, we survey several implementations along with their experimental studies for dynamic problems on undirected and directed graphs. The former case includes dynamic connectivity, dynamic minimum spanning trees, and the sparsification technique. The latter case includes dynamic transitive closure and dynamic shortest paths. We also discuss the design and implementation of a software library for dynamic graph algorithms.'],\n",
       "   '2000']],\n",
       " [[['Augmenting paper to enhance community information sharing.',\n",
       "    'Paper is traditionally considered as a major gap between the physical and electronic worlds, especially after the many attempts that have failed to attain a completely digital world. Paper based artifacts have many affordances that people want to continue to exploit. The work presented here is part of the Campiello project. It describes how the existing paper artifacts in use during the visits to cultural and tourist towns as well as artefacts used for local communities can be extended in order to become a bridge instead of a barrier to the richness of the digital world. After an introduction that describes the Campiello system and the principles that have driven its functionality, a complete design and the current implementation is presented. Finally, we discuss Paper Interface issues and survey the existing approaches in the field.'],\n",
       "   '2000']],\n",
       " [[['WebStickers: using physical tokens to access, manage and share bookmarks to the Web.',\n",
       "    'In the WebStickers system, where barcode stickers may be attached to physical objects making them act as bookmarks to the worldwide web in a convenient way to the user. Using readily available technology, i.e., standard barcode readers and adhesive stickers, WebStickers enable users to take advantage of their physical environment when organizing and sharing bookmarks. Starting from a user-centered rather than technology-driven point of view, we discuss how the affordances of physical tokens, as well as the context they are placed in, can act as useful cues for users. Since many objects already have barcodes printed on them, they can be used with the WebStickers system without physical modification. In addition, WebStickers meets proposed design criteria for information workspaces.'],\n",
       "   '2000']],\n",
       " [[['A comparison of spatial organization strategies in graphical and tangible user interfaces.',\n",
       "    'We present a study comparing how people use space in a Tangible User Interface (TUI) and in a Graphical User Interface (GUI). We asked subjects to read ten summaries of recent news articles and to think about the relationships between them. In our TUI condition, we bound each of the summaries to one of ten visually identical wooden blocks. In our GUI condition, each summary was represented by an icon on the screen. We asked subjects to indicate the location of each summary by pointing to the corresponding icon or wooden block. Afterward, we interviewed them about the strategies they used to position the blocks or icons during the task. We observed that TUI subjects performed better at the location recall task than GUI subjects. In addition, some TUI subjects used the spatial relationship between specific blocks and parts of the environment to help them remember the content of those blocks, while GUI subjects did not do this. Those TUI subjects who reported encoding information using this strategy tended to perform better at the recall task than those who did not.'],\n",
       "   '2000']],\n",
       " [[['Informative art: using amplified artworks as information displays.',\n",
       "    'Informative art is computer augmented, or amplified, works of art that not only are aesthetical objects but also information displays, in as much as they dynamically reflect information about their environment. Informative art can be seen as a kind of slow technology, i.e. a technology that promotes moments of concentration and reflection. Our aim is to present the design space of informative art. We do so by discussing its properties and possibilities in relation to work on information visualisation, novel information display strategies, as well as art. A number of examples based on different kinds of mapping relations between information and the properties of the composition of an artwork are described.'],\n",
       "   '2000']],\n",
       " [[['CyberCode: designing augmented reality environments with visual tags.',\n",
       "    'The CyberCode is a visual tagging system based on a 2D-barcode technology and provides several features not provided by other tagging systems. CyberCode tags can be recognized by the low-cost CMOS or CCD cameras found in more and more mobile devices, and it can also be used to determine the 3D position of the tagged object as well as its ID number. This paper describes examples of augmented reality applications based on CyberCode, and discusses some key characteristics of tagging technologies that must be taken into account when designing augmented reality environments.'],\n",
       "   '2000']],\n",
       " [[['Virtual Fault Simulation of Distributed IP-Based Designs.',\n",
       "    'Fault simulation and testability analysis are major concerns in design flows employing intellectual-property (IP) protected virtual components. In this paper we propose a paradigm for the fault simulation of IP-based designs that enables testability analysis without requiring IP disclosure, implemented within the JavaCAD framework for distributed design. As a proof of concept, stuck-at fault simulation has been performed for combinational circuits containing virtual components'],\n",
       "   '2000']],\n",
       " [[['Composite Signal Flow: A Computational Model Combining Events, Sampled Streams, and Vectors.',\n",
       "    'The composite signal flow model of computation targets systems with significant control and data processing parts. It builds on the data flow and synchronous data flow models and extends them to include three signal types: nonperiodic signals, sampled signals, and vectorized sampled signals. Vectorized sampled signals are used to represent vectors and computations on vectors. Several conversion processes are introduced to facilitate synchronization and communication with these signals. We discuss the severe implications, that these processes have on the causal behaviour of the system. We illustrate the model and its usefulness with three applications. A co-modelling and co-simulation environment combining Matlab and SDL; a high level timing analysis as a consequence of the operations on vectors; conditions for a parallel, distributed simulation'],\n",
       "   '2000']],\n",
       " [[['A BDD-Based Satisfiability Infrastructure Using the Unate Recursive Paradigm.',\n",
       "    'Binary Decision Diagrams have been widely used to solve the Boolean satisfiability (SAT) problem. The individual constraints can be represented using BDDs and the conjunction of all constraints provides all satisfying solutions. However, BDD-related SAT techniques suffer from size explosion problems. This paper presents two BDD-based algorithms to solve the SAT problem that attempt to contain the growth of BDD-size while identifying solutions quickly. The first algorithm, called BSAT, is a recursive, backtracking algorithm that uses an exhaustive search to find a SAT solution. The well known unate recursive paradigm is exploited to solve the SAT problem. The second algorithm is exploited to solve the SAT problem. The second algorithm, called INCOMPLETE-SEARCH-USAT (abbreviated IS-USAT), incorporates an incomplete search to find a solution. The search is incomplete inasmuch as it is restricted to only those regions that have a high likelihood of containing the solution, discarding the rest. Using our techniques we were able to find SAT solutions not only for all MCNC ISCAS benchmarks, but also for a variety of industry standard designs'],\n",
       "   '2000']],\n",
       " [[['Free MDD-Based Software Optimization Techniques for Embedded Systems.',\n",
       "    'Embedded systems make a heavy use of software to perform real-time embedded control tasks. Embedded software is characterized by a relatively long lifetime and by tight cost, performance and safety constraints. Several super-optimization techniques for embedded softwares based on multi-valued decision diagram (MDD) representations have been described in the literature, but they all share the same basic limitation. They are based on standard ordered MDD (OMDD) packages, and hence require a used order of evaluation for the MDD variables on every execution path. Free MDDs (FMDDs) lift this limitation, and hence open up more optimization opportunities. Finding the optimal variable ordering for FMDDs is a very difficult problem. Hence in this paper we describe a heuristic procedure that performs well in practice, and is based on FMDD cost estimation applied to recursive cofactoring. Experimental results show that our new variable ordering method obtains often. Smaller embedded software than previous (sifting-based) methods'],\n",
       "   '2000']],\n",
       " [[['Meeting Delay Constraints in DSM by Minimal Repeater Insertion.',\n",
       "    'We address the problem of inserting repeaters, selected from a library, at feasible locations in a placed and routed network to meet user-specified delay constraints for deep submicron (DSM) technology. We use minimal repeater area by taking advantage of slacks available in the network. Specifically, we transform the problem into an unconstrained optimization problem and solve it by iterative local refinement. We show that the optimal repeater locations and sizes that locally minimize the objective function in the unconstrained problem can be efficiently computed. We have implemented our algorithm and tested it on a set of benchmarks; experimental results are promising'],\n",
       "   '2000']],\n",
       " [[['Parallel and Distributed VHDL Simulation.',\n",
       "    \"This paper presents a methodology for parallel and distributed simulation of VHDL using the PDES (parallel discrete-event simulation) paradigm. To achieve better features and performance, some PDES protocols assume that simultaneous events may be processed in arbitrary order. We describe a solution of how to apply these algorithms to have a correct simulation of the distributed VHDL cycle, including the delta cycle. The solution is based on tie-breaking the simultaneous events using Lamport's logical clocks to causally order them according to the VHDL simulation cycle, and defining the VHDL virtual time as a pair of simulation physical time and cycle/phase logical time. The paper also shows how to use this method with a PDES protocol that relaxes the simulation of simultaneous events to arbitrary order; allowing the LPs to self-adapt to optimistic or conservative mode, without the lookahead requirement. The lookahead is application-dependent and for some systems may be zero or unknown. The parallel simulation of VHDL designs ranging from 5531 to 14704 LPs using these methods obtained a promising, almost linear speedup\"],\n",
       "   '2000']],\n",
       " [[['Bus Access Optimization for Distributed Embedded Systems Based on Schedulability Analysis.',\n",
       "    'We present an approach to bus access optimization and schedulability analysis for the synthesis of hard real-time distribution embedded systems. The communication model is based on a time-triggered protocol. We have developed an analysis for the communication delays proposing four different message scheduling policies over a time-triggered communication channel. Optimization strategies for the bus access scheme are developed, and the four approaches to message scheduling are compared using extensive experiments'],\n",
       "   '2000']],\n",
       " [[['System Level Design Using C++.',\n",
       "    'This paper discusses the use of C++ for the design of digital systems. The paper distinguishes a number of different approaches towards the use of programming languages for digital system design and will discuss in more detail how C++ can be used for system modeling and refinement, for simulation, and for architecture design'],\n",
       "   '2000']],\n",
       " [[['On-line algorithms for the channel assignment problem in cellular networks.',\n",
       "    'We consider the on-line channel assignment problem in the case of cellular networks and we formalize this problem as an on-line load balancing problem for temporary tasks with restricted assignment. For the latter problem, we provide a general solution (denoted as the cluster algorithm ) and we characterize its competitive ratio in terms of the combinatorial properties of the graph representing the network. We then compare the cluster algorithm with the greedy one when applied to the channel assignment problem: it turns out that the competitive ratio of the cluster algorithm is strictly better than the competitive ratio of the greedy algorithm. The cluster method is general enough to be applied to other on-line load balancing problems and, for some topologies, it can be proved to be optimal.'],\n",
       "   '2000']],\n",
       " [[['A decision-theoretic approach to resource allocation in wireless multimedia networks.',\n",
       "    'The allocation of scarce spectral resources to support as many user applications as possible while maintaining reasonable quality of service is a fundamental problem in wireless communication. We argue that the problem is best formulated in terms of decision theory. We propose a scheme that takes decision-theoretic concerns (like preferences) into account and discuss the difficulties and subtleties involved in applying standard techniques from the theory of Markov Decision Processes (MDPs) in constructing an algorithm that is decision-theoretically optimal. As an example of the proposed framework, we construct such an algorithm under some simplifying assumptions. Additionally, we present analysis and simulation results that show that our algorithm meets its design goals. Finally, we investigate how far from optimal one well-known heuristic is. The main contribution of our results is in providing insight and guidance for the design of near-optimal admission-control policies.'],\n",
       "   '2000']],\n",
       " [[['Clustering algorithms for wireless ad hoc networks.',\n",
       "    'Efficient clustering algorithms play a very important role in the fast connection establishment of ad hoc networks. In this paper, we describe a communication model that is derived directly from that of Bluetooth, an emerging technology for pervasive computing; this technology is expected to play a major role in future personal area network applications. We further propose two new distributed algorithms for clustering in wireless ad hoc networks. The existing algorithms often become infeasible because they use models where the discovering devices broadcast their Ids and exchange substantial information in the initial stages of the algorithm. We propose a 2-stage distributed O(N) randomized algorithm for an N node complete network, that always finds the minimum number of star-shaped clusters, which have maximum size. We then present a completely deterministic O(N) distributed algorithm for the same model, which achieves the same purpose. We describe in detail how these algorithms can be applied to Bluetooth for efficient scatternet formation. Finally, we evaluate both algorithms using simulation experiments based on the Bluetooth communication model, and compare their performance.'],\n",
       "   '2000']],\n",
       " [[['A Practical Algorithm to Find the Best Subsequence Patterns.',\n",
       "    'Given two sets of strings, consider the problem to find a subsequence that is common to one set but never appears in the other set. We regard it to find a subsequence pattern which separates these two sets. The problem is known to be NP-complete. We naturally generalize it to an optimization problem, where we try to find a subsequence pattern which maximally separates these two sets. We provide a practical algorithm to solve it exactly. Our algorithm uses two pruning heuristics based on the properties of subsequence languages, and utilizes the data structure called subsequence automata. We report some experimental results, which show these heuristics and the data structure contribute to reduce the search time.'],\n",
       "   '2000']],\n",
       " [[[': extracting relations from large plain-text collections.',\n",
       "    'Text documents often contain valuable structured data that is hidden Yin regular English sentences. This data is best exploited infavailable as arelational table that we could use for answering precise queries or running data mining tasks.We explore a technique for extracting such tables from document collections that requires only a handful of training examples from users. These examples are used to generate extraction patterns, that in turn result in new tuples being extracted from the document collection.We build on this idea and present our Snowball system. Snowball introduces novel strategies for generating patterns and extracting tuples from plain-text documents.At each iteration of the extraction process, Snowball evaluates the quality of these patterns and tuples without human intervention,and keeps only the most reliable ones for the next iteration. In this paper we also develop a scalable evaluation methodology and metrics for our task, and present a thorough experimental evaluation of Snowball and comparable techniques over a collection of more than 300,000 newspaper documents.'],\n",
       "   '2000']],\n",
       " [[['Patron-augmented digital libraries.',\n",
       "    'Digital library research is mostly focused on the generation of large collections of multimedia resources and state-of-the-art tools for their indexing and retrieval. However, digital libraries should provide more than advanced collection maintenance and retrieval services since the ultimate goal of any (academic) library is to serve the scholarly needs of its users. This paper begins by presenting a case for digital scholarship in which patrons perform all scholarly work electronically. A proposal is then made for patron-augmented digital libraries (PADLs), a class of digital libraries that supports the digital scholarship of its patrons. Finally, a prototype PADL (called Synchrony) providing access to video segments and associated textual transcripts is described. Synchrony allows patrons to search the library for artifacts, create annotations/original compositions, integrate these artifacts to form synchronized mixed text and video presentations and, after suitable review, publish these presentations into the digital library if desired. A study to evaluate the PADL concept and the usability of Synchrony is also discussed. The study revealed that participants were able to use Synchrony for the authoring and publishing of presentations and that attitudes toward PADLs were generally positive.'],\n",
       "   '2000']],\n",
       " [[['Server selection on the World Wide Web.',\n",
       "    'Significant efforts are being made to digitize rare and valuable library materials, with the goal of providing patrons and historians digital facsimiles that capture the \"look and feel\" of the original materials. This is often done by digitally photographing the materials and making high resolution 2D images available. The underlying assumption is that the objects are flat. However, older materials may not be flat in practice, being warped and crinkled due to decay, neglect, accident and the passing of time. In such cases, 2D imaging is insufficient to capture the \"look and feel\" of the original. For these materials, 3D acquisition is necessary to create a realistic facsimile. This paper outlines a technique for capturing an accurate 3D representation of library materials which can be integrated directly into current digitization setups. This will allow digitization efforts to provide patrons with more realistic digital facsimile of library materials.'],\n",
       "   '2000']],\n",
       " [[['A mediation infrastructure for digital library services.',\n",
       "    \"Digital library mediators allow interoperation between diverse information services. In this paper we describe a flexible and dynamic mediator infrastructure that allows mediators to be composed from a set of modules (``blades''). Each module implements a particular mediation function, such as protocol translation, query translation, or result merging. All the information used by the mediator, including the mediator logic itself, is represented by an RDF graph.We illustrate our approach using a mediation scenario involving a Dienst and a Z39.50 server, and we discuss the potential advantages and weaknesses of our framework.\"],\n",
       "   '2000']],\n",
       " [[['Content-based book recommending using learning for text categorization.',\n",
       "    \"Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes and dislikes. Most existing recommender systems use collaborative filtering methods that base recommendations on other users' preferences. By contrast,content-based methods use information about an item itself to make suggestions.This approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations. We describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization. Initial experimental results demonstrate that this approach can produce accurate recommendations.\"],\n",
       "   '2000']],\n",
       " [[['On the Quality of Service of Failure Detectors.',\n",
       "    'We study the quality of service (QoS) of failure detectors. By QoS, we mean a specification that quantifies (a) how fast the failure detector detects actual failures, and (b) how well it avoids false detections. We first propose a set of QoS metrics to specify failure detectors for systems with probabilistic behaviors, i.e., for systems where message delays and message losses follow some probability distributions. We then give a new failure detector algorithm and analyze its QoS in terms of the proposed metrics. We show that, among a large class of failure detectors, the new algorithm is optimal with respect to some of these QoS metrics. Given a set of failure detector QoS requirements, we show how to compute the parameters of our algorithm so that it satisfies these requirements, and we show how this can be done even if the probabilistic behavior of the system is not known. Finally, we briefly explain how to make our failure detector adaptive, so that it automatically reconfigures itself when there is a change in the probabilistic behavior of the network.'],\n",
       "   '2000']],\n",
       " [[['Implementing e-Transactions with Asynchronous Replication.',\n",
       "    'This paper describes a distributed algorithm that implements the abstraction of e-Transaction: a transaction that executes exactly-once despite failures. Our algorithm is based on an asynchronous replication scheme that generalizes well-known active-replication and primary-backup schemes. We devised the algorithm with a three-tier architecture in mind: the end-user interacts with front-end clients (e.g., browsers) that invoke middle-tier application servers (e.g., web servers) to access back-end databases. The algorithm preserves the three-tier nature of the architecture and introduces a very acceptable overhead with respect to unreliable solutions.'],\n",
       "   '2000']],\n",
       " [[['A Framework for the Combination and Characterization of Output Modalities.',\n",
       "    'This article proposes a framework that will help analyze current and future output multimodal user interfaces. We first define an output multimodal system. We then present our framework that identifies several different combinations of modalities and their characteristics. This framework assists in the selection of the most appropriate modalities for achieving efficient multimodal presentations. The discussion is illustrated with MulTab (Multimodal Table), an output multimodal system for managing large tables of numerical data.'],\n",
       "   '2000']],\n",
       " [[['A comparative study of static and profile-based heuristics for inlining.',\n",
       "    'In this paper, we present a comparative study of static and profile-based heuristics for inlining. Our motivation for this study is to use the results to design the best inlining algorithm that we can for the Jalape&ntilde;o dynamic optimizing compiler for Java [6]. We use a well-known approximation algorithm for the KNAPSACK problem as a common &ldquo;meta-algorithm&rdquo; for the inlining heuristics studied in this paper. We present performance results for an implementation of these inlining heuristics in the Jalape&ntilde;o dynamic optimizing compiler. Our performance results show that the inlining heuristics studied in this paper can lead to significant speedups in execution time (up to 1.68x) even with modest limits on code size expansion (at most 10%).'],\n",
       "   '2000']],\n",
       " [[['Derive: a tool that automatically reverse-engineers instruction encodings.',\n",
       "    'Many binary tools, such as disassemblers, dynamic code generation systems, and executable code rewriters, need to understand how machine instructions are encoded. Unfortunately, specifying such encodings is tedious and error-prone. Users must typically specify thousands of details of instruction layout, such as opcode and field locations values, legal operands, and jump offset encodings. We have built a tool called DERIVE that extracts these details from existing software: the system assembler. Users need only provide the assembly syntax for the instructions for which they want encodings. DERIVE automatically reverse-engineers instruction encoding knowledge from the assembler by feeding it permutations of instructions and doing equation solving on the output. DERIVE is robust and general. It derives instruction encodings for SPARC, MIPS, Alpha, PowerPC, ARM, and x86. In the last case, it handles variable-sized instructions, large instructions, instruction encodings determined by operand size, and other CISC features. DERIVE is also remarkably simple: it is a factor of 6 smaller than equivalent, more traditional systems. Finally, its declarative specifications eliminate the mis-specification errors that plague previous approaches, such as illegal registers used as operands or incorrect field offsets and sizes. This paper discusses our current DERIVE prototype, explains how it computes instruction encodings, and also discusses the more general implications of the ability to extract functionality from installed software.'],\n",
       "   '2000']],\n",
       " [[['Machine-adaptable dynamic binary translation.',\n",
       "    \"Dynamic binary translation is the process of translating and optimizing executable code for one machine to another at runtime, while the program is &ldquo;executing&rdquo; on the target machine. Dynamic translation techniques have normally been limited to two particular machines; a competitor's machine and the hardware manufacturer's machine. This research provides for a more general framework for dynamic translations, by providing a framework based on specifications of machines that can be reused or adapted to new hardware architectures. In this way, developers of such techniques can isolate design issues from machine descriptions and reuse many components and analyses. We describe our dynamic translation framework and provide some initial results obtained by using this system.\"],\n",
       "   '2000']],\n",
       " [[['A framework for remote dynamic program optimization.',\n",
       "    'Dynamic program optimization allows programs to be generated that are highly tuned for a given environment and input data set. Optimization techniques can be applied and re-applied as program and machine characteristics are discovered and change. In most dynamic optimization and compilation frameworks, the time spent in code generation and optimization must be minimized since it is directly reflected in the total program execution time. We propose a generic framework for remote dynamic program optimization that mitigates this need. A local optimizer thread monitors the program as it executes and selects program sections that should be optimized. An optimizer, running on a remote machine or a free processor of a multiprocessor, is then called to actually perform the optimization and generate a new code variant for the section. A dynamic selector is used to select the most appropriate code variant for each code interval based upon the current runtime environment. We describe this framework in detail and present an example of its use on a simple application. We show that our framework, when used with changing input, can outperform the best statically optimized version of the application.'],\n",
       "   '2000']],\n",
       " [[['Consistency-Based Diagnosis of Configuration Knowledge Bases.',\n",
       "    'Configuration problems are a thriving application area for declarative knowledge representation that currently experiences a constant increase in size and complexity of knowledge bases. Automated support of the debugging process of such knowledge bases is a necessary prerequisite for effective development of configurators. We show that this task can be achieved by consistency-based diagnosis techniques. Based on the formal definition of consistency-based configuration we develop a framework suitable for diagnosing configuration knowledge bases. During the test phase of configurators, valid and invalid examples are used to test the correctness of the system. In case such examples lead to unintended results, debugging of the knowledge base is initiated. Starting from a clear definition of diagnosis in the configuration domain we develop an algorithm based on conflicts. Our framework is general enough for its adaptation to diagnosing customer requirements to identify unachievable conditions during configuration sessions.A prototype implementation using commercial constraint-based configurator libraries shows the feasibility of diagnosis within the tight time bounds of interactive debugging sessions. Finally, we discuss the usefulness of the outcomes of the diagnostic process in different scenarios.'],\n",
       "   '2000']],\n",
       " [[['IMPSAC: Synthesis of Importance Sampling and Random Sample Consensus.',\n",
       "    'This paper proposes a new method for recovery of epipolar geometry and feature correspondence between images which have undergone a significant deformation, either due to large rotation or wide baseline of the cameras. The method also encodes the uncertainty by providing an arbitrarily close approximation to the posterior distribution of the two view relation. The method operates on a pyramid from coarse to fine resolution, thus raising the problem of how to propagate information from one level to another in a statistically consistent way. The distribution of the parameters at each resolution is encoded nonparametrically as a set of particles. At the coarsest level, a RANSAC-MCMC estimator is used to initialize this set of particles, the posterior can then be approximated as a mixture of Gaussians fitted to these particles. The distribution at a coarser level influences the distribution at a finer level using the technique of sampling-importance-resampling (SIR) and MCMC, which allows for asymptotically correct approximations of the posterior distribution. The estimate of the posterior distribution at the level above is being used as the importance sampling function to generate a new set of particles, which can be further improved by MCMC. It is shown that the method is superior to previous single resolution RANSAC-style feature matchers.'],\n",
       "   '2000']],\n",
       " [[['Exception Handling in Object-Oriented Databases.',\n",
       "    'Exceptions in database systems can be used for two different purposes: to store data not conforming to the description provided by the database schema, that is, exceptional data; and to handle exceptional situations during processing, that is, the usual execution exceptions of programming languages. In this paper we survey approaches to both kinds of exceptions in OODBMSs, we discuss some uses of exceptions peculiar to databases, and relate exceptions with triggers, a typical database functionality.'],\n",
       "   '2000']],\n",
       " [[['Error Handling in Process Support Systems.',\n",
       "    'Process Support Systems (PSSs) are software systems supporting the modeling, enactment, monitoring, and analysis of business processes. Process automation technology can be fully exploited when predictable and repetitive processes are executed. Unfortunately, many processes are faced with the need of managing exceptional situations that may occur during their execution, and possibly even more exceptions and failures can occur when the process execution is supported by a PSS. Exceptional situations may be caused by system (hardware or software) failures, or may by related to the semantics of the business process.In this paper we introduce a taxonomy of failures and exceptions and discuss the effect that they can have on a PSS and on its ability to support business processes. Then, we present the main approaches that commercial PSSs and research prototypes offer in order to capture and react to exceptional situations, and we show which classes of failure or exception can be managed by each approach.'],\n",
       "   '2000']],\n",
       " [[['ADOME-WFMS: Towards Cooperative Handling of Workflow Exceptions.',\n",
       "    'Exception handling in workflow management systems (WFMSs) is a very important problem since it is not possible to specify all possible outcomes and alternatives. Effective reuse of existing exception handlers can greatly help in dealing with workflow exceptions. On the other hand, cooperative support for user-driven resolution of unexpected exceptions and workflow evolution at run-time is vital for an adaptive WFMS. We have developed ADOME-WFMS via a meta-modeling approach as a comprehensive framework in which the problem of workflow exception handling can be adequately addressed. In this chapter, we present an overview of exception handling in ADOME-WFMS with procedures for supporting the following: reuse of exception handlers, thorough and automated resolution of expected exceptions, effective management of Problem Solving Agents, cooperative exception handling, user-driven computer supported resolution of unexpected exceptions, and workflow evolution.'],\n",
       "   '2000']],\n",
       " [[['An Architectural-Based Reflective Approach to Incorporating Exception Handling into Dependable Software.',\n",
       "    \"Modern object-oriented software is inherently complex and has to cope with an increasing number of exceptional conditions to meet the system's dependability requirements. In this context, the goal of our work is twofold: (i) to present an exception handhng model which is suitable for developing dependable object-oriented software, and (ii) to provide a systematic approach to incorporating exception handling during the design stage, that is, from the architectural design stage to the detailed design stage. The proposed approach employs the computational reflection concept to achieve a clear and transparent separation of concerns between the application's functionality and the exception handling facilities. This separation minimizes the complexity caused by the handling of abnormal behavior and facilitates the task of building dependable software with better readability, maintainability and reusability.\"],\n",
       "   '2000']],\n",
       " [[['Action-Oriented Exception Handling in Cooperative and Competitive Concurrent Object-Oriented Systems.',\n",
       "    'The chief aim of this survey is to discuss exception handing models which have been developed for concurrent object systems. In conducting this discussion we rely on the following fundamental principles: exception handling should be associated with structuring techniques; concurrent systems require exception handling which is different from that used in sequential systems; concurrent systems are best structured out of (nested) actions; atomicity of actions is crucial for developing complex systems. In this survey we adhere to the well-known classification of concurrent systems, developed in the 70s by C.A.R. Hoare, J.J. Horning and B. Randell, into cooperative, competitive and disjoint ones. Competitive systems are structured using atomic transactions. Atomic actions are used for structuring cooperative systems. Complex systems in which components can compete and cooperate are structured using Coordinated Atomic actions. The focus of the survey is on outlining models and schemes which combine these action-based structuring approaches with exception handling. In conclusion we emphasise that exception handling models should be adequate to the system development paradigm and structuring approaches used.'],\n",
       "   '2000']],\n",
       " [[['Portable Implementation of Continuation Operators in Imperative Languages by Exception Handling.',\n",
       "    'This paper describes a scheme of manipulating (partial) continuations in imperative languages such as Java and C++ in a portable manner, where the <i>portability</i> means that this scheme does not depend on structure of the native stack frame nor implementation of virtual machines and runtime systems. Exception handling plays a significant role in this scheme to reduce overheads. The scheme is based on program transformation, but in contrast to CPS transformation, our scheme preserves the call graph of the original program. This scheme has two important applications: <i>transparent migration</i> in mobile computation and <i>checkpointing</i> in a highly reliable system. The former technology enables running computations to move to a remote computer, while the latter one enables running computations to be saved into storages.'],\n",
       "   '2000']],\n",
       " [[['Exception Handling in Agent-Oriented Systems.',\n",
       "    'Agent-oriented programming may be the next generation paradigm to try and tame the software complexity beast. Agents are active objects capable of autonomous behavior. Mobility can be one of the attributes of agents in open systems. A software system could be structured as a dynamic, and possibly evolving, ensemble of cooperating agents. However, there is very little in the literature on how to effectively handle exceptions in agent-oriented software systems. Agent-oriented systems have all the exception handling concerns of sequential and concurrent systems, as well as some new issues that arise due to mobility and security in open systems. This paper develops an exception handling model whose salient feature is the separation and encapsulation of exception handling for an agent environment in a special agent called a <i>guardian</i>. The model presented here builds upon the notions of events, exceptions, notifications, and commands in an agent ensemble, and presents a number of exception handling patterns that can be used by a guardian. The model presented here is being investigated in the context of the Ajanta mobile agent programming system.'],\n",
       "   '2000']],\n",
       " [[['Introduction to Stochastic Petri Nets.',\n",
       "    'Stochastic Petri Nets are a modelling formalism that can be conveniently used for the analysis of complex models of Discrete Event Dynami Systems (DEDS) and for their performance and reliability evaluation. The automatic construction of the probabilistic models that underly the dynamic behaviours of these nets rely on a set of results that derive from the theory of untimed Petri nets. The paper introduces the basic motivations for modelling DEDS and briefly overviews the basic results of net theory that are useful for the definition of Stochastic Petri Nets and Generalized Stochastic Petri Nets. The different approaches that have been used for introducing the concept of time in these models are discussed in order to provide the basis for the definition of SPNs and GSPNs as well. Details on the solution techniques and on their computational aspects are provided. A brief overview of more advanced material is included at the end of the paper to highlight the state of the art in this field and to give pointers to relevant results published in the literature.'],\n",
       "   '2000']],\n",
       " [[['Process Algebra and Markov Chains.',\n",
       "    'This paper surveys and relates the basic concepts of process algebra and the modelling of continuous time Markov chains. It provides basic introductions to both fields, where we also study the Markov chains from an algebraic perspective, viz. that of Markov chain algebra. We then proceed to study the interrelation of reactive processes and Markov chains in this setting, and introduce the algebra of Interactive Markov Chains as an orthogonal extension of both process and Markov chain algebra. We conclude with comparing this approach to related (Markovian) stochastic process algebras by analysing the algebraic principles that they support.'],\n",
       "   '2000']],\n",
       " [[['Markovian Models for Performance and Dependability Evaluation.',\n",
       "    'Markovian models have been used for about a century now for the evaluation of the performance and dependability of computer and communication systems. In this paper, we give a concise overview of the most widely used classes of Markovian models, their solution and application. After a brief introduction to performance and dependability evaluation in general, we introduce discrete-time. Markov chains, continuous-time Markov chains and semi-Markov chains. Stepwisely, we develop the main equations that govern the steady-state and the transient behaviour of such Markov chains. We thereby emphasise on intuitively appealing explanations rather than on mathematical rigor. The relation between the various Markov chain types is explained in detail. Then, we discuss means to numerically solve the systems of linear equations (both direct and iterative ones) and the systems of differential equations that arise when solving for the steady-state and transient behaviour of Markovian models.'],\n",
       "   '2000']],\n",
       " [[['General Distributions in Process Algebra.',\n",
       "    'This paper is an informal tutorial on stochastic process algebras, i.e., process calculi where action occurrences may be subject to a delay that is governed by a (mostly continuous) random variable. Whereas most stochastic process algebras consider delays determined by negative exponential distributions, this tutorial is concerned with the integration of general, non-exponential distributions into a process algebraic setting. We discuss the issue of incorporating such distributions in an interleaving semantics, and present some existing solutions to this problem. In particular, we present a process algebra for the specification of stochastic discrete-event systems modeled as generalized semi-Markov chains (GSMCs). Using this language stochastic discrete-event systems can be described in an abstract and modular way. The operational semantics of this process algebra is given in terms of stochastic automata, a novel mixture of timed automata and GSMCs. We show that GSMCs are a proper subset of stochastic automata, discuss various notions of equivalence, present congruence results, treat equational reasoning, and argue how an expansion law in the process algebra can be obtained. As a case study, we specify the root contention phase within the standardized IEEE 1394 serial bus protocol and study the delay until root contention resolution. An overview of related work on general distributions in process algebra and a discussion of trends and future work complete this tutorial.'],\n",
       "   '2000']],\n",
       " [[['Verification of Randomized Distributed Algorithms.',\n",
       "    'We describe modular verification techniques for randomized distributed algorithms as extensions of techniques for ordinary, non-randomized, distributed algorithms. The main difficulty to overcome arises from the subtle interplay between probability and nondeterminism, where probability is due to the random choices that occur within an algorithm, and nondeterminism is due to the unknown speeds and scheduling policies of the processes. The techniques that we introduce are based on separation of probability from nondeterminism. When the nondeterminism is factored out, the analysis of an algorithm has several pieces that are in common with the area of performance evaluation. Thus, the techniques that we describe are likely to constitute a bridge to export typical performance evaluation techniques to the area of concurrent nondeterministic systems and, vice versa, to understand alternative ways for handling nondeterminism when it arises.'],\n",
       "   '2000']],\n",
       " [[['Constructing Automata from Temporal Logic Formulas: A Tutorial.',\n",
       "    'This paper presents a tutorial introduction to the construction of finite-automata on infinite words from linear-time temporal logic formulas. After defining the source and target formalisms, it describes a first construction whose correctness is quite direct to establish, but whose behavior is always equal to the worst-case upper bound. It then turns to the techniques that can be used to improve this algorithm in order to obtain the quite effective algorithms that are now in use.'],\n",
       "   '2000']],\n",
       " [[['Ontological Analysis of Taxonomic Relationships.',\n",
       "    \"Taxonomies based on a partial-ordering relation commonly known as is-a, class inclusion or subsumption have become an important tool in conceptual modeling. A well-formed taxonomy has significant implications for understanding, reuse, and integration, however the intuitive simplicity of taxonomic relations has led to widespread misuse, making clear the need for rigorous analysis techniques. Where previous work has focused largely on the semantics of the is-a relation itself, we concentrate here on the ontological nature of the arguments of this relation, in order to be able to tell whether a single is-a link is ontologically well-founded. For this purpose, we discuss techniques based on the philosophical notions of identity, unity, essence, and dependence, which have been adapted to the needs of information systems design. We demonstrate the effectiveness of these techniques by taking real examples of poorly structured taxonomies, and revealing cases of invalid generalization. The result of the analysis is a cleaner taxonomy that clarifies the modeler's ontological commitments.\"],\n",
       "   '2000']],\n",
       " [[['X-Ray - Towards Integrating XML and Relational Database Systems.',\n",
       "    'Relational databases get more and more employed in order to store the content of a web site. At the same time, XML is fast emerging as the dominant standard at the hypertext level of web site management describing pages and links between them. Thus, the integration of XML with relational database systems to enable the storage, retrieval and update of XML documents is of major importance. This paper presents X-Ray, a generic approach for integrating XML with relational database systems. The key idea is that mappings may be defined between XML DTDs and relational schemata while preserving their autonomy. This is made possible by introducing a meta schema and meta knowledge for resolving data model heterogeneity and schema heterogeneity. Since the mapping knowledge is not hard-coded but rather reified within the meta schema, maintainability and changeability is enhanced. The meta schema provides the basis for X-Ray to automatically compose XML documents out of the relational database when requested and decompose them when they have to be stored.'],\n",
       "   '2000']],\n",
       " [[['Behavior Consistent Inheritance in UML.',\n",
       "    'Object-oriented design methods express the behavior an object exhibits over time, i.e., the object life cycle, by notations based on Petri nets or state charts. The paper considers the specialization of life cycles via inheritance relationships as a combination of extension and refinement, viewed in the context of UML state machines. Extension corresponds to the addition of states and actions, refinement refers to the decomposition of states into substates. We use the notions of observation consistency and invocation consistency to compare the behavior of object life cycles and present a set of rules to check for behavior consistency of UML state machines, based on a one-to-one mapping of a meaningful subset of state machines to Object/Behavior Diagrams.'],\n",
       "   '2000']],\n",
       " [[['Constan Ratio Approximation Algorithms for the Rectangle Stabbing Problem and the Rectilinear Partitioning Problem.',\n",
       "    'We provide constant ratio approximation algorithms for two NP-hard problems, the rectangle stabbing problem and the rectilinear partitioning problem. In the rectangle stabbing problem, we are given a set of rectangles in two-dimensional space, with the objective of stabbing all rectangles with the minimum number of lines parallel to the x and y axes. We provide a 2-approximation algorithm, while the best known approximation ratio for this problem is O(log n). This algorithm is then extended to a 4-approximation algorithm for the rectilinear partitioning problem, which, given an mx × my array of nonnegative integers and positive integers υ, h, asks to find a set of υ vertical and h horizontal lines such that the maximum load of a subrectangle (i.e., the sum of the numbers in it) is minimized. The best known approximation ratio for this problem is 27. Our approximation ratio 4 is close to the best possible, as it is known to be NP-hard to approximate within any factor less than 2. The results are then extended to the d-dimensional space for d ≥ 2, where a d-approximation algorithm for the stabbing problem and a dd-approximation algorithm for the partitioning problem are developed.'],\n",
       "   '2000']],\n",
       " [[['First-Class Structures for Standard ML.',\n",
       "    'Standard ML is a statically typed programming language that is suited for the construction of both small and large programs. \"Programming in the small\" is captured by Standard ML\\'s Core language. \"Programming in the large\" is captured by Standard ML\\'s Modules language that provides constructs for organising related Core language definitions into self-contained modules with descriptive interfaces. While the Core is used to express details of algorithms and data structures, Modules is used to express the overall architecture of a software system. The Modules and Core languages are stratified in the sense that modules may not be manipulated as ordinary values of the Core. This is a limitation, since it means that the architecture of a program cannot be reconfigured according to run-time demands. We propose a novel and practical extension of the language that allows modules to be manipulated as first-class values of the Core language.'],\n",
       "   '2000']],\n",
       " [[['Information Retrieval on the Web.',\n",
       "    'Information Retrieval (IR) on the Web can be considered from many different perspectives, but one objective and relevant aspect to consider is that on mid-1999 the estimated number of pages being published and available for indexing in the Web was 800 million for 6 terabytes of textual data. Those Web pages were estimated to be distributed over 3 million Web servers. This means that anyone cannot effort to explore all the information distributed over those pages, but anyone necessarily needs to be supported by tools that help the end users to choose the most relevant Web pages to answer any specific request of information. The Web has started to operate only 10 years ago, and just a few years after the first information retrieval tools have been made available to help Web users to find Web pages with relevant information. To deal with the complexity and heterogeneity of the Web, we need search tools implementing algorithms for indexing and retrieval that are more advanced than those currently employed in IR. These advanced algorithms need to exploit the structure of, and the inter-relationships among Web pages. From a research point of view, we need also to re-think evaluation because of the different characteristics of Web IR, which can be expressed in terms of data, functionalities, architecture, and tools. These characteristics affect \"how\" to carry evaluation out and \"what\" to evaluate. This chapter faces the different aspects of IR on the Web that can be considered and analysed, that is: history of IR on the Web, different types of tools for performing IR on the Web which have been designed and developed to answer different user requirements, architecture and components of those IR Web tools, indexing and retrieval algorithms that can be employed for making Web IR effective, and methods for evaluation of Web IR.'],\n",
       "   '2000']],\n",
       " [[['Modeling Vagueness in Information Retrieval.',\n",
       "    'This paper reviews some applications of fuzzy set theory to model flexible information retrieval systems, i.e., systems that can represent and interpret the vagueness typical of human communication and reasoning. The paper focuses on the following topics: a description of fuzzy indexing procedures defined to represent structured documents, the definition of flexible query languages which allow the expression of vague selection conditions, and some fuzzy associative retrieval mechanisms based on fuzzy pseudo-thesauri of terms and fuzzy clustering techniques.'],\n",
       "   '2000']],\n",
       " [[['Information Retrieval and Structured Documents.',\n",
       "    'Standard Information Retrieval considers documents as atomic units of information that are indexed and retrieved as a whole. Modern evolution of document design and storage have since a long time introduced more elaborate representations of documents; standards such as SGML, then HTML and XML are of course major contributions in this domain. These standards underly today evolutions towards modern electronic documents. In this context, retrieving structured documents refers to index and retrieve information according to a given structure of documents. This means that documents are no longer considered as atomic entities, but as aggregates of interrelated objects that can be retrieved separately; given a retrieval query, one may retrieve the set of document components that are most relevant to this query. In this chapter we shall first emphasise some aspects which, in our opinion, relate explicit use of document structure to interactive retrieval performances, such as efficiency while browsing or querying information. In a second step we shall investigate two classes of implementation approaches dealing with indexing and retrieving structured documents: passage retrieval and explicit use of hierarchical structures of documents.'],\n",
       "   '2000']],\n",
       " [[['Logic and Uncertainty in Information Retrieval.',\n",
       "    'The use of logic in Information Retrieval (IR) enables one to formulate models that are more general than other well known IR models. Indeed, some logical models are able to represent, within a uniform framework, various features of IR systems, such as hypermedia links, mulimedia content, and user knowledge. Logic also provides a common approach to the integration of IR systems with logical database systems. Finally, logic makes it possible to reason about an IR model and its properties. This latter possibility is becoming increasingly important since conventional evaluation methods, although good indicators of the effectiveness of IR systems, often give results which cannot be predicted, or for that matter satisfactorily explained. However, logic by itself cannot fully model IR. In determining the relevance of a document to a query the truth value or the validity of a logical formula relating the two is not enough. It is necessary to take into account the uncertainty inherent in such a formulation. This paper gives an overview of how past and current research have combined the use of logical and uncertainty theories for the formulation of more advanced models for the representation and retrieval of information.'],\n",
       "   '2000']],\n",
       " [[['Models in Information Retrieval.',\n",
       "    \"Retrieval models form the theoretical basis for computing the answer to a query. They differ not only in the syntax and expressiveness of the query language, but also in the representation of the documents. Following Rijsbergen's approach of regarding IR as uncertain inference, we can distinguish models according to the expressiveness of the underlying logic and the way uncertainty is handled. Classical retrieval models are based on propositional logic. In the vector space model, documents and queries are represented as vectors in a vector space spanned by the index terms, and uncertainty is modelled by considering geometric similarity. Probabilistic models make assumptions about the distribution of terms in relevant and nonrelevant documents in order to estimate the probability of relevance of a document for a query. Language models compute the probability that the query is generated from a document. All these models can be interpreted within a framework that is based on a probabilistic concept space. For IR applications dealing not only with texts, but also with multimedia or factual data, propositional logic is not sufficient. Therefore, advanced IR models use restricted forms of predicate logic as basis. Terminological/description logics are rooted in semantic networks and terminological languages like e.g. KL-ONE. Datalog uses function-free horn clauses. Probabilistic versions of both approaches are able to cope with the intrinsic uncertainty of IR.\"],\n",
       "   '2000']],\n",
       " [[['Users in Context.',\n",
       "    \"Users as actors in interactive information retrieval (IIR) are seen in the contexts of their perceived work tasks and information seeking behaviour. The paper models IIR processes by demonstrating a variety of approaches, ranging from Ingwersen's cognitive communication model for IR interaction, over Saraceveic' stratified model which includes a typology of relevance conceptions, to Borlund's model of work task perception, information need development and relevance assessments. Other associated models and perspectives of IIR are discussed when appropriate to the major focus points of the contribution: information need development and typology; understanding of relevance in IIR; and experimental problems in IIR.\"],\n",
       "   '2000']],\n",
       " [[['Multilingual Information Access.',\n",
       "    'The global information society has radically changed the way in which knowledge is acquired, disseminated and exchanged. Users of internationally distributed networks need to be able to find, retrieve and understand relevant information in whatever language and form it may have been stored. For this reason, much attention has been given over the past few years to the study and development of tools and technologies for multilingual information access (MLIA). This is a complex, multidisciplinary area in which methodologies and tools developed in the fields of information retrieval and natural language processing converge. Two main sectors are involved: multiple language recognition, manipulation and display; cross-language search and retrieval. The paper provides an overview of the main issues of interest in both these areas. Topics covered include: multilingual document indexing, specific requirements of particular languages and scripts, techniques for cross-language information retrieval (CLIR), resources, and system and component evaluation.'],\n",
       "   '2000']],\n",
       " [[['Design issues of iDICT: a gaze-assisted translation aid.',\n",
       "    \"Eye-aware applications have existed for long, but mostly for very special and restricted target populations. We have designed and are currently implementing an eye-aware application, called iDict, which is a general-purpose translation aid aimed at mass markets. iDict monitors the user's gaze path while s/he is reading text written in a foreign language. When the reader encounters difficulties, iDict steps in and provides assistance with the translation. To accomplish this, the system makes use of information obtained from reading research, a language model, and the user profile. This paper describes the idea of the iDict application, the design problems and the key solutions for resolving these problems.\"],\n",
       "   '2000']],\n",
       " [[['Efficient Proofs that a Committed Number Lies in an Interval.',\n",
       "    'Alice wants to prove that she is young enough to borrow money from her bank, without revealing her age. She therefore needs a tool for proving that a committed number lies in a specific interval. Up to now, such tools were either inefficient (too many bits to compute and to transmit) or inexact (i.e. proved membership to a much larger interval). This paper presents a new proof, which is both efficient and exact. Here, \"efficient\" means that there are less than 20 exponentiations to perform and less than 2 Kbytes to transmit. The potential areas of application of this proof are numerous (electronic cash, group signatures, publicly verifiable secret encryption, etc ...).'],\n",
       "   '2000']],\n",
       " [[['Noisy Polynomial Interpolation and Noisy Chinese Remaindering.',\n",
       "    \"The noisy polynomial interpolation problem is a new intractability assumption introduced last year in oblivious polynomial evaluation. It also appeared independently in password identification schemes, due to its connection with secret sharing schemes based on Lagrange's polynomial interpolation. This paper presents new algorithms to solve the noisy polynomial interpolation problem. In particular, we prove a reduction from noisy polynomial interpolation to the lattice shortest vector problem, when the parameters satisfy a certain condition that we make explicit. Standard lattice reduction techniques appear to solve many instances of the problem. It follows that noisy polynomial interpolation is much easier than expected. We therefore suggest simple modifications to several cryptographic schemes recently proposed, in order to change the intractability assumption. We also discuss analogous methods for the related noisy Chinese remaindering problem arising from the well-known analogy between polynomials and integers.\"],\n",
       "   '2000']],\n",
       " [[['Authenticated Key Exchange Secure against Dictionary Attacks.',\n",
       "    'Password-based protocols for authenticated key exchange (AKE) are designed to work despite the use of passwords drawn from a space so small that an adversary might well enumerate, off line, all possible passwords. While several such protocols have been suggested, the underlying theory has been lagging. We begin by defining a model for this problem, one rich enough to deal with password guessing, forward secrecy, server compromise, and loss of session keys. The one model can be used to define various goals. We take AKE (with \"implicit\" authentication) as the \"basic\" goal, and we give definitions for it, and for entity-authentication goals as well. Then we prove correctness for the idea at the center of the Encrypted Key-Exchange (EKE) protocol of Bellovin and Merritt: we prove security, in an ideal-cipher model, of the two-flow protocol at the core of EKE.'],\n",
       "   '2000']],\n",
       " [[['Provably Secure Password-Authenticated Key Exchange Using Diffie-Hellman.',\n",
       "    'When designing password-authenticated key exchange protocols (as opposed to key exchange protocols authenticated using cryptographically secure keys), one must not allow any information to be leaked that would allow verification of the password (a weak shared key), since an attacker who obtains this information may be able to run an off-line dictionary attack to determine the correct password. We present a new protocol called PAK which is the first Diffie-Hellman-based password-authenticated key exchange protocol to provide a formal proof of security (in the random oracle model) against both passive and active adversaries. In addition to the PAK protocol that provides mutual explicit authentication, we also show a more efficient protocol called PPK that is provably secure in the implicit -authentication model. We then extend PAK to a protocol called PAK-X, in which one side (the client) stores a plaintext version of the password, while the other side (the server) only stores a verifier for the password. We formally prove security of PAK-X, even when the server is compromised. Our formal model for password-authenticated key exchange is new, and may be of independent interest.'],\n",
       "   '2000']],\n",
       " [[['Exposure-Resilient Functions and All-or-Nothing Transforms.',\n",
       "    'We study the problem of partial key exposure. Standard cryptographic definitions and constructions do not guarantee any security even if a tiny fraction of the secret key is compromised. We show how to build cryptographic primitives that remain secure even when an adversary is able to learn almost all of the secret key. The key to our approach is a new primitive of independent interest, which we call an Exposure-Resilient Function (ERF) - a deterministic function whose output appears random (in a perfect, statistical or computational sense) even if almost all the bits of the input are known. ERF\\'s by themselves efficiently solve the partial key exposure problem in the setting where the secret is simply a random value, like in private-key cryptography. They can also be viewed as very secure pseudorandom generators, and have many other applications. To solve the general partial key exposure problem, we use the (generalized) notion of an All-Or-Nothing Transform (AONT), an invertible (randomized) transformation T which, nevertheless, reveals \"no information\" about x even if almost all the bits of T(x) are known. By applying an AONT to the secret key of any cryptographic system, we obtain security against partial key exposure. To date, the only known security analyses of AONT candidates were made in the random oracle model. We show how to construct ERF\\'s and AONT\\'s with nearly optimal parameters. Our computational constructions are based on any one-way function. We also provide several applications and additional properties concerning these notions.'],\n",
       "   '2000']],\n",
       " [[['General Secure Multi-party Computation from any Linear Secret-Sharing Scheme.',\n",
       "    'We show that verifiable secret sharing (VSS) and secure multi-party computation (MPC) among a set of n players can efficiently be based on any linear secret sharing scheme (LSSS) for the players, provided that the access structure of the LSSS allows MPC or VSS at all. Because an LSSS neither guarantees reconstructability when some shares are false, nor verifiability of a shared value, nor allows for the multiplication of shared values, an LSSS is an apparently much weaker primitive than VSS or MPC. Our approach to secure MPC is generic and applies to both the information-theoretic and the cryptographic setting. The construction is based on 1) a formalization of the special multiplicative property of an LSSS that is needed to perform a multiplication on shared values, 2) an efficient generic construction to obtain from any LSSS a multiplicative LSSS for the same access structure, and 3) an efficient generic construction to build verifiability into every LSSS (always assuming that the adversary structure allows for MPC or VSS at all). The protocols are efficient. In contrast to all previous information-theoretically secure protocols, the field size is not restricted (e.g, to be greater than n). Moreover, we exhibit adversary structures for which our protocols are polynomial in n while all previous approaches to MPC for non-threshold adversaries provably have super-polynomial complexity.'],\n",
       "   '2000']],\n",
       " [[['Propagation Characteristics and Correlation-Immunity of Highly Nonlinear Boolean Functions.',\n",
       "    'We investigate the link between the nonlinearity of a Boolean function and its propagation characteristics. We prove that highly nonlinear functions usually have good propagation properties regarding different criteria. Conversely, any Boolean function satisfying the propagation criterion with respect to a linear subspace of codimension 1 or 2 has a high nonlinearity. We also point out that most highly nonlinear functions with a three-valued Walsh spectrum can be transformed into 1-resilient functions.'],\n",
       "   '2000']],\n",
       " [[['Efficient Concurrent Zero-Knowledge in the Auxiliary String Model.',\n",
       "    'We show that if any one-way function exists, then 3-round concurrent zero-knowledge arguments for all NP problems can be built in a model where a short auxiliary string with a prescribed distribution is available to the players. We also show that a wide range of known efficient proofs of knowledge using specialized assumptions can be modified to work in this model with no essential loss of efficiency. We argue that the assumptions of the model will be satisfied in many practical scenarios where public key cryptography is used, in particular our construction works given any secure public key infrastructure. Finally, we point out that in a model with preprocessing (and no auxiliary string) proposed earlier, concurrent zero-knowledge for NP can be based on any one-way function.'],\n",
       "   '2000']],\n",
       " [[['How to Break a Practical MIX and Design a New One.',\n",
       "    \"A MIX net takes a list of ciphertexts (c1; ... ; cN) and outputs a permuted list of the plaintexts (m1; ... ;mN) without revealing the relationship between (c1; ... ; cN) and (m1; ... ;mN). This paper first shows that the Jakobsson's MIX net of Eurocrypt'98, which was believed to be resilient and very efficient, is broken. We next propose an efficient t-resilient MIX net with O(t2) servers in which the cost of each MIX server is O(N). Two new concepts are introduced, existential-honesty and limited-open-verification. They will be useful for distributed computation in general.\"],\n",
       "   '2000']],\n",
       " [[['Using Hash Functions as a Hedge against Chosen Ciphertext Attack.',\n",
       "    'The cryptosystem recently proposed by Cramer and Shoup [CS98] is a practical public key cryptosystem that is secure against adaptive chosen ciphertext attack provided the Decisional Diffie-Hellman assumption is true. Although this is a reasonable intractability assumption, it would be preferable to base a security proof on a weaker assumption, such as the Computational Diffie-Hellman assumption. Indeed, this cryptosystem in its most basic form is in fact insecure if the Decisional Diffie-Hellman assumption is false. In this paper we present a practical hybrid scheme that is just as efficient as the scheme of of Cramer and Shoup; indeed, the scheme is slightly more efficient than the one originally presented by Cramer and Shoup; we prove that the scheme is secure if the Decisional Diffie-Hellman assumption is true; we give strong evidence that the scheme is secure if the weaker, Computational Diffie-Hellman assumption is true by providing a proof of security in the random oracle model.'],\n",
       "   '2000']],\n",
       " [[['Spectral Compression of Mesh Geometry.',\n",
       "    'We show how spectral methods may be applied to 3D mesh data to obtain compact representations. This is achieved by projecting the mesh geometry onto an orthonormal basis derived from the mesh topology. To reduce complexity, the mesh is partitioned into a number of balanced submeshes with minimal interaction, each of which are compressed independently. Our methods may be used for compression and progressive transmission of 3D content, and are shown to be vastly superior to existing methods using spatial techniques, if slight loss can be tolerated.'],\n",
       "   '2000']],\n",
       " [[['Feature Specification and Automatic Conflict Detection.',\n",
       "    'Large software systems, especially in the telecommunications field, are often specified as a collection of features. We present a formal specification language for describing features, and a method of automatically detecting conflicts (\"undesirable interactions\") amongst features at the specification stage. Conflict detection at this early stage can help prevent costly and time consuming problem fixes during implementation. Features are specified using temporal logic; two features conflict essentially if their specifications are mutually inconsistent under axioms about the underlying system behavior. We show how this inconsistency check may be performed automatically with existing model checking tools. In addition, the model checking tools can be used to provide witness scenarios, both when two features conflict as well as when the features are mutually consistent. Both types of witnesses are useful for refining the specifications. We have implemented a conflict detection tool, FIX (Feature Interaction eXtractor), which uses the model checker COSPAN for the inconsistency check. We describe our experience in applying this tool to a collection of telecommunications feature specifications obtained from the Telcordia (Bellcore) standards. Using FIX, we were able to detect most known interactions and some new ones, fully automatically, in a few hours processing time.'],\n",
       "   '2000']],\n",
       " [[['An Algorithm for Strongly Connected Component Analysis in log Symbolic Steps.',\n",
       "    \"We present a symbolic algorithm for strongly connected component decomposition. The algorithm performs ¿(n log n) image and preimage computations in the worst case, where n is the number of nodes in the graph. This is an improvement over the previously known quadratic bound. The algorithm can be used to decide emptiness of Büchi automata with the same complexity bound, improving Emerson and Lei's quadratic bound, and emptiness of Streett automata, with a similar bound in terms of nodes. It also leads to an improved procedure for the generation of nonemptiness witnesses.\"],\n",
       "   '2000']],\n",
       " [[['Using TAME to prove invariants of automata models: Two case studies.',\n",
       "    \"TAME is a special-purpose interface to PVS designed to support developers of software systems in proving properties of automata models. One of TAME's major goals is to allow a software developer who has basic knowledge of standard logic, and can do hand proofs, to use PVS to represent and to prove properties about an automaton model without first becoming a PVS expert. A second goal is for a human to be able to read and understand the content of saved TAME proofs without running them through the PVS proof checker. A third goal is to make proving properties of automata with TAME less costly in human time than proving such properties using PVS directly. Recent work by Romijn and Devillers et al., based on the I/O automata model, has provided the basis for two case studies on how well TAME achieves these goals. Romijn specified the RPC-Memory Problem and its solution, while Devillers et al. specified a tree identify protocol. Hand proofs of specification properties were provided by the authors. In addition, Devillers et al. used PVS directly to mechanize the specifications and proofs of the tree identify protocol. In one case study, the third author, a new TAME user with no previous PVS experience, used TAME to create PVS specifications of the I/O automata presented by Romijn and Devillers et al. and to check the hand proofs of invariant properties. The PVS specifications and proofs of Devillers et al. \\\\hspace*{-.03in} provide the basis for the other case study, which compares the TAME approach to an alternate approach which uses PVS directly.\"],\n",
       "   '2000']],\n",
       " [[['Fault origin adjudication.',\n",
       "    'When a program P fails to satisfy a requirement R supposedly ensured by a detailed specification S that was used to implement P, there is a question about whether the problem arises in S or in P. We call this determination fault origin adjudication and illustrate its significance in various software engineering contexts. The primary contribution of this paper is a framework for formal fault origin adjudication for network protocols using the NS simulator and the SPIN model checker. We describe our architecture and illustrate its use in a case study involving a standard specification for packet radio routing.'],\n",
       "   '2000']],\n",
       " [[['Specification, validation, and synthesis of email agent controllers: A case study in function rich reactive system design.',\n",
       "    'With a few exceptions, previous formal methods for reactive system design have focused on finite state machines represented in terms of boolean states and boolean next-state functions. By contrast, in many reactive system domains requirements engineers and developers think in terms of complex data types and expressive next-state functions. Formal methods for reactive system design must be extended to meet their needs as well. I term a reactive system function rich if expressing its state, next-state function, or output function naturally requires this higher expressive power. ISAT, a prototype formal-methods based tool environment, is intended to assist in the creation of function rich reactive systems. This paper describes a case study I have carried out using ISAT to design, validate, synthesize, and evolve controllers for the email agent components making up a novel spam-free email system that I deployed in a user trial in July 1999. The trial has been running since, with high availability, through several evolutionary specification changes and resulting software releases. In addition to summarizing ISAT and the trial, this paper discusses tool requirements placed by the domain and task, the simple and powerful platform/controller/pure-functions software architecture of the components, as well as lessons learned from the study.'],\n",
       "   '2000']],\n",
       " [[['Using predicate abstraction to reduce object-oriented programs for model checking.',\n",
       "    'While it is becoming more common to see model checking applied to software requirements specifications, it is seldom applied to software implementations. The Automated Software Engineering group at NASA Ames is currently investigating the use of model checking for actual source code, with the eventual goal of allowing software developers to augment traditional testing with model checking. Because model checking suffers from the state-explosion problem, one of the main hurdles for program model checking is reducing the size of the program. In this paper we investigate the use of abstraction techniques to reduce the state-space of a real-time operating system kernel written in C++. We show how informal abstraction arguments could be formalized and improved upon within the framework of predicate abstraction, a technique based on abstract interpretation. We introduce some extensions to predicate abstraction that all allow it to be used within the class-instance framework of object-oriented languages. We then demonstrate how these extensions were integrated into an abstraction tool that performs automated predicate abstraction of Java programs.'],\n",
       "   '2000']],\n",
       " [[['Fast Broadcasting and Gossiping in Radio Networks.',\n",
       "    'We establish an O(nlog2n) upper bound on the time for deterministic distributed broadcasting in multi-hop radio networks with unknown topology. This nearly matches the known lower bound of Ω(n log n). The fastest previously known algorithm for this problem works in time O(n3/2). Using our broadcasting algorithm, we develop an O(n3/2log2n) algorithm for gossiping in the same network model.'],\n",
       "   '2000']],\n",
       " [[['Existential Second-Order Logic over Graphs: Charting the Tractability Frontier.',\n",
       "    \"Fagin's theorem, the first important result of descriptive complexity, asserts that a property of graphs is in NP if and only if it is definable by an existential second-order formula. In this article, we study the complexity of evaluating existential second-order formulas that belong to prefix classses of existential second-order logic, where a prefix class is the collection of all existential second-order formulas in prenex normal form such that the second-order and the first-order quantifiers obey a certain quantifier pattern. We completely characterize the computational complexity of prefix classes of existential second-order logic in three different contexts: (1) over directed graphs, (2) over undirected graphs with self-loops and (3) over undirected graphs without self-loops. Our main result is that in each of these three contexts a dichotomy holds, that is to say, each prefix class of existential second-order logic either contains sentences that can express NP-complete problems, or each of its sentences expresses a polynomial-time solvable problem. Although the boundary of the dichotomy coincides for the first two cases, it changes, as one moves to undirected graphs without self-loops. The key difference is that a certain prefix class, based on the well-known Ackermann class of first-order logic, contains sentences that can express NP-complete problems over graphs of the first two types, but becomes tractable over undirected graphs without self-loops. Moreover, establishing the dichotomy over undirected graphs without self-loops turns out to be a technically challenging problem that requires the use of sophisticated machinery from graph theory and combinatorics, including results about graphs of bounded tree-width and Ramsey's theorem.\"],\n",
       "   '2000']],\n",
       " [[['Stable Distributions, Pseudorandom Generators, Embeddings and Data Stream Computation.',\n",
       "    'In this article, we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular:---We show that, for any p &isin; (0, 2], one can maintain (using only O(log n/&epsi;2) words of storage) a sketch C(q) of a point q &isin; lnp under dynamic updates of its coordinates. The sketch has the property that, given C(q) and C(s), one can estimate &Verbar;q &minus; s&Verbar;p up to a factor of (1 &plus; &epsi;) with large probability. This solves the main open problem of Feigenbaum et al. [1999].---We show that the aforementioned sketching approach directly translates into an approximate algorithm that, for a fixed linear mapping A, and given x &isin; &real;n and y &isin; &real;m, estimates &Verbar;Ax &minus; y&Verbar;p in O(n &plus; m) time, for any p &isin; (0, 2]. This generalizes an earlier algorithm of Wasserman and Blum [1997] which worked for the case p &equals; 2.---We obtain another sketch function C&prime; which probabilistically embeds ln1 into a normed space lm1. The embedding guarantees that, if we set m &equals; log(1/&delta;)O(1/&epsi;), then for any pair of points q, s &isin; ln1, the distance between q and s does not increase by more than (1 &plus; &epsi;) with constant probability, and it does not decrease by more than (1 &minus; &epsi;) with probability 1 &minus; &delta;. This is the only known dimensionality reduction theorem for the l1 norm. In fact, stronger theorems of this type (i.e., that guarantee very low probability of expansion as well as of contraction) cannot exist [Brinkman and Charikar 2003].---We give an explicit embedding of ln2 into lnO(log n)1 with distortion (1 &plus; 1/n&Theta;(1)).'],\n",
       "   '2000']],\n",
       " [[['On Clusterings - Good, Bad and Spectral.',\n",
       "    'We motivate and develop a natural bicriteria measure for assessing the quality of a clustering that avoids the drawbacks of existing measures. A simple recursive heuristic is shown to have poly-logarithmic worst-case guarantees under the new measure. The main result of the article is the analysis of a popular spectral algorithm. One variant of spectral clustering turns out to have effective worst-case guarantees; another finds a \"good\" clustering, if one exists.'],\n",
       "   '2000']],\n",
       " [[['Hierarchical Graph Transformation.',\n",
       "    'When graph transformation is used for programming purposes, large graphs should be structured in order to be comprehensible. In this paper, we present an approach for the rule-based transformation of hierarchically structured hypergraphs. In these graphs, distinguished hyperedges contain graphs that can be hierarchical again. Our framework extends the well-known double-pushout approach from flat to hierarchical graphs. In particular, we show how pushouts and pushout complements of hierarchical graphs and graph morphisms can be constructed recursively. Moreover, we make rules more expressive by introducing variables which allow us to copy and remove hierarchical subgraphs in a single rule application.'],\n",
       "   '2000']],\n",
       " [[['Tolerating operational faults in cluster-based FPGAs.',\n",
       "    'In recent years the application space of reconfigurable devices has grown to include many platforms with a strong need for fault tolerance. While these systems frequently contain hardware redundancy to allow for continued operation in the presence of operational faults, the need to recover faulty hardware and return it to full functionality quickly and efficiently is great. In addition to providing functional density, FPGAs provide a level of fault tolerance generally not found in mask-programmable devices by including the capability to reconfigure around operational faults in the field. In this paper, incremental CAD techniques are described that allow functional recovery of FPGA design configurations in the presence of single or multiple operational faults. Our preferred approach to fault recovery takes advantage of device routing hierarchy in architectural families such as Xilinx Virtex [2] and Altera Apex [3] to quickly swap unused logic and routing resources in place of faulty ones within logic clusters. These algorithms allow for straight-forward implementation within a local fault-tolerant system without the need to access a remote processing location. If initial recovery attempts through localized swapping fail, an incremental router based on the widely-used PathFinder maze routing algorithm [10] can be applied remotely in an attempt to form connections between newly-allocated logic and interconnect based on the history of the initial design route.'],\n",
       "   '2000']],\n",
       " [[['Approximate Swapped Matching.',\n",
       "    \"Let a text string T of n symbols and a pattern string P of m symbols from alphabet Σ be given. A swapped version P' of P is a length m string derived from P by a series of local swaps (i.e., p'l ← pl+1 and p'l+1 ← pl), where each element can participate in no more than one swap. The Pattern Matching with Swaps problem is that of finding all locations i of T for which there exists a swapped version P' of P with an exact matching of P' in location i of T.Recently, some efficient algorithms were developed for this problem. Their time complexity is better than the best known algorithms for pattern matching with mismatches. However, the Approximate Pattern Matching with Swaps problem was not known to be solved faster than the Pattern Matching with Mismatches problem.In the Approximate Pattern Matching with Swaps problem the output is, for every text location i where there is a swapped match of P, the number of swaps necessary to create the swapped version that matches location i. The fastest known method to-date is that of counting mismatches and dividing by two. The time complexity of this method is O(n√m log m) for a general alphabet Σ.In this paper we show an algorithm that counts the number of swaps at every location where there is a swapped matching in time O(n log m log σ), where σ = min(m, |Σ|). Consequently, the total time for solving the approximate pattern matching with swaps problem is O(f(n, m) + n log m log σ), where f(n, m). is the time necessary for solving the Pattern Matching with Swaps problem. Since f(n, m) was shown to be O(n log m log σ) this means our algorithm's running time is O(n log m log σ).\"],\n",
       "   '2000']],\n",
       " [[['Text Sparsification via Local Maxima.',\n",
       "    'In this paper we investigate some properties and algorithms related to a text sparsification technique based on the identification of local maxima in the given string. As the number of local maxima depends on the order assigned to the alphabet symbols, we first consider the case in which the order can be chosen in an arbitrary way. We show that looking for an order that minimizes the number of local maxima in the given text string is an NP-hard problem. Then, we consider the case in which the order is fixed a priori. Even though the order is not necessarily optimal, we can exploit the property that the average number of local maxima induced by the order in an arbitrary text is approximately one third of the text length. In particular, we describe how to iterate the process of selecting the local maxima by one or more iterations, so as to obtain a sparsified text. We show how to use this technique to filter the access to unstructured texts, which appear to have no natural division in words. Finally, we experimentally show that our approach can be successfully used in order to create a space efficient index for searching sufficiently long patterns in a DNA sequence as quickly as a full index.'],\n",
       "   '2000']],\n",
       " [[['A Multi-dimensional Approach to Force-Directed Layouts of Large Graphs.',\n",
       "    'We present a novel hierarchical force-directed method for drawing large graphs. Given a graph <i>G=(V.E)</i>, the algorithm produces an embedding for <i>G</i> in an Euclidean space E of any dimension. A two or three dimensional drawing of the graph is then obtained by projecting a higher-dimensional embedding into a two or three dimensional subspace of E. Such projections typically result in drawings that are \"smoother\" and more symmetric than direct drawings in 2D and 3D. In order to obtain fast placement of the vertices of the graph our algorithm employs a multi-scale technique based on a maximal independent set filtration of vertices of the graph. While most existing force-directed algorithms begin with an initial random placement of all the vertices, our algorithm attempts to place vertices \"intelligently\", close to their final positions. Other notable features of our approach include a fast energy function minimization strategy and efficient memory management. Our implementation of the algorithm can draw graphs with tens of thousands of vertices using a negligible amount of memory in less than one minute on a 550 MHz Pentium PC.'],\n",
       "   '2000']],\n",
       " [[['Cardinal relations between regions with a broad boundary.',\n",
       "    'Despite the innovative relevance of the results about spatial relations today available, the expressiveness of spatial query languages needs to be pushed further significantly in order to cope with the complexity of spatial entities. In particular, more research is necessary in order to support queries against data with uncertainty. In connection with cardinal directions, the best method today available is the Boolean, 3 &times; 3 direction-relation matrix proposed very recently by Goyal and Egenhofer. Our paper extends such a method to the case of regions with a broad boundary by introducing a 4-value, 5 &times; 5 direction-relation matrix. Furthermore, the present contribution studies the notion of consistency for a direction-relation matrix and proposes a set of conditions which are necessary and sufficient for assessing consistency.'],\n",
       "   '2000']],\n",
       " [[['Clone join and shadow join: two parallel spatial join algorithms.',\n",
       "    'Spatial applications frequently need to join two data sets based on some spatial relationship between objects in the two data sets. This operation, called a spatial join, is an expensive operation and in the past many algorithms have been proposed for evaluating the spatial join operation on a single processor system. However, the use of parallelism for handling queries involving large volumes of spatial data has received little attention. In this paper, we explore the use of parallelism for evaluating the spatial join operation. We first propose two strategies for storing spatial data in a parallel database system. We propose a number of spatial join algorithms based on these declustering strategies. Two algorithms are identified as the key algorithms in this design space. We analyze these two algorithms both analytically and experimentally. The experimental evaluation uses real data sets and is based on an actual implementation in a parallel database system. The experiments show that both algorithms can effectively exploit parallelism.'],\n",
       "   '2000']],\n",
       " [[['Processing object-oientation-based direction queries: a summary of results.',\n",
       "    'Direction based spatial relationships are critical in many domains including geographic information systems(GIS) and image interpretation. They are also frequently used as selection conditions in spatial queries. In this paper, we explore the processing of queries based on object-orientation-based directional relationships. A new open shape based strategy(OSS) is proposed. OSS converts the processing of the direction predicates to the processing of topological operations between open shapes and closed geometry objects. Since OSS models the direction region as an OpenShape, it does not need to know the boundary of the embedding world, and also eliminating the computation related to the world boundary. The experimental evaluations show that OSS consistently outperforms classical range query strategy both in I/O and CPU cost. This paper is a summary of the results.'],\n",
       "   '2000']],\n",
       " [[['Query operations for moving objects database systems.',\n",
       "    \"Geographical Information Systems were originally intended to deal with snapshots representing a single state of some reality but there are more and more applications requiring the representation and querying of time-varying information. This work addresses the representation of moving objects on GIS. The continuous nature of movement raises problems for representation in information systems due to the limited capacity of storage systems and the inherently discrete nature of measurement instruments. The stored information has therefore to be partial and does not allow an exact inference of the real-world object's behavior. To cope with this, query operations must take uncertainty into consideration in their semantics in order to give accurate answers to the users. The paper proposes a set of operations to be included in a GIS or a spatial database to make it able to answer queries on the spatio-temporal behavior of moving objects. The operations have been selected according to the requirements of real applications and their semantics with respect to uncertainty is specified. A collection of examples from a case study is included to illustrate the expressiveness of the proposed operations.\"],\n",
       "   '2000']],\n",
       " [[['On Verifying Distributed Multithreaded Java Programs.',\n",
       "    'Distributed multithreaded software systems are becoming more and more important in modern networked environment. For these systems, concurrency control and thread synchronization make it much harder to do traditional extensive testing to guarantee the quality of the systems. In contrast to testing, software verification under certain formalisms and methodologies usually gives us higher confidence about the system. In this paper, we consider translating some parts of program code that are sensitive to concurrency control into certain formal description so that we can reuse existing verification tools to enhance our confidence in the final code.Java language is gaining increasing popularity in distributed multithreaded system development, and CCS is one of the convenient tools for describing concurrent and multi-process systems. Under a set of reasonable restrictions, we present a general framework on how to translate the thread control and synchronization portion of distributed, multithreaded Java programs into formal specification in CCS. With the translated process terms, we are able to use some model checkers to verify properties expressed in modal mu-calculus, such as invariance, eventualities, fairness etc, which are by nature hard to test.'],\n",
       "   '2000']],\n",
       " [[['Mutation Testing Applied to Estelle Specifications.',\n",
       "    'Many researchers have pursued the establishment of a low-cost, effective testing and validation strategy at the program level as well as at the specification level. Mutation Testing is an error-based approach, originally introduced for program testing, that provides testers a systematic way to evaluate how good a given test set is. Some studies have also investigated its use to generate test sets. In this article, the application of Mutation Testing for validating Estelle specifications is proposed. A mutant operator set for Estelle&mdash;one of the crucial points for effectively applying Mutation Testing&mdash;is defined, addressing: the validation of the behavior of the modules, the communication among modules and the architecture of the specification. In this scope, these operators can be taken as a fault model. Considering this context, a strategy for validating Estelle-based specification is proposed and exemplified using the Alternating-bit protocol.'],\n",
       "   '2000']],\n",
       " [[['On the Improvement of Anthropometry and Pose Estimation from a Single Uncalibrated Image.',\n",
       "    'Recently, we developed a technique that allows semi-automatic estimation of anthropometry and pose from a single image. However, estimation was limited to a class of images for which an adequate number of human body segments were almost parallel to the image plane. In this paper, we present a generalization of that estimation algorithm that exploits pairwise geometric relationships of body segments to allow estimation from a broader class of images. In addition, we refine our search space by constructing a fully populated discrete hyper-ellipsoid of stick human body models in order to capture the variance of the statistical anthropometric information. As a result, a better initial estimate can be computed by our algorithm and thus the number of iterations needed during minimization are reduced tenfold. We present our results over a variety of images to demonstrate the broad coverage of our algorithm.'],\n",
       "   '2000']],\n",
       " [[['Conversion of Coloring Algorithms into Maximum Weight Independent Set Algorithms.',\n",
       "    'A general technique for converting approximation algorithms for the vertex coloring problem in a class of graphs into approximation algorithms for the maximum weight independent set problem (MWIS) in the same class of graphs is presented. The technique consists of solving an LP-relaxation of the MWIS problem with certain clique inequalities, constructing an instance of the vertex coloring problem from the LP solution, applying the coloring algorithm to this instance, and selecting the best resulting color class as the MWIS solution. The approximation ratio obtained is the product of the approximation ratio with which the LP formulation can be solved (usually equal to one) and the approximation ratio of the coloring algorithm with respect to the size of the largest relevant clique. Applying this technique, the best known approximation algorithms are obtained for the maximum weight edge-disjoint paths problem in bidirected trees and in bidirected two-dimensional meshes with row-column routing, and for time-constrained scheduling of weighted packets in the same topologies. These problems are also proved to be MAX SNP-hard.'],\n",
       "   '2000']],\n",
       " [[['Information Flow vs. Resource Access in the Asynchronous Pi-Calculus.',\n",
       "    'We propose an extension of the asynchronous &pi;-calculus in which a variety of security properties may be captured using types. These are an extension of the input/output types for the &pi;-calculus in which I/O capabilities are assigned specific security levels. The main innovation is a uniform typing system that, by varying slightly the allowed set of types, captures different notions of security.We first define a typing system that ensures that processes running at security level &sigma; cannot access resources with a security level higher than &sigma;. The notion of access control guaranteed by this system is formalized in terms of a Type Safety Theorem.We then show that, by restricting the allowed types, our system prohibits implicit information flow from high-level to low-level processes. We prove that low-level behavior can not be influenced by changes to high-level behavior. This is formalized as a noninterference theorem with respect to may testing.'],\n",
       "   '2000']],\n",
       " [[['Polynominal Time Approximation Schemes for General Multiprocessor Job Shop Scheduling.',\n",
       "    'We study preemptive and non-preemptive versions of the general multiprocessor job shop scheduling problem: Given a set of n tasks each consisting of at most µ ordered operations that can be processed on different (possibly all) subsets of m machines with different processing times, compute a schedule (preemptive or non-preemptive, depending on the model) with minimum makespan where operations belonging to the same task have to be scheduled according to the specified order. We propose algorithms for both preemptive and non-preemptive variants of this problem that compute approximate solutions of any positive ε accuracy and run in O(n) time for any fixed values of m, µ, and ε. These results include (as special cases) many recent developments on polynomial time approximation schemes for scheduling jobs on unrelated machines, multiprocessor tasks, and classical open, flow and job shops.'],\n",
       "   '2000']],\n",
       " [[['Decidable First-Order Transition Logics for PA-Processes.',\n",
       "    'We show the decidability of model checking PA-processes against several first-order logics based upon the reachability predicate. The main tool for this result is the recognizability by tree automata of the reachability relation. The tree automata approach and the transition logics we use allow a smooth and general treatment of parameterized model checking for PA. This approach is extended to handle a quite general notion of costs of PA steps. In particular, when costs are Parikh images of traces, we show decidability of a transition logic extended by some form of first-order reasoning over costs.'],\n",
       "   '2000']],\n",
       " [[['An Optimal Minimum Spanning Tree Algorithm.',\n",
       "    \"We establish that the algorithmic complexity of the minimum spanning tree problem is equal to its decision-tree complexity. Specifically, we present a deterministic algorithm to find a minimum spanning tree of a graph with n vertices and m edges that runs in time O(T*(m,n)) where T* is the minimum number of edge-weight comparisons needed to determine the solution. The algorithm is quite simple and can be implemented on a pointer machine.Although our time bound is optimal, the exact function describing it is not known at present. The current best bounds known for T* are T*(m,n) = &Omega;(m) and T*(m,n) = O(m ∙ &alpha;(m,n)), where &alpha; is a certain natural inverse of Ackermann's function.Even under the assumption that T* is superlinear, we show that if the input graph is selected from Gn,m, our algorithm runs in linear time with high probability, regardless of n, m, or the permutation of edge weights. The analysis uses a new martingale for Gn,m similar to the edge-exposure martingale for Gn,p.\"],\n",
       "   '2000']],\n",
       " [[['Multilevel Optimization for Large-Scale Circuit Placement.',\n",
       "    'We have designed and implemented a new class of fast and highly scalable placement algorithms that directly handle complex constraints and achieve total wirelengths comparable to the state of the art. Our approach exploits recent advances in (i) multilevel methods for hierarchical computation, (ii) interior-point methods for nonconvex nonlinear programming, and (iii) the Fast Multipole Method for the order N evaluation of sums over the N (N - 1)/2 pairwise interactions of N components. Significant adaptation of these methods for the placement problem is required, and we have therefore developed a set of customized discrete algorithms for clustering, declustering, slot assignment, and local refinement with which the continuous algorithms are naturally combined. Preliminary test runs on benchmark circuits with up to 184,000 cells produce total wirelengths within approximately 5-10% of those of GORDIAN-L [1] in less than one tenth the run time. Such an ultra-fast placement engine is badly needed for timing convergence of the synthesis and layout phases of integrated circuit design.'],\n",
       "   '2000']],\n",
       " [[['Miller Factor for Gate-Level Coupling Delay Calculation.',\n",
       "    'In coupling delay computation, a Miller factor of more than 2X may be necessary to account for active coupling capacitance when modeling the delay of deep submicron circuitry in the presence of active coupling capacitance. We propose an efficient method to estimate this factor such that the delay response of a decoupling circuit model can emulate the original coupling circuit. Under the assumptions of zero initial voltage, equal charge transfer, and 0.5VDD as the switching threshold voltage, an upper bound of 3X for maximum delay and a lower bound of -1X for minimum delay can be proven. Efficient Newton-Raphson iteration is also proposed as a technique for computing the Miller factor or effective capacitance. This result is highly applicable to crosstalk coupling delay calculation in deep submicron gate-level static timing analysis. Detailed analysis and approximation are presented. SPICE simulations are demonstrated to show high correlation with these approximations.'],\n",
       "   '2000']],\n",
       " [[['Switching Window Computation for Static Timing Analysis in Presence of Crosstalk Noise.',\n",
       "    'Crosstalk effect is crucial for timing analysis in very deep submicron design. In this paper, we present and compare multiple scheduling algorithms to compute switching windows for static timing analysis in presence of crosstalk noise. We also introduce an efficient technique to evaluate the worst case alignment of multiple aggressors.'],\n",
       "   '2000']],\n",
       " [[['Incremental CAD.',\n",
       "    'Comprehensive study of incremental algorithms and solutions in the context of CAD tool development is an open area of research with a great deal of potential. Incremental algorithms for synthesis and layout are needed when design undergoes local or incremental change. Often these local changes are made to react to local change in the design, correct local errors or to make local improvements in one or more of the design quality metrics. In this paper we outline fundamental problems in incremental logic synthesis and physical design. Preliminary solutions to a subset of these problems will be outlined.'],\n",
       "   '2000']],\n",
       " [[['Provably Good Global Buffering Using an Available Buffer Block Plan.',\n",
       "    'To implement high-performance global interconnect without impacting the performance of existing blocks, the use of buffer blocks is increasingly popular in structured-custom and block-based ASIC/SOC methodologies. Recent works by Cong et al. [6] and Tang and Wong [25] give algorithms to solve the buffer block planning problem. In this paper we address the problem of how to perform buffering of global nets given an existing buffer block plan. Assuming as in [6, 25] that global nets have been already decomposed into two-pin connections, we give a provably good algorithm based on a recent approach of Garg and K&ouml;nemann [8] and Fleischer [7]. Our method routes connections using available buffer blocks, such that required upper and lower bounds on buffer intervals -- as well as wirelength upper bounds per connection -- are satisfied. Unlike [6, 25], our model allows more than one buffer to be inserted into any given connection. In addition, our algorithm observes buffer parity constraints, i.e., it will choose to use an inverter or a buffer (= co-located pair of inverters) according to source and destination signal parity. The algorithm outperforms previous approaches [6] and has been validated on top-level layouts extracted from a recent high-end microprocessor design.'],\n",
       "   '2000']],\n",
       " [[['MIST: An Algorithm for Memory Miss Traffic Management.',\n",
       "    'Cache misses represent a major bottleneck in embedded systems performance. Traditionally, compilers optimistically treated all memory accesses as cache hits, relying on the memory controller to account for longer miss delays. However, the memory controller has only a local view of the program, and is not able to efficiently hide the latency of these memory operations. Our compiler technique actively manages cache misses, and performs global miss traffic optimizations, to better hide the latency of the memory operations. Our memory-aware compiler scheduled several benchmarks on the TIC6211 processor architecture with a direct mapped cache, and generated an average of 61.6% improvement over the best schedule of the traditional (memory-transparent) optimizing compiler, demonstrating the utility of our miss traffic optimization approach.'],\n",
       "   '2000']],\n",
       " [[['Decomposing Refinement Proofs Using Assume-Guarantee Reasoning.',\n",
       "    'Model-checking algorithms can be used to verify, formally and automatically, if a low-level description of a design conforms with a high-level description. However, for designs with very large state spaces, prior to the application of an algorithm, the refinement-checking task needs to be decomposed into subtasks of manageable complexity. It is natural to decompose the task following the component structure of the design. However, an individual component often does not satisfy its requirements unless the component is put into the right context, which constrains the inputs to the component. Thus, in order to verify each component individually, we need to make assumptions about its inputs, which are provided by the other components of the design. This reasoning is circular: component A is verified under the assumption that context B behaves correctly, and symmetrically, B is verified assuming the correctness of A. The assume-guarantee paradigm provides a systematic theory and methodology for ensuring the soundness of the circular style of postulating and discharging assumptions in component-based reasoning.We give a tutorial introduction to the assume-guarantee paradigm for decomposing refinement-checking tasks. To illustrate the method, we step in detail through the formal verification of a processor pipeline against an instruction set architecture. In this example, the verification of a three-stage pipeline is broken up into three subtasks, one for each stage of the pipeline.'],\n",
       "   '2000']],\n",
       " [[['Smart Simulation Using Collaborative Formal and Simulation Engines.',\n",
       "    'We present Ketchum, a tool that was developed to improve the productivity of simulation-based functional verification by providing two capabilities: (1) automatic test generation and (2) unreachability analysis. Given a set of \"interesting\" signals in the design under test (DUT), automatic test generation creates input stimuli that drive the DUT through as many different combinations (called coverage states) of these signals as possible to thoroughly exercise the DUT. Unreachability analysis identifies as many unreachable coverage states as possible.Ketchum differs from the previous published results for several reasons. First, Ketchum provides 10x higher capacity than previous published results. The higher capacity is achieved by carefully orchestrating simulation and multiple formal methods including symbolic simulation, SAT-based BMC, symbolic fixpoint computation and automatic abstraction. Second, Ketchum performs not only automatic test generation but also unreachability analysis, which enables the test generation effort to be focused on coverage states that are not unreachable. Third, the backbone of Ketchum is an off-the-shelf commercial simulator. It enables Ketchum to reach deep states of the design quickly and supports simulation monitors through the standard API of the simulator during test generation.We applied Ketchum to several industrial designs, including the picoJava microprocessor from SUN and the DW8051 microcontroller from Synopsys and obtained very promising results. The experiments show that Ketchum can (1) handle design blocks containing more than 4500 latches and 170K gates, (2) reach up to 6x more coverage states than random simulation and (3) identify a majority of the unreachable coverage states.'],\n",
       "   '2000']],\n",
       " [[['Synthesis of Operation-Centric Hardware Descriptions.',\n",
       "    'Most hardware description frameworks, whether schematic or textual, use cooperating finite state machines (CFSM) as the underlying abstraction. In the CFSM framework, a designer explicitly manages the concurrency by scheduling the exact cycle-by-cycle interactions between multiple concurrent state machines. Design mistakes are common in coordinating interactions between two state machines because transitions in different state machines are not semantically coupled. It is also difficult to modify one state machine without considering its interaction with the rest of the system.This paper presents a method for hardware synthesis from an \"operation centric\" description, where the behavior of a system is described as a collection of \"atomic\" operations in the form of rules. Typically, a rule is defined by a predicate condition and an effect on the state of the system. The atomicity requirement simplifies the task of hardware description by permitting the designer to formulate each rule as if the rest of the system is static.An implementation can execute several rules concurrently in a clock cycle, provided some sequential execution of those rules can reproduce the behavior of the concurrent execution. In fact, detecting and scheduling valid concurrent execution of rules is the central issue in hardware synthesis from operation-centric descriptions. The result of this paper shows that an operation-centric framework offers significant reduction in design time, without loss in implementation quality.'],\n",
       "   '2000']],\n",
       " [[['Simulation Coverage Enhancement Using Test Stimulus Transformations.',\n",
       "    'This paper introduces the concept of abstract state exploration histories to a simulation environment, and present a test stimulus transformation (TST) technique to improve simulation coverage. State exploration histories are adapted from reachability analysis in Formal Verification. In TST, an aggressively abstracted state exploration history is maintained during simulation. While this history is being collected, test stimuli from an existing test bench are transformed on-the-fly to explore new scenarios that are not in the history. The results showed that 3-fold increase in transition coverage for a cache coherence controller, and 10 times faster coverage convergence for a MPEG2 decoder can be achieved.'],\n",
       "   '2000']],\n",
       " [[['On Mismatches between Incremental Optimizers and Instance Perturbations in Physical Design Tools.',\n",
       "    'The incremental, \"construct by correction\" design methodology has become widespread in constraint-dominated DSM design. We study the problem of ECO for physical design domains in the general context of incremental optimization. We observe that an incremental design methodology is typically built from a full optimizer that generates a solution for an initial instance, and an incremental optimizer that generates a sequence of solutions corresponding to a sequence of perturbed instances. Our hypothesis is that in practice, there can be a mismatch between the strength of the incremental optimizer and the magnitude of the perturbation between successive instances. When such a mismatch occurs, the solution quality will degrade -- perhaps to the point where the incremental optimizer should be replaced by the full optimizer. We document this phenomenon for three distinct domains -- partitioning, placement and routing -- using leading industry and academic tools. Our experiments show that current CAD tools may not be correctly designed for ECO-dominated design processes. Thus, compatibility between optimizer and instance perturbation merits attention both as a research question and as a matter of industry design practice.'],\n",
       "   '2000']],\n",
       " [[['Generalized Symmetries in Boolean Functions.',\n",
       "    \"In this paper we take a fresh look at the notion of symmetries in Boolean functions. Our studies are motivated by the fact that the classical characterization of symmetries based on invariance under variable swaps is a special case of a more general invariance based on unrestricted variable permutations. We propose a generalization of classical symmetry that allows for the simultaneous swap of ordered and unordered groups of variables, and show that it captures more of a function's invariant permutations without undue computational requirements. We apply the new symmetry definition to analyze a large set of benchmark circuits and provide extensive data showing the existence of substantial symmetries in those circuits. Specific case studies of several of these benchmarks reveal additional insights about their functional structure and how it might be related to their circuit structure.\"],\n",
       "   '2000']],\n",
       " [[['Efficient Exploration of the SoC Communication Architecture Design Space.',\n",
       "    'In this paper, we present a methodology and efficient algorithms for the design of high-performance system-on-chip communication architectures. Our methodology automatically and optimally maps the various communications between system components onto a target communication architecture template that can consist of an arbitrary interconnection of shared or dedicated channels. In addition, our techniques simultaneously configure the communication protocols of each channel in the architecture in order to optimize system performance.We motivate the need for systematic exploration of the communication architecture design space, and highlight the issues involved through illustrative examples. We present a methodology and algorithms that address these issues, including the size and complexity of the design space. We present experimental results on example systems, including a cell forwarding unit of an ATM switch, that demonstrate the benefits of using the proposed techniques. Experimental results indicate that our techniques are successful in achieving significant improvements in system performance over conventional communication architectures (observed speedups over typical architectures such as single shared buses averaged 53%). Moreover, we demonstrate that our design space exploration methodology and optimization algorithms are efficient (low CPU times), underlining their usefulness as part of any system design flow.'],\n",
       "   '2000']],\n",
       " [[['Power-Conscious Joint Scheduling of Periodic Task Graphs and Aperiodic Tasks in Distributed Real-Time Embedded Systems.',\n",
       "    'In this paper, we present a power-conscious algorithm for jointly scheduling multi-rate periodic task graphs and aperiodic tasks in distributed real-time embedded systems. While the periodic task graphs have hard deadlines, the aperiodic tasks can have either hard or soft deadlines. Periodic task graphs are first scheduled statically. Slots are created in this static schedule to accommodate hard aperiodic tasks. Soft aperiodic tasks are scheduled dynamically with an on-line scheduler. Flexibility is introduced into the static schedule and optimized to allow the on-line scheduler to make dynamic modifications to the static schedule. This helps minimize the response times of soft aperiodic tasks through both resource reclaiming and slack stealing. Of course, the validity of the static schedule is maintained. The on-line scheduler also employs dynamic voltage scaling and power management to obtain a power-efficient schedule. Experimental results show that the flexibility introduced into the static schedule helps improve the response times of soft aperiodic tasks by up to 43%. Dynamic voltage scaling and power management reduce power by up to 68%. The scheme in which the static schedule is allowed to be flexible achieves up to 32% more power saving compared to the scheme in which no flexibility is allowed, when both schemes are power-conscious. Our work gives an average architecture price saving of 30% over a previous approach for embedded system architectures synthesized with execution slots for hard aperiodic tasks present.'],\n",
       "   '2000']],\n",
       " [[['Simulation Based Test Generation for Scan Designs.',\n",
       "    'We describe a simulation-based test generation procedure for scan designs. A test sequence generated by this procedure consists of a sequence of one or more primary input vectors embedded between a scan-in operation and a scan-out operation. We consider the set of faults that can be detected by test sequences of this form, compared to the case where scan is applied with every test vector. The proposed procedure constructs test sequences that traverse as many pairs of fault-free/faulty states as possible, and thus avoids the use of branch-and-bound test generation techniques. Additional techniques are incorporated into this basic procedure to enhance its effectiveness.'],\n",
       "   '2000']],\n",
       " [[['UST/DME: A Clock Tree Router for General Skew Constraints.',\n",
       "    'In this paper, we propose new approaches for solving the useful-skew tree (UST) routing problem [17]: Clock routing subject to general skew constraints. The clock layout synthesis engine of our UST algorithms is based on the deferred-merge embedding (DME) paradigm for zero-skew tree [5; 1] and bounded-skew tree [8; 2] routings; hence, the names UST/DME and Greedy-UST/DME for our algorithms. They simultaneously perform skew scheduling and tree routing such that each local skew range is incrementally refined to a skew value that minimizes the wirelength during the bottom-up merging phase of DME. The resulting skew schedule is not only feasible, but is also best for routing in terms of wirelength. The experimental results show very encouraging improvement over the previous BST/DME algorithm on three ISCAS89 benchmarks under general skew constraints in terms of total wirelength.'],\n",
       "   '2000']],\n",
       " [[['Data Path Placement with Regularity.',\n",
       "    'As more data processing functions are integrated into systems-on-chip, data path is becoming a critical part of the whole VLSI design. However, traditional physical design methodology can not satisfy the data path performance requirement because it has no knowledge of the data path bit-sliced structure. In this paper, an Abstract Physical Model (APM) is proposed to extract bit-slice regularity information from Data Flow Graph (DFG) and it is used for interconnect and congestion planning. A two step heuristic algorithm is introduced to optimize the linear placement of APM to satisfy both the wire length and routing track budget.'],\n",
       "   '2000']],\n",
       " [[['Test of Future System-on-Chips.',\n",
       "    'Spurred by technology leading to the availability of millions of gates per chip, system-level integration is evolving as a new paradigm, allowing entire systems to be built on a single chip. Being able to rapidly develop, manufacture, test, debug and verify complex SOCs is crucial for the continued success of the electronics industry. This growth is expected to continue full force at least for the next decade, while making possible the production of multimillion transistor chips. However, to make its production practical and cost effective, the industry road maps identify a number of major hurdles to be overcome. The key hurdle is related to test and diagnosis. This embedded tutorial analyzes these hurdles, relates them to the advancements in semiconductor technology and presents potential solutions to address them. These solutions are meant to ensure that test and diagnosis contribute to the overall growth of the SOC industry and do not slow it down. This embedded tutorial in addition presents the state-of-the-art in system-level integration and addresses the strategies and current industrial practices in the test of system-on-chip. It discusses the requirements for test reuse in hierarchical design, such as embedded test strategies for individual cores, test access mechanisms, optimizing test resource partitioning, and embedded test management and integration at the System-on-Chip level. Processor cores being one of the most common cores embedded in a SOC, issues related to self-testing embedded processor cores are addressed. Future research challenges and opportunities are discussed in enabling testing of future SOCs which use deep submicron technologies.'],\n",
       "   '2000']],\n",
       " [[['Rectilinear Block Placement Using B*-Trees.',\n",
       "    'Due to the layout complexity in modern VLSI designs, integrated circuit blocks may not be rectangular. However, literature on general rectilinear block placement is still quite limited. In this article, we present approaches for handling the placement for arbitrarily shaped rectilinear blocks using B*-trees [Chang et al. 2000]. We derive the feasibility conditions of B*-trees to guide the placement of rectilinear blocks. Experimental results show that our algorithm achieves optimal or near-optimal block placement for benchmarks with various shaped blocks.'],\n",
       "   '2000']],\n",
       " [[['Automating Statistics Management for Query Optimizers.',\n",
       "    'Statistics play a key role in influencing the quality of plans chosen by a database query optimizer. In this paper, we identify the statistics that are essential for an optimizer. We introduce novel techniques that help significantly reduce the set of statistics that need to be created without sacrificing the quality of query plans generated. We discuss how these techniques can be leveraged to automate statistics management in databases. We have implemented and experimentally evaluated our approach on Microsoft SQL Server 7.0.'],\n",
       "   '2000']],\n",
       " [[['Finding Interesting Associations without Support Pruning.',\n",
       "    'Association-rule mining has heretofore relied on the condition of high support to do its work efficiently. In particular, the well-known a priori algorithm is only effective when the only rules of interest are relationships that occur very frequently. However, there are a number of applications, such as data mining, identification of similar web documents, clustering, and collaborative filtering, where the rules of interest have comparatively few instances in the data. In these cases, we must look for highly correlated items, or possibly even causal relationships between infrequent items. We develop a family of algorithms for solving this problem, employing a combination of random sampling and hashing techniques. We provide analysis of the algorithms developed and conduct experiments on real and synthetic data to obtain a comparative performance analysis.'],\n",
       "   '2000']],\n",
       " [[['DEMON: Mining and Monitoring Evolving Data.',\n",
       "    'Data mining algorithms have been the focus of much research recently. In practice, the input data to a data mining process resides in a large data warehouse whose data is kept up-to-date through periodic or occasional addition and deletion of blocks of data. Most data mining algorithms have either assumed that the input data is static, or have been designed for arbitrary insertions and deletions of data records. In this paper, we consider a dynamic environment that evolves through systematic addition or deletion of blocks of data. We introduce a new dimension, called the data span dimension, which allows user-defined selections of a temporal subset of the database. Taking this new degree of freedom into account, we describe efficient model maintenance algorithms for frequent itemsets and clusters. We then describe a generic algorithm that takes any traditional incremental model maintenance algorithm and transforms it into an algorithm that allows restrictions on the data span dimension. We also develop an algorithm for automatically discovering a specific class of interesting block selection sequences. In a detailed experimental study, we examine the validity and performance of our ideas on synthetic and real datasets.'],\n",
       "   '2000']],\n",
       " [[['An operational semantics for parallel lazy evaluation.',\n",
       "    'We present an operational semantics for parallel lazy evaluation that accurately models the parallel behaviour of the non-strict parallel functional language GpH. Parallelism is modelled synchronously, that is, single reductions are carried out separately then combined before proceeding to the next set of reductions. Consequently the semantics has two levels, with transition rules for individual threads at one level and combining rules at the other. Each parallel thread is modelled by a binding labelled with an indication of its activity status. To the best of our knowledge this is the first semantics that models such thread states. A set of labelled bindings corresponds to a heap and is used to model sharing.The semantics is set at a higher level of abstraction than an abstract machine and is therefore more manageable for proofs about programs rather than implementations. At the same time, it is sufficiently low level to allow us to reason about programs in terms of parallelism (i.e. the number of processors used) as well as work and run-time with different numbers of processors.The framework used by the semantics is sufficiently flexible and general that it can easily be adapted to express other evaluation models such as sequential call-by-need, speculative evaluation, non-deterministic choice and others.'],\n",
       "   '2000']],\n",
       " [[['Recursive monadic bindings.',\n",
       "    'Monads have become a popular tool for dealing with computational effects in Haskell for two significant reasons: equational reasoning is retained even in the presence of effects; and program modularity is enhanced by hiding \"plumbing\" issues inside the monadic infrastructure. Unfortunately, not all the facilities provided by the underlying language are readily available for monadic computations. In particular, while recursive monadic computations can be defined directly using Haskell\\'s built-in recursion capabilities, there is no natural way to express recursion over the values of monadic actions. Using examples, we illustrate why this is a problem, and we propose an extension to Haskell\\'s donotation to remedy the situation. It turns out that the structure of monadic value-recursion depends on the structure of the underlying monad. We propose an axiomatization of the recursion operation and provide a catalogue of definitions that satisfy our criteria.'],\n",
       "   '2000']],\n",
       " [[['Cheap eagerness: speculative evaluation in a lazy functional language.',\n",
       "    'Cheap eagerness is an optimization where cheap and safe expressions are evaluated before it is known that their values are needed. Many compilers for lazy functional languages implement this optimization, but they are limited by a lack of information about the global flow of control and about which variables are already evaluated. Without this information, even a variable reference is a potentially unsafe expression!In this paper we show that significant speedups are achievable by cheap eagerness. Our cheapness analysis uses the results of a program-wide data and control flow analysis to find out which variables may be unevaluated and which variables may be bound to functions which are dangerous to call.'],\n",
       "   '2000']],\n",
       " [[['More types for nested data parallel programming.',\n",
       "    'This paper generalises the flattening transformation---a technique for the efficient implementation of nested data parallelism---and reconciles it with main stream functional programming. Nested data parallelism is significantly more expressive and convenient to use than the flat data parallelism typically used in conventional parallel languages like High Performance Fortran and C*. The flattening transformation of Blelloch and Sabot is a key technique for the efficient implementation of nested parallelism via flat parallelism, but originally it was severely restricted, as it did not permit general sum types, recursive types, higher-order functions, and separate compilation. Subsequent work, including some of our own, generalised the transformation and allowed higher-order functions and recursive types. In this paper, we take the final step of generalising flattening to cover the full range of types available in modern languages like Haskell and ML; furthermore, we enable the use of separate compilation. In addition, we present a completely new formulation of the transformation, which is based on the standard lambda calculus notation, and replace a previously ad-hoc transformation step by a systematic generic programming technique. First experiments demonstrate the efficiency of our approach.'],\n",
       "   '2000']],\n",
       " [[['Non-stop Haskell.',\n",
       "    'We describe an efficient technique for incorporating Baker\\'s incremental garbage collection algorithm into the Spineless Tagless G-machine on stock hardware. This algorithm eliminates the stop/go execution associated with bulk copying collection algorithms, allowing the system to place an upper bound on the pauses due to garbage collection. The technique exploits the fact that objects are always accessed by jumping to code rather than being explicitly dereferenced. It works by modifying the entry code-pointer when an object is in the transient state of being evacuated but not scavenged. An attempt to enter it from the mutator causes the object to \"self-scavenge\" transparently before resetting its entry code pointer. We describe an implementation of the scheme in v4.01 of the Glasgow Haskell Compiler and report performance results obtained by executing a range of applications. These experiments show that the read barrier can be implemented in dynamic dispatching systems such as the STG-machine with very short mutator pause times and with negligible overhead on execution time.'],\n",
       "   '2000']],\n",
       " [[['Recursive subtyping revealed: functional pearl.',\n",
       "    'Algorithms for checking subtyping between recursive types lie at the core of many programming language implementations. But the fundamental theory of these algorithms and how they relate to simpler declarative specifications is not widely understood, due in part to the difficulty of the available introductions to the area. This tutorial paper offers an \"end-to-end\" introduction to recursive types and subtyping algorithms, from basic theory to efficient implementation, set in the unifying mathematical framework of coinduction.'],\n",
       "   '2000']],\n",
       " [[['QuickCheck: a lightweight tool for random testing of Haskell programs.',\n",
       "    'Quick Check is a tool which aids the Haskell programmer in formulating and testing properties of programs. Properties are described as Haskell functions, and can be automatically tested on random input, but it is also possible to define custom test data generators. We present a number of case studies, in which the tool was successfully used, and also point out some pitfalls to avoid. Random testing is especially suitable for functional programs because properties can be stated at a fine grain. When a function is built from separately tested components, then random testing suffices to obtain good coverage of the definition under test.'],\n",
       "   '2000']],\n",
       " [[['Syntactic accidents in program analysis: on the impact of the CPS transformation.',\n",
       "    'We show that a non-duplicating transformation into Continuation-Passing Style (CPS) has no effect on control-flow analysis, a positive effect on binding-time analysis for traditional partial evaluation, and no effect on binding-time analysis for continuation-based partial evaluation: a monovariant control-flow analysis yields equivalent results on a direct-style program and on its CPS counterpart, a monovariant binding-time analysis yields less precise results on a direct-style program than on its CPS counterpart, and an enhanced monovariant binding-time analysis yields equivalent results on a direct-style program and on its CPS counterpart. Our proof technique amounts to constructing the CPS counterpart of flow information and of binding times. Our results formalize and confirm a folklore theorem about traditional binding-time analysis, namely that CPS has a positive effect on binding times. What may be more surprising is that the benefit does not arise from a standard refinement of program analysis, as, for instance, duplicating continuations. The present study is symptomatic of an unsettling property of program analyses: their quality is unpredictably vulnerable to syntactic accidents in source programs, i.e., to the way these programs are written. More reliable program analyses require a better understanding of the effect of syntactic change.'],\n",
       "   '2000']],\n",
       " [[['Intersection types and computational effects.',\n",
       "    'We show that standard formulations of intersection type systems are unsound in the presence of computational effects, and propose a solution similar to the value restriction for polymorphism adopted in the revised definition of Standard ML. It differs in that it is not tied to let-expressions and requires an additional weakening of the usual subtyping rules. We also present a bi-directional type-checking algorithm for the resulting language that does not require an excessive amount of type annotations and illustrate it through some examples. We further show that the type assignment system can be extended to incorporate parametric polymorphism. Taken together, we see our system and associated type-checking algorithm as a significant step towards the introduction of intersection types into realistic programming languages. The added expressive power would allow many more properties of programs to be stated by the programmer and statically verified by a compiler.'],\n",
       "   '2000']],\n",
       " [[['Deriving backtracking monad transformers.',\n",
       "    'In a paper about pretty printing J. Hughes introduced two fundamental techniques for deriving programs from their specification, where a specification consists of a signature and properties that the operations of the signature are required to satisfy. Briefly, the first technique, the term implementation, represents the operations by terms and works by defining a mapping from operations to observations --- this mapping can be seen as defining a simple interpreter. The second, the context-passing implementation, represents operations as functions from their calling context to observations. We apply both techniques to derive a backtracking monad transformer that adds backtracking to an arbitrary monad. In addition to the usual backtracking operations --- failure and nondeterministic choice --- the prolog cut and an operation for delimiting the effect of a cut are supported.'],\n",
       "   '2000']],\n",
       " [[['Regular expression types for XML.',\n",
       "    \"We propose regular expression types as a foundation for statically typed XML processing languages. Regular expression types, like most schema languages for XML, introduce regular expression notations such as repetition (&ast;), alternation (&verbar;), etc., to describe XML documents. The novelty of our type system is a semantic presentation of subtyping, as inclusion between the sets of documents denoted by two types. We give several examples illustrating the usefulness of this form of subtyping in XML processing.The decision problem for the subtype relation reduces to the inclusion problem between tree automata, which is known to be EXPTIME-complete. To avoid this high complexity in typical cases, we develop a practical algorithm that, unlike classical algorithms based on determinization of tree automata, checks the inclusion relation by a top-down traversal of the original type expressions. The main advantage of this algorithm is that it can exploit the property that type expressions being compared often share portions of their representations. Our algorithm is a variant of Aiken and Murphy's set-inclusion constraint solver, to which are added several new implementation techniques, correctness proofs, and preliminary performance measurements on some small programs in the domain of typed XML processing.\"],\n",
       "   '2000']],\n",
       " [[['Information flow inference for free.',\n",
       "    'This paper shows how to systematically extend an arbitrary type system with dependency information, and how soundness and non-interference proofs for the new system may rely upon, rather than duplicate, the soundness proof of the original system. This allows enriching virtually any of the type systems known today with information flow analysis, while requiring only a minimal proof effort.Our approach is based on an untyped operational semantics for a labelled calculus akin to core ML. Thus, it is simple, and should be applicable to other computing paradigms, such as object or process calculi.The paper also discusses access control, and shows it may be viewed as entirely independent of information flow control. Letting the two mechanisms coexist, without interacting, yields a simple and expressive type system, which allows, in particular, \"selective\" declassification.'],\n",
       "   '2000']],\n",
       " [[['Make it practical: a generic linear-time algorithm for solving maximum-weightsum problems.',\n",
       "    'In this paper we propose a new method for deriving a practical linear-time algorithm from the specification of a maximum-weightsum problem: From the elements of a data structure x, find a subset which satisfies a certain property p and whose weightsum is maximum. Previously proposed methods for automatically generating linear-time algorithms are theoretically appealing, but the algorithms generated are hardly useful in practice due to a huge constant factor for space and time. The key points of our approach are to express the property p by a recursive boolean function over the structure x rather than a usual logical predicate and to apply program transformation techniques to reduce the constant factor. We present an optimization theorem, give a calculational strategy for applying the theorem, and demonstrate the effectiveness of our approach through several nontrivial examples which would be difficult to deal with when using the methods previously available.'],\n",
       "   '2000']],\n",
       " [[['Understanding memory allocation of scheme programs.',\n",
       "    'Memory is the performance bottleneck of modern architectures. Keeping memory consumption as low as possible enables fast and unobtrusive applications. But it is not easy to estimate the memory use of programs implemented in functional languages, due to both the complex translations of some high level constructs, and the use of automatic memory managers.To help understand memory allocation behavior of Scheme programs, we have designed two complementary tools. The first one reports on frequency of allocation, heap configurations and on memory reclamation. The second tracks down memory leaks1. We have applied these tools to our Scheme compiler, the largest Scheme program we have been developing. This has allowed us to drastically reduce the amount of memory consumed during its bootstrap process, without requiring much development time.Development tools will be neglected unless they are both conveniently accessible and easy to use. In order to avoid this pitfall, we have carefully designed the user interface of these two tools. Their integration into a real programming environment for Scheme is detailed in the paper.'],\n",
       "   '2000']],\n",
       " [[['Fully reflexive intensional type analysis.',\n",
       "    'Compilers for polymorphic languages can use runtime type inspection to support advanced implementation techniques such as tagless garbage collection, polymorphic marshalling, and flattened data structures. Intensional type analysis is a type-theoretic framework for expressing and certifying such type-analyzing computations. Unfortunately, existing approaches to intensional analysis do not work well on types with universal, existential, or fixpoint quantifiers. This makes it impossible to code applications such as garbage collection, persistence, or marshalling which must be able to examine the type of any runtime value.We present a typed intermediate language that supports fully reflexive intensional type analysis. By fully reflexive, we mean that type-analyzing operations are applicable to the type of any runtime value in the language. In particular, we provide both type-level and term-level constructs for analyzing quantified types. Our system supports structural induction on quantified types yet type checking remains decidable. We show how to use reflexive type analysis to support type-safe marshalling and how to generate certified type-analyzing object code.'],\n",
       "   '2000']],\n",
       " [[['Type-safe cast: functional pearl.',\n",
       "    'In a language with non-parametric or ad-hoc polymorphism, it is possible to determine the identity of a type variable at run-time. With this facility, we can write a function to convert a term from one abstract type to another, if the two hidden types are identical. However, the naive implementation of this function requires that the term be destructed and rebuilt. In this paper, we show how to eliminate this overhead using higher-order type abstraction. We demonstrate this solution in two frameworks for ad-hoc polymorphism: intensional type analysis and type classes.'],\n",
       "   '2000']],\n",
       " [[['The functional guts of the Kleisli query system.',\n",
       "    'Kleisli is a modern data integration system that has made a significant impact on bioinformatics data integration. The primary query language provided by Kleisli is called CPL, which is a functional query language whose surface syntax is based on the comprehension syntax. Kleisli is itself implemented using the functional language SML. This paper describes the influence of functional programming research that benefits the Kleisli system, especially the less obvious ones at the implementation level.'],\n",
       "   '2000']],\n",
       " [[['Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers.',\n",
       "    'We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm. The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical <em>multiclass</em> loss bound given the empirical loss of the individual <em>binary</em> learning algorithms. The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms.'],\n",
       "   '2000']],\n",
       " [[['Online Ensemble Learning: An Empirical Study.',\n",
       "    'We study resource-limited online learning, motivated by the problem of conditional-branch outcome prediction in computer architecture. In particular, we consider (parallel) time and space-efficient ensemble learners for online settings, empirically demonstrating benefits similar to those shown previously for offline ensembles. Our learning algorithms are inspired by the previously published &ldquo;boosting by filtering&rdquo; framework as well as the offline Arc-x4 boosting-style algorithm. We train ensembles of online decision trees using a novel variant of the ID4 online decision-tree algorithm as the base learner, and show empirical results for both boosting and bagging-style online ensemble methods. Our results evaluate these methods on both our branch prediction domain and online variants of three familiar machine-learning benchmarks. Our data justifies three key claims. First, we show empirically that our extensions to ID4 significantly improve performance for single trees and additionally are critical to achieving performance gains in tree ensembles. Second, our results indicate significant improvements in predictive accuracy with ensemble size for the boosting-style algorithm. The bagging algorithms we tried showed poor performance relative to the boosting-style algorithm (but still improve upon individual base learners). Third, we show that ensembles of small trees are often able to outperform large single trees with the same number of nodes (and similarly outperform smaller ensembles of larger trees that use the same total number of nodes). This makes online boosting particularly useful in domains such as branch prediction with tight space restrictions (i.e., the available real-estate on a microprocessor chip).'],\n",
       "   '2000']],\n",
       " [[['Identifying Parallelism in Programs with Cyclic Graphs.',\n",
       "    'Dependence analysis algorithms have been proposed to identify parallelism in programs with tree-like data structures. However, they cannot analyze the dependence of statements if recursive data structures of programs are cyclic. This paper presents a technique to identify parallelism in programs with cyclic graphs. The technique consists of three steps: (1) traversal patterns that loops or recursive procedures traverse graphs are identified, and the statements that construct the links of traversal patterns will be located by definition-use chains of recursive data structures; (2) traversal-pattern-sensitive shape analysis is performed to estimate possible shapes of traversal patterns, (3) dependence analysis is performed to identify parallelism using the result of shape analysis. This approach can identify parallelism in programs with cyclic data structures due to the facts that many programs follow acyclic structures (i.e. traversal patterns) to access all nodes on the cyclic data structures. Once the traversal patterns are isolated from the overall data structures, dependence analysis can be applied to identify parallelism.'],\n",
       "   '2000']],\n",
       " [[['Nonblocking WDM Multicast Switching Networks.',\n",
       "    'With ever increasing demands on bandwidth from emerging bandwidth-intensive applications, such as video conferencing, E-commerce, and video-on-demand services, there has been an acute need for very high bandwidth transport network facilities. Optical networks are a promising candidate for this type of applications. At the same time, many bandwidth-intensive applications require multicast services for efficiency purposes. Multicast has been extensively studied in the parallel processing and electronic networking community and has started to receive attention in the optical network community recently. In particular, as WDM (wavelength division multiplexing) networks emerge, supporting WDM multicast becomes increasingly attractive. In this paper, we consider efficient designs of multicast-capable WDM switching networks, which are significantly different and, hence, require nontrivial extensions from their electronic counterparts. We first discuss various multicast models in WDM networks and analyze the nonblocking multicast capacity and network cost under these models. We then propose two methods to construct nonblocking multistage WDM networks to reduce the network cost.'],\n",
       "   '2000']],\n",
       " [[['Symbolic Graph Matching Using the EM Algorithm and Singular Value Decomposition.',\n",
       "    'This paper describes an efficient algorithm for inexact graph matching. The method is purely structural, that is, it uses only the edge or connectivity structure of the graph and does not draw on node or edge attributes. We make two contributions: 1) commencing from a probability distribution for matching errors, we show how the problem of graph matching can be posed as maximum-likelihood estimation using the apparatus of the EM algorithm; and 2) we cast the recovery of correspondence matches between the graph nodes in a matrix framework. This allows one to efficiently recover correspondence matches using the singular value decomposition. We experiment with the method on both real-world and synthetic data. Here, we demonstrate that the method offers comparable performance to more computationally demanding methods'],\n",
       "   '2000']],\n",
       " [[['A Method for Fine Registration of Multiple View Range Images Considering the Measurement Error Properties.',\n",
       "    'This paper presents a new method for fine registration of two range images from different viewpoints that have been roughly registered. Our method deals with the properties of the measurement error of the range image data. The error distribution is different for each point in the image and is usually dependent on both the viewing direction and the distance to the object surface. We find the best transformation of two range images to align each measured point and reconstruct 3D total object shape by taking such properties of the measurement error into account. The position of each measured point is corrected according to the variance and the extent of the distribution of its measurement error. The best transformation is selected by the evaluation of the effectiveness of this correction of every measured point. The experiments showed that our method produced better results than the conventional ICP method.'],\n",
       "   '2000']],\n",
       " [[['Measuring Shape: Ellipticity, Rectangularity, and Triangularity.',\n",
       "    'Object classification often operates by making decisions based on the values of several shape properties measured from an image of the object. This paper describes several algorithms (both old and new) for calculating ellipticity, rectangularity, and triangularity shape descriptors. The methods are evaluated by testing on both synthetic and real data.'],\n",
       "   '2000']],\n",
       " [[['Synthesizing transformations for locality enhancement of imperfectly-nested loop nests.',\n",
       "    'Linear loop transformations and tiling are known to be very effective for enhancing locality of reference in perfectly-nested loops. However, they cannot be applied directly to imperfectly-nested loops. Some compilers attempt to convert imperfectly-nested loops into perfectly-nested loops by using statement sinking, loop fusion, etc., and then apply locality enhancing transformations to the resulting perfectly-nested loops, but the approaches used are fairly ad hoc and may fail even for simple programs. In this paper, we present a systematic approach for synthesizing transformations to enhance locality in imperfectly-nested loops. The key idea is to embed the iteration space of each statement into a special iteration space called the product space. The product space can be viewed as a perfectly-nested loop nest, so embedding generalizes techniques like statement sinking and loop fusion which are used in ad hoc ways in current compilers to produce perfectly-nested loops from imperfectly-nested ones. In contrast to these ad hoc techniques however, our embeddings are chosen carefully to enhance locality. The product space can itself be transformed to increase locality further, after which fully permutable loops can be tiled. The final code generation step may produce imperfectly-nested loops as output if that is desirable. We present experimental evidence for the effectiveness of this approach, using dense numerical linear algebra benchmarks, relaxation codes, and the tomcatv code from the SPEC benchmarks.'],\n",
       "   '2000']],\n",
       " [[['Automatic loop transformations and parallelization for Java.',\n",
       "    'From a software engineering perspective, the Java programming language provides an attractive platform for writing numerically intensive applications. A major drawback hampering its widespread adoption in this domain has been its poor performance on numerical codes. This paper describes a prototype Java compiler which demonstrates that it is possible to achieve performance levels approaching those of current state-of-the-art C, C++ and Fortran compilers on numerical codes. We describe a new transformation called alias versioning that takes advantage of the simplicity of pointers in Java. This transformation, combined with other techniques that we have developed, enables the compiler to perform high order loop transformations (for better data locality) and parallelization completely automatically. We believe that our compiler is the first to have such capabilities of optimizing numerical Java codes. We achieve, with Java, between 80 and 100% of the performance of highly optimized Fortran code in a variety of benchmarks. Furthermore, the automatic parallelization achieves speedups of up to 3.8 on four processors. Combining this compiler technology with packages containing the features expected by programmers of numerical applications would enable Java to become a serious contender for implementing new numerical applications.'],\n",
       "   '2000']],\n",
       " [[['A low-complexity issue logic.',\n",
       "    \"One of the main concerns in today's processor design is the issue logic. Instruction-level parallelism is usually favored by an out-of-order issue mechanism where instructions can issue independently of the program order. The out-of-order scheme yields the best performance but at the same time introduces important hardware costs such as an associative look-up, which might be prohibitive for wide issue processors with large instruction windows. This associative search may slow-down the clock-rate and it has an important impact on power consumption. In this work, two new issue schemes that reduce the hardware complexity of the issue logic with minimal impact on the average number of instructions executed per cycle are presented.\"],\n",
       "   '2000']],\n",
       " [[['Characterizing processor architectures for programmable network interfaces.',\n",
       "    'The rapid advancements of networking technology have boosted potential bandwidth to the point that the cabling is no longer the bottleneck. Rather, the bottlenecks lie at the crossing points, the nodes of the network, where data traffic is intercepted or forwarded. As a result, there has been tremendous interest in speeding those nodes, making the equipment run faster by means of specialized chips to handle data trafficking. The Network Processor is the blanket name thrown over such chips in their varied forms. To date, no performance data exist to aid in the decision of what processor architecture to use in next generation network processor. Our goal is to remedy this situation. In this study, we characterize both the application workloads that network processors need to support as well as emerging applications that we anticipate may be supported in the future. Then, we consider the performance of three sample benchmarks drawn from these workloads on several state-of-the-art processor architectures, including: an aggressive, out-of-order, speculative super-scalar processor, a fine-grained multithreaded processor, a single chip multiprocessor, and a simultaneous multithreaded processor (SMT). The network interface environment is simulated in detail, and our results indicate that SMT is the architecture best suited to this environment.'],\n",
       "   '2000']],\n",
       " [[['Compiling object-oriented data intensive applications.',\n",
       "    'Processing and analyzing large volumes of data plays an increasingly important role in many domains of scientific research. High-level language and compiler support for developing applications that analyze and process such datasets has, however, been lacking so far.In this paper, we present a set of language extensions and a prototype compiler for supporting high-level object-oriented programming of data intensive reduction operations over multidimensional data. We have chosen a dialect of Java with data-parallel extensions for specifying collection of objects, a parallel for loop, and reduction variables as our source high-level language. Our compiler analyzes parallel loops and optimizes the processing of datasets through the use of an existing run-time system, called Active Data Repository (ADR). We show how loop fission followed by interprocedural static program slicing can be used by the compiler to extract required information for the run-time system. We present the design of a compiler/n-time interface which allows the compiler to effectively utilize the existing run-time system.A prototype compiler incorporating these techniques has been developed using the Titanium front-end from Berkeley. We have evaluated this compiler by comparing the performance of compiler generated code with hand customized ADR code for three templates, from the areas of digital microscopy and scientific simulations. Our experimental results show that the performance of compiler generated versions is, on the average 21% lower, and in all cases within a factor of two, of the performance of hand coded versions.'],\n",
       "   '2000']],\n",
       " [[['Automated cache optimizations using CME driven diagnosis.',\n",
       "    'Demonstrating our framework on a collection of scientific loop nests, we were able to reduce an average of 84% of cache misses in the optimizable loop nests. This work lays the groundwork for handling a wide range of optimizations through further study of solution patterns in the CME solution table.'],\n",
       "   '2000']],\n",
       " [[['Binary translation and architecture convergence issues for IBM system/390.',\n",
       "    'We describe the design issues in an implementation of the ESA/390 architecture based on binary translation to a very long instruction word (VLIW) processor. During binary translation, complex ESA/390 instructions are decomposed into instruction &ldquo;primitives&rdquo; which are then scheduled onto a wide-issue machine. The aim is to achieve high instruction level parallelism due to the increased scheduling and optimization opportunities which can be exploited by binary translation software, combined with the efficiency of long instruction word architectures. A further aim is to study the feasibility of a common execution platform for different instruction set architectures, such as ESA/390, RS?6000, AS/400 and the Java Virtual Machine, so that multiple systems can be built around a common execution platform.'],\n",
       "   '2000']],\n",
       " [[['Fast greedy weighted fusion.',\n",
       "    'Loop fusion is an important compiler strategy for managing memory hierarchy. By fusing loops that use the same data elements, a compiler can reduce the distance between accesses to the same datum and avoid costly cache misses. Unfortunately the problem of optimal loop fusion for reuse has been shown to be NP-hard, so compilers must resort to heuristics to avoid unreasonably long compile times. Greedy strategies are often excellent heuristics that produce high-quality solutions quickly. We present an algorithm for greedy weighted fusion, in which the heaviest edge (the one with the most reuse) is selected for possible fusion on each step. The algorithm is shown to be fast in the sense that it takes O(V(E+V)) time, which is arguably optimal for producing the greedy solution. In addition, this algorithm has the advantage that it requires only O(E) edge reweighting operations after fusions. This means that it is suitable for use on the problem of enhancing cache reuse, for which the ideal reweighting operation is much more complex than addition. If each reweighting operation requires no more than O(V) time, the time bound of the overall fusion process remains at O(V(E+V)).'],\n",
       "   '2000']],\n",
       " [[['Using complete system simulation to characterize SPECjvm98 benchmarks.',\n",
       "    'Complete system simulation to understand the influence of architecture and operating systems on application execution has been identified to be crucial for systems design. While there have been previous attempts at understanding the architectural impact of Java programs, there has been no prior work investigating the operating system (kernel) activity during their executions. This problem is particularly interesting in the context of Java since it is not only the application that can invoke kernel services, but so does the underlying Java Virtual Machine (JVM) implementation which runs these programs. Further, the JVM style (JIT compiler or interpreter) and the manner in which the different JVM components (such as the garbage collector and class loader) are exercised, can have a significant impact on the kernel activities.To investigate these issues, this research uses complete system simulation of the SPECjvm98 benchmarks on the SimOS simulation platform. The execution of these benchmarks on both JIT compilers and interpreters is profiled in detail, to identify and quantify where time is spent in each component. The kernel activity of SPECjvm98 applications constitutes up to 17% of the execution time in the large dataset and up to 31% in the small dataset. The average kernel activity in the large dataset is approximately 10%, in comparison to around 2% in four SPECInt benchmarks studied. Of the kernel services, TLB miss handling is the most dominant in all applications. The TLB miss rates in the JIT compiler, dynamic class loader and garbage collector portions of the JVM are individually analyzed. In addition to such execution profiles, the ILP in the user and kernel mode are also quantified. The Java code is seen to limit exploitable parallelism and aggressive instruction issue is seen to be less efficient for SPECjvm98 benchmarks in comparison to SPEC95 programs. Also, the kernel mode of execution does not exhibit as much ILP as the user mode.'],\n",
       "   '2000']],\n",
       " [[['Using profiling to reduce branch misprediction costs on a dynamically scheduled processor.',\n",
       "    'Modern dynamically scheduled processors use branch prediction hardware to speculatively fetch and execute most likely executed paths in a program. Complex branch predictors have been proposed which attempt to identify these paths accurately such that the hardware can benefit from out-of-order (OOO) execution. Recent studies have shown that inspite of such complex prediction schemes, there still exist many frequently executed branches which are difficult to predict. Predicated execution has been proposed as an alternative technique to eliminate some of these branches in various forms ranging from a restrictive support to a full-blown support. We call the restrictive form of predicated execution as guarded execution.In this paper, we propose a new algorithm which uses profiling and selectively performs if-conversion for architectures with guarded execution support. Branch profiling is used to gather the taken, non-taken and misprediction counts for every branch. This combined with block profiling is used to select paths which suffer from heavy mispredictions and are profitable to if-convert. Effects of three different selection criterias, namely size-based, predictability-based and profiled-based, on net cycle improvements, branch mispredictions and mis-speculated instructions are then studied. We also propose new mechanisms to convert unsafe instructions to safe form to enhance the applicability of the technique. Finally, we explain numerous adjustments that were made to the selection criterias to better reflect the OOO processor behavior.'],\n",
       "   '2000']],\n",
       " [[['A case for use-level dynamic page migration.',\n",
       "    'This paper presents user-level dynamic page migration, a runtime technique which transparently enables parallel programs to tune their memory performance on distributed shared memory multiprocessors, with feedback obtained from dynamic monitoring of memory activity. Our technique exploits the iterative nature of parallel programs and information available to the program both at compile time and at runtime in order to improve the accuracy and the timeliness of page migrations, as well as amortize better the overhead, compared to page migration engines implemented in the operating system. We present an adaptive page migration algorithm based on a competitive and a predictive criterion. The competitive criterion is used to correct poor page placement decisions of the operating system, while the predictive criterion makes the algorithm responsive to scheduling events that necessitate immediate page migrations, such as preemptions and migrations of threads. We also present a new technique for preventing page pingpong and a mechanism for monitoring the performance of page migration algorithms at runtime and tuning their sensitive parameters accordingly. Our experimental evidence on a SGI Origin2000 shows that unmodified OpenMP codes linked with our runtime system for dynamic page migration are effectively immune to the page placement strategy of the operating system and the associated problems with data locality. Furthermore, our runtime system achieves solid performance improvements compared to the IRIX 6.5.5 page migration engine, for single parallel OpenMP codes and multiprogrammed workloads.'],\n",
       "   '2000']],\n",
       " [[['Hardware spatial forwarding for widely shared data.',\n",
       "    'Applications with widely shared data do not perform well on cc-NUMA multiprocessors due to the hot-spots they create in the system. In this paper we address this problem by enhancing the memory controller with a forwarding mechanism capable of hiding the read latency of widely shared data, while potentially decreasing the memory and network contention. Based on the influx of requests, the memory anticipates the next read references and forwards the data in advance to the processors. To identify the set of processors the data is to be forwarded to we use a heuristic based on the spatial locality of memory blocks. To increase the forwarding effectiveness and minimize the number of messages, we incorporate simple filters combined with a feedback mechanism. We also show that further improvements are possible using a combined software-prefetching/hardware-forwarding approach. Our experimental results obtained with a detailed execution driven simulator with ILP processors show significant improvements in execution time (up to 37%).'],\n",
       "   '2000']],\n",
       " [[['Optimized unrolling of nested loops.',\n",
       "    'Loop unrolling is a well known loop transformation that has been used in optimizing compilers for over three decades. In this paper, we address the problems of automatically selecting unroll factors for perfectly nested loops, and generating compact code for the selected unroll factors. Compared to past work, the contributions of our work include (i) a more detailed cost model that includes register locality, instruction-level parallelism and instruction-cache considerations; (ii) a new code generation algorithm that generates more compact code than the unroll-and-jam transformation; and (iii) a new algorithm for efficiently enumerating feasible unroll vectors. Our experimental results confirm the wide applicability of our approach by showing a 2.2&times; speedup on matrix multiply, and an average 1.08&times; speedup on seven of the SPEC95fp benchmarks (with a 1.2&times; speedup for two benchmarks). Larger performance improvements can be expected on processors that have larger numbers of registers and larger degrees of instruction-level parallelism than the processor used for our measurements (PowerPC 604).'],\n",
       "   '2000']],\n",
       " [[['Table size reduction for data value predictors by exploiting narrow width values.',\n",
       "    'Recently, the practice of speculation in resolving data dependences has been studied as a means of extracting more instruction level parallelism (ILP). An outcome of an instruction is predicted by value predictors. The instruction and its dependent instructions can be executed simultaneously, thereby exploiting ILP aggressively. One of the serious hurdles for realizing data speculation is huge hardware budget of the predictors. In this paper, we propose a technique reducing the budget by exploiting narrow width values. The hardware budget of value predictors is reduced by up to 45.1%. Simulation results show that the technique, called 2-mode scheme, maintains processor performance with slight decrease of the value prediction accuracy.'],\n",
       "   '2000']],\n",
       " [[['A novel application development environment for large-scale scientific computations.',\n",
       "    'Our results demonstrate that our novel application development environment provides both ease-of-use and high performance for large-scale, I/O-intensive scientific applications.'],\n",
       "   '2000']],\n",
       " [[['Push vs. pull: data movement for linked data structures.',\n",
       "    'As the performance gap between the CPU and main memory continues to grow, techniques to hide memory latency are essential to deliver a high performance computer system. Prefetching can often overlap memory latency with computation for array-based numeric applications. However, prefetching for pointer-intensive applications still remains a challenging problem. Prefetching linked data structures (LDS) is difficult because the address sequence of LDS traversal does not present the same arithmetic regularity as array-based applications and the data dependence of pointer dereferences can serialize the address generation process.In this paper, we propose a cooperative hardware/software mechanism to reduce memory access latencies for linked data structures. Instead of relying on the past address history to predict future accesses, we identify the load instructions that traverse the LDS, and execute them ahead of the actual computation. To overcome the serial nature of the LDS address generation, we attach a prefetch controller to each level of the memory hierarchy and push, rather than pull, data to the CPU. Our simulations, using four pointer-intensive applications, show that the push model can achieve between 4% and 30% larger reductions in execution time compared to the pull model.'],\n",
       "   '2000']],\n",
       " [[['Hardware-only stream prefetching and dynamic access ordering.',\n",
       "    'Memory system bottlenecks limit performance for many applications, and computations with strided access patterns are among the hardest hit. The streams used in such applications have extremely poor cache behavior. These access patterns have the advantage of being predictable, though, and this can be exploited to improve the efficiency of the memory subsystem in two ways: memory latencies can be masked by prefetching stream data, and the latencies can be reduced by reordering stream accesses to exploit parallelism and locality within the DRAMs. Many researchers have studied hardware prefetching in its various forms. Others have examined dynamic memory scheduling to help bridge the performance gap between processors and DRAM memory systems. This study builds on these results, combining a stride-based reference prediction table, a mechanism that prefetches L2 cache lines, and a memory controller that dynamically schedules accesses to a Direct Rambus memory subsystem. We find that such a system delivers good speedups for scientific applications with regular access patterns without negatively affecting the performance of nonstreaming programs.'],\n",
       "   '2000']],\n",
       " [[['Automatic compiler techniques for thread coarsening for multithreaded architectures.',\n",
       "    'Multithreaded architectures are emerging as an important class of parallel machines. By allowing fast context switching between threads on the same processor, these systems hide communication and synchronization latencies and allow scalable parallelism for dynamic and irregular applications. Thread partitioning is the most important task in compiling high-level languages for multithreaded architectures. Non-preemptive multithreaded architectures, which can be built from off-the-shelf components, require that if a thread issues a potentially remote memory request, then any statement that is dependent upon this request must be in a separate thread.When performing thread partitioning on codes that use pointer-based recursive data structures, it is often difficult to extract accurate dependence information. As a result, threads of unnecessarily small granularity get generated, which, because of thread switching costs, leads to increased execution time. In this paper, we present three techniques that lead to improved extraction and representation of dependence information in the presence of structured control flow, references through fields of structures, and pointer-based data structures. The benefit of these techniques is the generation of coarser-grained threads and, therefore, decreased execution time. Our experiments were performed using the EARTH-C compiler and the EARTH multithreaded architecture model emulated on both a cluster of Pentium PCs and a distributed memory multiprocessor. On our set of 6 pointer-based programs, these techniques reduced the static number of threads by 38%. Reductions in execution times ranged from 16% to 45% on the four programs we measured runtime performance.'],\n",
       "   '2000']],\n",
       " [[['Inference of message sequence charts.',\n",
       "    'Software designers draw Message Sequence Charts for early modeling of the individual behaviors they expect from the concurrent system under design. Can they be sure that precisely the behaviors they have described are realizable by some implementation of the components of the concurrent system? If so, can we automatically synthesize concurrent state machines realizing the given MSCs? If, on the other hand, other unspecified and possibly unwanted scenarios are \"implied\" by their MSCs, can the software designer be automatically warned and provided the implied MSCs? In this paper, we provide a framework in which all these questions are answered positively. We first describe the formal framework within which one can derive implied MSCs and then provide polynomial-time algorithms for implication, realizability, and synthesis.'],\n",
       "   '2000']],\n",
       " [[['Dragonfly: linking conceptual and implementation architectures of multiuser interactive systems.',\n",
       "    'Software architecture styles for developing multiuser applications are usually defined at a conceptual level, abstracting such low-level issues of distributed implementation as code replication, caching strategies and concurrency control policies. Ultimately, such conceptual architectures must be cast into code. The iterative design inherent in interactive systems implies that significant evolution will take place at the conceptual level. Equally, however, evolution occurs at the implementation level in order to tune performance. This paper introduces Dragonfly, a software architecture style that maintains a tight, bidirectional link between conceptual and implementation software architectures, allowing evolution to be performed at either level. Dragonfly has been implemented in the Java-based TeleComputing Developer (TCD) toolkit.'],\n",
       "   '2000']],\n",
       " [[['Broad-spectrum studies of log file analysis.',\n",
       "    'This paper reports on research into applying the technique of log file analysis for checking test results to a broad range of testing and other tasks. The studies undertaken included applying log file analysis to both unit- and system-level testing and to requirements of both safety-critical and non-critical systems, and the use of log file analysis in combination with other testing methods. The paper also reports on the technique of using log file analyzers to simulate the software under test, both in order to validate the analyzers and to clarify requirements. It also discusses practical issues to do with the completeness of the approach, and includes comparisons to other recently-published approaches to log file analysis.'],\n",
       "   '2000']],\n",
       " [[['Deriving test plans from architectural descriptions.',\n",
       "    'The paper presents an approach for deriving test plans for the conformance testing of a system implementation with respect to the formal description of its software architecture (SA). The SA describes a system in terms of its components and connections, therefore the derived test plans address the integration testing phase. We base our approach on a labelled transition system (LTS) modeling the SA dynamics, and on suitable abstractions of it, the Abstract Labelled Transition Systems (ALTSs). ALTSs offer specific views of the SA dynamics by concentrating on relevant features and abstracting away from uninteresting ones. ALTS is a tool we provide to the software architect that lets him/her focus on relevant behavioral patterns and more easily identify those that are meaningful for validation purposes. Intuitively, deriving an adequate set of functional test classes means deriving a set of paths appropriately covering the ALTS. We describe our approach in the scope of a real world case study and discuss in detail all the steps of our methodology, from ALTS identification to test plan generation'],\n",
       "   '2000']],\n",
       " [[['A replicated assessment and comparison of common software cost modeling techniques.',\n",
       "    \"Delivering a software product on time, within budget, and to an agreed level of quality is a critical concern for many software organizations. Underestimating software costs can have detrimental effects on the quality of the delivered software and thus on a company's business reputation and competitiveness. On the other hand, overestimation of software cost can result in missed opportunities to funds in other projects. In response to industry demand, a myriad of estimation techniques has been proposed during the last three decades. In order to assess the suitability of a technique from a diverse selection, its performance and relative merits must be compared.The current study replicates a comprehensive comparison of common estimation techniques within different organizational contexts, using data from the European Space Agency. Our study is motivated by the challenge to assess the feasibility of using multi-organization data to build cost models and the benefits gained from company-specific data collection. Using the European Space Agency data set, we investigated a yet unexplored application domain, including military and space projects. The results showed that traditional techniques, namely, ordinary least-squares regression and analysis of variance outperformed Analogy-based estimation and regression trees. Consistent with the results of the replicated study no significant difference was found in accuracy between estimates derived from company-specific data and estimates derived from multi-organizational data.\"],\n",
       "   '2000']],\n",
       " [[['Action Language: a specification language for model checking reactive systems.',\n",
       "    'We present a specification language called Action Language for model checking software specifications. Action Language forms an interface between transition system models that a model checker generates and high level specification languages such as Statecharts, RSML and SCR&mdash;similar to an assembly language between a microprocessor and a programming language. We show that Action Language translations of Statecharts and SCR specifications are compact and they preserve the structure of the original specification. Action Language allows specification of both synchronous and asynchronous systems. It also supports modular specifications to enable compositional model checking.'],\n",
       "   '2000']],\n",
       " [[['Bandera: extracting finite-state models from Java source code.',\n",
       "    'Finite-state verification techniques, such as model checking, have shown promise as a cost-effective means for finding defects in hardware designs. To date, the application of these techniques to software has been hindered by several obstacles. Chief among these is the problem of constructing a finite-state model that approximates the executable behavior of the software system of interest. Current best-practice involves hand-construction of models which is expensive (prohibitive for all but the smallest systems), prone to errors (which can result in misleading verification results), and difficult to optimize (which is necessary to combat the exponential complexity of verification algorithms).In this paper, we describe an integrated collection of program analysis and transformation components, called Bandera, that enables the automatic extraction of safe, compact finite-state models from program source code. Bandera takes as input Java source code and generates a program model in the input language of one of several existing verification tools; Bandera also maps verifier outputs back to the original source code. We discuss the major components of Bandera and give an overview of how it can be used to model check correctness properties of Java programs.'],\n",
       "   '2000']],\n",
       " [[['Three approximation techniques for ASTRAL symbolic model checking of infinite state real-time systems.',\n",
       "    'ASTRAL is a high-level formal specification language for real-time systems. It has structuring mechanisms that allow one to build modularized specifications of complex real-time systems with layering. Based upon the ASTRAL symbolic model checler reported in [13], three approximation techniques to speed-up the model checking process for use in debugging a specification are presented. The techniques are random walk, partial image and dynamic environment generation. Ten mutation tests on a railroad crossing benchmark are used to compare the performance of the techniques applied separately and in combination. The test results are presented and analyzed.'],\n",
       "   '2000']],\n",
       " [[['A logical framework for design composition.',\n",
       "    'The design of a large component-based software system typically involves the composition of different components. The lack of rigorous reasoning about the correctness of composition is an important barrier towards the promise of &ldquo;plug and play&rdquo;. In this paper, we describe a rigorous logic framework to reason about component compositions. We focus our analysis on design components, such as design patterns, which have been used by a large number of applications. We also propose methods to verify structural and behavioral composition correctness.'],\n",
       "   '2000']],\n",
       " [[['Implementing incremental code migration with XML.',\n",
       "    'We demonstrate how XML and related technologies can be used for code mobility at any granularity, thus overcoming the restrictions of existing approaches. By not fixing a particular granularity for mobile code, we enable complete programs as well as individual lines of code to be sent across the network. We define the concept of incremental code mobility as the ability to migrate and add, remove, or replace code fragments (i.e., increments) in a remote program. The combination of fine-grained and incremental migration achieves a previously unavailable degree of flexibility. We examine the application of incremental and fine-grained code migration to a variety of domains, including user interface management, application management on mobile thin clients, for example PDAs, and management of distributed documents.'],\n",
       "   '2000']],\n",
       " [[['Principled design of the modern Web architecture.',\n",
       "    'The World Wide Web has succeeded in large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia application. The modern Web architecture emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. In this article we introduce the Representational State Transfer (REST) architectural style, developed as an abstract model of the Web architecture and used to guide our redesign and definition of the Hypertext Transfer Protocol and Uniform Resource Identifiers. We describe the software engineering principles guiding REST and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. We then compare the abstract model to the currently deployed Web architecture in order to elicit mismatches between the existing protocols and the applications they are intended to support.'],\n",
       "   '2000']],\n",
       " [[['An approach to architectural analysis of product lines.',\n",
       "    \"This paper addresses the issue of how to perform architectural analysis on an existing product line architecture. The con tribution of the paper is to identify and demonstrate a repeatable product line architecture analysis process. The approach defines a &ldquo;good&rdquo; product line architecture in terms of those quality attributes required by the particular product line under development. It then analyzes the architecture against these criteria by both manual and tool-supported methods. The phased approach described in this paper provides a structured analysis of an existing product line architecture using (1) formal specification of the high-level architecture, (2) manual analysis of scenarios to exercise the architecture's support for required variabilities, and (3) model checking of critical behaviors at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain and evaluate the approach.\"],\n",
       "   '2000']],\n",
       " [[['Component design of retargetable program analysis tools that reuse intermediate representations.',\n",
       "    \"Interactive program analysis tools are often tailored to one particular representation of programs, making adaptation to a new language costly. One way to ease adaptability is to introduce an intermediate abstraction&mdash;an adaptation layer&mdash;between an existing language representation and the program analysis tool. This adaptation layer translates the tool's queries into queries on the particular representation.Our experiments with this approach on the StarTool program analysis tool resulted in low-cost retargets for C, Tcl/Tk, and Ada. Required adjustments to the approach, however, led to insights for improving a client's retargetability. First, retargeting was eased by having our tool import a tool-centric (i.e., client-centric) interface rather than a general-purpose, language-neutral representation interface. Second, our adaptation layer exports two interfaces, a representation interface supporting queries on the represented program and a language interface that the client queries to configure itself suitably for the given language. Straightforward object-oriented extensions enhance reuse and ease the development of multi-language tools.\"],\n",
       "   '2000']],\n",
       " [[['An inheritance-based technique for building simulation proofs incrementally.',\n",
       "    \"This paper presents a formal technique for incremental construction of system specifications, algorithm descriptions, and simulation proofs showing that algorithms meet their specifications.The technique for building specifications and algorithms incrementally allows a child specification or algorithm to inherit from its parent by two forms of incremental modification: (a) signature extension, where new actions are added to the parent, and (b) specialization (subtyping), where the child's behavior is a specialization (restriction) of the parent's behavior. The combination of signature extension and specialization provides a powerful and expressive incremental modification mechanism for introducing new types of behavior without overriding behavior of the parent; this mechanism corresponds to the subclassing for extension form of inheritance.In the case when incremental modifications are applied to both a parent specification S and a parent algorithm A, the technique allows a simulation proof showing that the child algorithm A&prime; implements the child specification S&prime; to be constructed incrementally by extending a simulation proof that algorithm A implements specification S. The new proof involves reasoning about the modifications only, without repeating the reasoning done in the original simulation proof.The paper presents the technique mathematically, in terms of automata. The technique has been used to model and verify a complex middleware system; the methodology and results of that experiment are summarized in this paper.\"],\n",
       "   '2000']],\n",
       " [[['An empirical study of regression test application frequency.',\n",
       "    'Regression testing is an expensive maintenance process used to revalidate modified software. Regression test selection (RTS) techniques try to lower the cost of regression testing by selecting and running a subset of the existing test cases. Many such techniques have been proposed and initial studies show that they can produce savings. We believe, however, that issues such as the frequency with which testing is done have a strong effect on the behavior of these techniques. Therefore, we conducted an experiment to assess the effects of test application frequency on the costs and benefits of regression test selection techniques. Our results expose essential tradeoffs that should be considered when using these techniques over a series of software releases.'],\n",
       "   '2000']],\n",
       " [[['Requirements engineering in the year 00: a research perspective.',\n",
       "    'Requirements engineering (RE) is concerned with the identification of the goals to be achieved by the envisioned system, the operationalization of such goals into services and constraints, and the assignment of responsibilities for the resulting requirements to agents such as humans, devices, and software. The processes involved in RE include domain analysis, elicitation, specification, assessment, negotiation, documentation, and evolution. Getting high-quality requirements is difficult and critical. Recent surveys have confirmed the growing recognition of RE as an area of utmost importance in software engineering research and practice.The paper presents a brief history of the main concepts and techniques developed to date to support the RE task, with a special focus on modeling as a common denominator to all RE processes. The initial description of a complex safety-critical system is used to illustrate a number of current research trends in RE-specific areas such as goal-oriented requirements elaboration, conflict management, and the handling of abnormal agent behaviors. Opportunities for goal-based architecture derivation are also discussed together with research directions to let the field move towards more disciplined habits.'],\n",
       "   '2000']],\n",
       " [[['Graphical animation of behavior models.',\n",
       "    'Graphical animation is a way of visualizing the behavior of design models. This visualization is of use in validating a design model against informally specified requirements and in interpreting the meaning and significance of analysis results in relation to the problem domain. In this paper we describe how behavior models specified by Labeled Transition Systems (LTS) can drive graphical animations. The semantic framework for the approach is based on Timed Automata. Animations are described by an XML document that is used to generate a set of JavaBeans. The elaborated JavaBeans perform the animation actions as directed by the LTS model.'],\n",
       "   '2000']],\n",
       " [[['Light-weight context recovery for efficient and accurate program analyses.',\n",
       "    'To compute accurate information efficiently for programs that use pointer variables, a program analysis must account for the fact that a procedure may access different sets of memory locations when the procedure is invoked under different callsites. This paper presents light-weight context recovery, a technique that can efficiently determine whether a memory location is accessed by a procedure under a specific callsite. The paper also presents a technique that uses this information to improve the precision and efficiency of program analyses. Our empirical studies show that (1) light-weight context recovery can be quite precise in identifying the memory locations accessed by a procedure under a specific call-site and (2) distinguishing memory locations accessed by a procedure under different callsites can significantly improve the precision and the efficiency of program analyses on programs that use pointer variables.'],\n",
       "   '2000']],\n",
       " [[['A study on exception detecton and handling using aspect-oriented programming.',\n",
       "    \"Aspect-Oriented Programming (AOP) is intended to ease situations that involve many kinds of code tangling. This paper reports on a study to investigate AOP's ability to ease tangling related to exception detection and handling. We took an existing framework written in Java&trade;, the JWAM framework, and partially reengineered its exception detection and handling aspects using AspectJ&trade;, an aspect-oriented programming extension to Java.We found that AspectJ supported implementations that drastically reduced the portion of the code related to exception detection and handling. In one scenario, we were able to reduce that code by a factor of 4. We also found that, with respect to the original implementation in plain Java, AspectJ provided better support for different configurations of exceptional behaviors, more tolerance for changes in the specifications of exceptional behaviors, better support for incremental development, better reuse, automatic enforcement of contracts in applications that use the framework, and cleaner program texts. We also found some weaknesses of AspectJ that should be addressed in the future.\"],\n",
       "   '2000']],\n",
       " [[['Towards a taxonomy of software connectors.',\n",
       "    'Software systems of today are frequently composed from prefabricated, heterogeneous components that provide complex functionality and engage in complex interactions. Existing research on component-based development has mostly focused on component structure, interfaces, and functionality. Recently, software architecture has emerged as an area that also places significant importance on component interactions, embodied in the notion of software connectors. However, the current level of understanding and support for connectors has been insufficient. This has resulted in their inconsistent treatment and a notable lack of understanding of what the fundamental building blocks of software interaction are and how they can be composed into more complex interactions. This paper attempts to address this problem. It presents a comprehensive classification framework and taxonomy of software connectors. The taxonomy is obtained through an extensive analysis of existing component interactions. The taxonomy is used both to understand existing software connectors and to suggest new, unprecedented connectors. We demonstrate the use of the taxonomy on the architecture of a large, existing system.'],\n",
       "   '2000']],\n",
       " [[['An integrated cost model for software reuse.',\n",
       "    'Several cost models have been proposed in the past for estimating, predicting, and analyzing the costs of software reuse. In this paper we analyze existing models, explain their variance, and propose a tool-supported comprehensive model that encompasses most of the existing models.'],\n",
       "   '2000']],\n",
       " [[['A formal approach for designing CORBA based applications.',\n",
       "    'The design of distributed applications in a CORBA based environment can be carried out by means of an incremental approach, which starts from the specification and leads to the high level architectural design. This is done by introducing in the specification all typical elements of CORBA and by providing a methodological support to the designers. The paper discusses a methodology to transform a formal specification written in TRIO into a high level design document written using an extension of TRIO named TC. The TC language is suited to formally describe the high level architecture of a CORBA based application. The methodology and the associated language are presented by means of an example involving a real Supervision and Control System.'],\n",
       "   '2000']],\n",
       " [[['Verification of time partitioning in the DEOS scheduler kernel.',\n",
       "    'This paper describes an experiment to use the Spin model checking system to support automated verification of time partitioning in the Honeywell DEOS real-time scheduling kernel. The goal of the experiment was to investigate whether model checking could be used to find a subtle implementation error that was originally discovered and fixed during the standard formal review process. To conduct the experiment, a core slice of the DEOS scheduling kernel was first translated without abstraction from C++ into Promela (the input language for Spin). We constructed an abstract &ldquo;test-driver&rdquo; environment and carefully introduced several abstractions into the system to support verification. Several experiments were run to attempt to verify that the system implementation adhered to the critical time partitioning requirements. During these experiments, the known error was rediscovered in the time partitioning implementation. We believe this case study provides several insights into how to develop cost-effective methods and tools to support the software design and implementation review process.'],\n",
       "   '2000']],\n",
       " [[['Software evolution in componentware using requirements/assurances contracts.',\n",
       "    'In practice, pure top-down and refinement-based development processes are not sufficient. Usually, an iterative and incremental approach is applied instead. Existing methodologies, however, do not support such evolutionary development processes very well. In this paper, we present the basic concepts of an overall methodology based on component ware and software evolution. The foundation of our methodology is a novel, well-founded model for component-based systems. This model is sufficiently powerful to handle the fundamental structural and behavioral aspects of component ware and object-orientation. Based on the model, we are able to provide a clear definition of a software evolution step.During development, each evolution step implies changes of an appropriate set of development documents. In order to model and track the dependencies between these documents, we introduce the concept of Requirements/Assurances Contracts. These contracts can be rechecked whenever the specification of a component evolves, enabling us to determine the impacts of the respective evolution step. Based on the proposed approach, developers are able to track and manage the software evolution process and to recognize and avoid failures due to software evolution. A short example shows the usefulness of the presented concepts and introduces a practical description technique for Requirements/Assurances Contracts.'],\n",
       "   '2000']],\n",
       " [[['WYSIWYT testing in the spreadsheet paradigm: an empirical evaluation.',\n",
       "    \"Is it possible to achieve some of the benefits of formal testing within the informal programming conventions of the spreadsheet paradigm? We have been working on an approach that attempts to do so via the development of a testing methodology for this paradigm. Our &ldquo;What You See Is What You Test&rdquo; (WYSIWYT) methodology supplements the convention by which spreadsheets provide automatic immediate visual feedback about values by providing automatic immediate visual feedback about &ldquo;testedness&rdquo;. In previous work we described this methodology; in this paper, we present empirical data about the methodology's effectiveness. Our results show that the use of the methodology was associated with significant improvement in testing effectiveness and efficiency even with no training on the theory of testing or test adequacy that the model implements. These results may be due at least in part to the fact that use of the methodology was associated with a significant reduction in overconfidence.\"],\n",
       "   '2000']],\n",
       " [[['Supporting diversity with component frameworks as architectural elements.',\n",
       "    'In this paper, we describe our experience with component frameworks within a family architecture for a medical imaging product family. The component frameworks are handled as an integral part of the architectural approach and are an important means to support diversity in the functionality provided by the individual family members.This paper focuses on a particular kind of component framework that has been applied throughout the medical imaging product family. This kind of framework is useful when the various family members are based on the same concepts and the diversity is formed by the differences in the specific instances of these concepts that are present in the family members. These component frameworks have a number of similarities, allowing a standardised approach to their development. They support the division of the system into a generic architectural skeleton, which can be extended with plug-ins to realise specific family members, each with their own set of features.'],\n",
       "   '2000']],\n",
       " [[['Achieving Extensibility Through Product-Lines and Domain-Specific Languages: A Case Study.',\n",
       "    'This is a case study in the use of product-line architectures (PLAs) and domain-specific languages (DSLs) to design an extensible command-and-control simulator for Army fire support. The reusable components of our PLA are layers or \"aspects\" whose addition or removal simultaneously impacts the source code of multiple objects in multiple, distributed programs. The complexity of our component specifications is substantially reduced by using a DSL for defining and refining state machines, abstractions that are fundamental to simulators. We present preliminary results that show how our PLA and DSL synergistically produce a more flexible way of implementing state-machine-based simulators than is possible with a pure Java implementation.'],\n",
       "   '2000']],\n",
       " [[['Non-determinism Analysis in a Parallel-Functional Language.',\n",
       "    'The parallel-functional language Eden has a non-deterministic construct, the process abstraction merge, which interleaves a set of input lists to produce a single non-deterministic list. Its non-deterministic behaviour is a consequence of its reactivity: it immediately copies to the output list any value appearing at any of the input lists. This feature is essential in reactive systems and very useful in some deterministic parallel algorithms. The presence of non-determinism creates some problems such that some internal transformations in the compiler must be disallowed. The paper describes several non-determinism analyses developed for Eden aimed at detecting the parts of the program that, even in the presence of a process merge, still exhibit a deterministic behaviour. A polynomial cost algorithm which annotates Eden expressions is described in detail. A denotational semantics is described for Eden and the correctness of all the analyses is proved with respect to this semantics.'],\n",
       "   '2000']],\n",
       " [[['Broadcast Disks with Polynomial Cost Functions.',\n",
       "    'In broadcast disks systems, information is broadcasted in a shared medium. When a client needs an item from the disk, it waits until that item is broadcasted. Broadcast disks systems are particularly attractive in settings where the potential customers have a highly-asymmetric communication capabilities, i.e., receiving is significantly cheaper than transmitting. This is the case with satellite networks, mobile hosts in wireless networks, and Teletext system.The fundamental algorithmic problem for such systems is to determine the broadcast schedule based on the demand probability of items, and the cost incurred to the system by clients waiting. The goal is to minimize the mean access cost of a random client. Typically, it was assumed that the access cost is proportional to the waiting time. In this paper, we ask what are the best broadcast schedules for access costs which are arbitrary polynomials in the waiting time. These may serve as reasonable representations of reality in many cases, where the \"patience\" of a client is not necessarily proportional to its waiting time.We present an asymptotically optimal algorithm for a fractional model, where the bandwidth may be divided to allow for fractional concurrent broadcasting. This algorithm, besides being justified in its own right, also serves as a lower bound against which we test known discrete algorithms. We show that the Greedy algorithm has the best performance in most cases. Then we show that the performance of other algorithms deteriorate exponentially with the degree of the cost polynomial and approaches the fractional solution for sub-linear cost. Finally, we study the quality of approximating the greedy schedule by a finite schedule.'],\n",
       "   '2000']],\n",
       " [[['End-to-End Congestion Control Schemes: Utility Functions, Random Losses and ECN Marks.',\n",
       "    'We present a framework for designing end-to-end congestion control schemes in a network where each user may have a different utility function and may experience noncongestion-related losses. We first show that there exists an additive-increase-multiplicative-decrease scheme using only end-to-end measurable losses such that a socially optimal solution can be reached. We incorporate round-trip delay in this model, and show that one can generalize observations regarding TCP-type congestion avoidance to more general window flow control schemes. We then consider explicit congestion notification (ECN) as an alternate mechanism (instead of losses) for signaling congestion and show that ECN marking levels can be designed to nearly eliminate losses in the network by choosing the marking level independently for each node in the network. While the ECN marking level at each node may depend on the number of flows through the node, the appropriate marking level can be estimated using only aggregate flow measurements, i.e., per-flow measurements are not required.'],\n",
       "   '2000']],\n",
       " [[['Alternate Path Routing for Multicast.',\n",
       "    \"Current network-layer multicast routing protocols build multicast trees based only on hop count and policy. If a tree cannot meet application requirements, the receivers have no alternative. In this paper, we propose a general and modular architecture that integrates alternate path routing with the network's multicast services. This enables individual multicast receivers to reroute a multicast tree according to their needs, subject to policy restrictions. Our design focuses on the two primary components of this architecture--a loop-free path installation protocol and a scalable, distributed path computation algorithm. Based on a simulation study, we demonstrate that using alternate path routing enables receivers to find acceptable paths nearly as well as a link-state protocol, with much lower overhead. We also show that our approach scales to large networks and that performance improves as a multicast group grows in size.\"],\n",
       "   '2000']],\n",
       " [[['Redefining the Focus and Context of Focus+Context Visualizations.',\n",
       "    'The increasing diversity of computers, especially among small mobile devices such as mobile phones and PDAs, raise new questions about information visualization techniques developed for the desktop computer. Using a series of examples ranging from applications for ordinary desktop displays to web-browsers and other applications for PDAs, we describe how a focus+context technique, Flip Zooming, is changed due to the situation it is used in. Based on these examples, we discuss how the use of ¿focus¿ and ¿context¿ in focus+context techniques change in order to fit new areas of use for information visualization.'],\n",
       "   '2000']],\n",
       " [[['ThemeRiver: Visualizing Theme Changes over Time.',\n",
       "    'ThemeRiver(tm) is a prototype system that visualizes thematic variations over time within a large collection of documents. The ¿river¿ flows from left to right through time, changing width to depict changes in thematic strength of temporally associated documents. Colored ¿currents¿ flowing within the river narrow or widen to indicate decreases or increases in the strength of an individual topic or a group of topics in the associated documents. The river is shown within the context of a timeline and a corresponding textual presentation of external events.'],\n",
       "   '2000']],\n",
       " [[['Density Functions for Visual Attributes and Effective Partitioning in Graph Visualization.',\n",
       "    'Two tasks in Graph Visualization require partitioning: the assignment of visual attributes and divisive clustering. Often, we would like to assign a color or other visual attributes to a node or edge that indicates an associated value. In an application involving divisive clustering, we would like to partition the graph into subsets of graph elements based on metric values in such a way that all subsets are evenly populated. Assuming a uniform distribution of metric values during either partitioning or coloring can have undesired effects such as empty clusters or only one level of emphasis for the entire graph. Probability density functions derived from statistics about a metric can help systems succeed at these tasks.'],\n",
       "   '2000']],\n",
       " [[['A Scalable Framework for Information Visualization.',\n",
       "    'This paper describes major concepts of a scalable information visualization framework. We assume that the exploration of heterogeneous information spaces at arbitrary levels of detail requires a suitable preprocessing of information quantities, the combination of different graphical interfaces and the illustration of the frame of reference of given information sets. The innovative features of our system include dynamic hierarchy computation and user controlled refinement of those hierarchies for preprocessing unstructured information spaces, a new Focus+Context technique for visualizing complex hierarchy graphs, a new paradigm for visualizing information structures within their frame of reference and a new graphical interface that utilizes textual similarities to arrange objects of high dimensional information space in 3-dimensional visualization space.'],\n",
       "   '2000']],\n",
       " [[['From Metaphor to Method: Cartographic Perspectives on Information Visualization.',\n",
       "    'By virtue of their spatio-cognitive abilities, humans are able to navigate through geographic space as well as meaningfully communicate geographic information represented in cartographic form. The current dominance of spatial metaphors in information visualization research is the result of the realization that those cognitive skills also have value in the exploration and analysis of non-geographic information. While mapping or landscape metaphors are routinely used in this field, there is a noticeable lack of consideration for existing cartographic expertise. This is especially apparent whenever problematic issues are encountered, such as graphic complexity or feature labeling. There are a number of areas in which a cartographic outlook could provide a valuable perspective. This paper discusses how geographic and cartographic notions may influence the design of visualizations for textual information spaces. Map projections, generalization, feature labeling and map design issues are discussed.'],\n",
       "   '2000']],\n",
       " [[['Visualizing Sequential Patterns for Text Mining.',\n",
       "    'A sequential pattern in data mining is a finite series of elements such as A .B .C .D where A, B, C, and D are elements of the same domain. The mining of sequential patterns is designed to find patterns of discrete events that frequently happen in the same arrangement along a timeline. Like association and clustering, the mining of sequential patterns is among the most popular knowledge discovery techniques that apply statistical measures to extract useful information from large datasets. As our computers become more powerful, we are able to mine bigger datasets and obtain hundreds of thousands of sequential patterns in full detail. With this vast amount of data, we argue that neither data mining nor visualization by itself can manage the information and reflect the knowledge effectively. Subsequently, we apply visualization to augment data mining in a study of sequential patterns in large text corpora. The result shows that we can learn increasingly quickly in an integrated visual data-mining environment.'],\n",
       "   '2000']],\n",
       " [[['Optimal All-to-All Personalized Exchange in a Class of Optical Multistage Networks.',\n",
       "    'All-to-all personalized exchange is one of the most dense collective communication patterns and it occurs in many important parallel computing/networking applications. In this paper, we look into the issue of realizing an all-to-all personalized exchange in a class of optical multistage networks. Advances in electrooptic technologies have made optical communication a promising networking choice to meet the increasing demands for high channel bandwidth and low communication latency of high-performance computing/communication applications. Although optical multistage networks hold great promise and have demonstrated advantages over their electronic counterpart, they also hold their own challenges. Due to the unique properties of optics, crosstalk in optical switches should be avoided to make them work properly. In this paper, we will provide a systematic scheme for realizing an all-to-all personalized exchange in a class of unique-path optical multistage networks crosstalk-free. The basic idea of realizing an all-to-all personalized exchange in such a multistage network is to transform it to multiple semipermutations and ensure that each of them can be realized crosstalk-free in a single pass. As can be seen, the all-to-all personalized exchange algorithm we propose has $O(n)$ time complexity for $n$ processors, which is optimal for an all-to-all personalized exchange. The optimal time complexity combined with the property of a single input/output port per processor suggests that a multistage network could be a better choice for implementing an all-to-all personalized exchange due to its shorter communication latency and better scalability.'],\n",
       "   '2000']],\n",
       " [[['On Efficient Fixed Parameter Algorithms for WEIGHTED VERTEX COVER.',\n",
       "    'We investigate the fixed-parameter complexity of WEIGHTED VERTEX COVER. Given a graph G = (V, E), a weight function ω: V → R+, and k ∈ R+, WEIGHTED VERTEX COVER (WVC for short) asks for a vertex subset C ⊆ V of total weight at most k such that every edge of G has at least one endpoint in C. WVC and its natural variants are NP-complete. We observe that, when restricting the range of ω to positive integers, the so-called INTEGER-WVC can be solved as fast as unweighted VERTEX COVER. Our main result is that if the range of ω is restricted to positive reals ≥ 1, then so-called REAL-WVC can be solved in time O(1.3954k + k|V|). By way of contrast, unless P = NP, the problem is not fixed-parameter tractable if arbitrary weights > 0 are allowed. Using dynamic programming, at the expense of exponential memory use, we can improve the running time of REALWVC to O(1.3788k + k|V|). The same technique applied to a known algorithm yields the so far fastest algorithm for unweighted VERTEX COVER, running in time O(1.2832kk + k|V|).'],\n",
       "   '2000']],\n",
       " [[['Clock rate versus IPC: the end of the road for conventional microarchitectures.',\n",
       "    'The doubling of microprocessor performance every three years has been the result of two factors: more transistors per chip and superlinear scali ng of the processor clock with technology generation. Our results show that, due to both diminishing improvements in clock rates and poor wire scaling as semiconductor devices shrink, the achievable performance growth of conventional microarchitectures will slow substantially. In this paper, we describe technology-driven models for wire capacitance, wire delay, and microarchitectural component delay. Using the results of these models, we measure the simulated performance&mdash;estimating both clock rate and IPC &mdash;of an aggressive out-of-order microarchitecture as it is scaled from a 250nm technology to a 35nm technology. We perform this analysis for three clock scaling targets and two microarchitecture scaling strategies: pipeline scaling and capacity scaling. We find that no scaling strategy permits annual performance improvements of better than 12.5%, which is far worse than the annual 50-60% to which we have grown accustomed.'],\n",
       "   '2000']],\n",
       " [[['Piranha: a scalable architecture based on single-chip multiprocessing.',\n",
       "    'The microprocessor industry is currently struggling with higher development costs and longer design times that arise from exceedingly complex processors that are pushing the limits of instruction-level parallelism. Meanwhile, such designs are especially ill suited for important commercial applications, such as on-line transaction processing (OLTP), which suffer from large memory stall times and exhibit little instruction-level parallelism. Given that commercial applications constitute by far the most important market for high-performance servers, the above trends emphasize the need to consider alternative processor designs that specifically target such workloads. The abundance of explicit thread-level parallelism in commercial workloads, along with advances in semiconductor integration density, identify chip multiprocessing (CMP) as potentially the most promising approach for designing processors targeted at commercial servers. This paper describes the Piranha system, a research prototype being developed at Compaq that aggressively exploits chip multi-processing by integrating eight simple Alpha processor cores along with a two-level cache hierarchy onto a single chip. Piranha also integrates further on-chip functionality to allow for scalable multiprocessor configurations to be built in a glueless and modular fashion. The use of simple processor cores combined with an industry-standard ASIC design methodology allow us to complete our prototype within a short time-frame, with a team size and investment that are an order of magnitude smaller than that of a commercial microprocessor. Our detailed simulation results show that while each Piranha processor core is substantially slower than an aggressive next-generation processor, the integration of eight cores onto a single chip allows Piranha to outperform next-generation processors by up to 2.9 times (on a per chip basis) on important workloads such as OLTP. This performance advantage can approach a factor of five by using full-custom instead of ASIC logic. In addition to exploiting chip multiprocessing, the Piranha prototype incorporates several other unique design choices including a shared second-level cache with no inclusion, a highly optimized cache coherence protocol, and a novel I/O architecture.'],\n",
       "   '2000']],\n",
       " [[['Early load address resolution via register tracking.',\n",
       "    \"Higher microprocessor frequencies accentuate the performance cost of memory accesses. This is especially noticeable in the Intel's IA32 architecture where lack of registers results in increased number of memory accesses. This paper presents novel, non-speculative technique that partially hides the increasing load-to-use latency, by allowing the early issue of load instructions. Early load address resolution relies on register tracking to safely compute the addresses of memory references in the front-end part of the processor pipeline. Register tracking enables decode-time computation of register values by tracking simple operations of the form reg&plusmn;immediate. Register tracking may be performed in any pipeline stage following instruction decode and prior to execution. Several tracking schemes are proposed in this paper: Stack pointer tracking allows safe early resolution of stack references by keeping track of the value of the ESP register (the stack pointer). About 25% of all loads are stack loads and 95% of these loads may be resolved in the front-end. Absolute address tracking allows the early resolution of constant-address loads. Displacement-based tracking tackles all loads with addresses of the form reg&plusmn;immediate by tracking the values of all general-purpose registers. This class corresponds to 82% of all loads, and about 65% of these loads can be safely resolved in the front-end pipeline. The paper describes the tracking schemes, analyzes their performance potential in a deeply pipelined processor and discusses the integration of tracking with memory disambiguation.\"],\n",
       "   '2000']],\n",
       " [[['Architectural support for scalable speculative parallelization in shared-memory multiprocessors.',\n",
       "    'Speculative parallelization aggressively executes in parallel codes that cannot be fully parallelized by the compiler. Past proposals of hardware schemes have mostly focused on single-chip multiprocessors (CMPs), whose effectiveness is necessarily limited by their small size. Very few schemes have attempted this technique in the context of scalable shared-memory systems. In this paper, we present and evaluate a new hardware scheme for scalable speculative parallelization. This design needs relatively simple hardware and is efficiently integrated into a cache-coherent NUMA system. We have designed the scheme in a hierarchical manner that largely abstracts away the internals of the node. We effectively utilize a speculative CMP as the building block for our scheme. Simulations show that the architecture proposed delivers good speedups at a modest hardware cost. For a set of important non-analyzable scientific loops, we report average speedups of 4.2 for 16 processors. We show that support for per-word speculative state is required by our applications, or else the performance suffers greatly.'],\n",
       "   '2000']],\n",
       " [[['Wattch: a framework for architectural-level power analysis and optimizations.',\n",
       "    \"Power dissipation and thermal issues are increasingly significant in modern processors. As a result, it is crucial that power/performance tradeoffs be made more visible to chip architects and even compiler writers, in addition to circuit designers. Most existing power analysis tools achieve high accuracy by calculating power estimates for designs only after layout or floorplanning are complete. In addition to being available only late in the design process, such tools are often quite slow, which compounds the difficulty of running them for a large space of design possibilities. This paper presents Wattch, a framework for analyzing and optimizing microprocessor power dissipation at the architecture-level. Wattch is 1000X or more faster than existing layout-level power tools, and yet maintains accuracy within 10% of their estimates as verified using industry tools on leading-edge designs. This paper presents several validations of Wattch's accuracy. In addition, we present three examples that demonstrate how architects or compiler writers might use Wattch to evaluate power consumption in their design process. We see Wattch as a complement to existing lower-level tools; it allows architects to explore and cull the design space early on, using faster, higher-level tools. It also opens up the field of power-efficient computing to a wider range of researchers by providing a power evaluation methodology within the portable and familiar SimpleScalar framework.\"],\n",
       "   '2000']],\n",
       " [[['Multiple-banked register file architectures.',\n",
       "    'The register file access time is one of the critical delays in current superscalar processors. Its impact on processor performance is likely to increase in future processor generations, as they are expected to increase the issue width (which implies more register ports) and the size of the instruction window (which implies more registers), and to use some kind of multithreading. Under this scenario, the register file access time could be a dominant delay and a pipelined implementation would be desirable to allow for high clock rates. However, a multi-stage register file has severe implications for processor performance (e.g. higher branch misprediction penalty) and complexity (more levels of bypass logic). To tackle these two problems, in this paper we propose a register file architecture composed of multiple banks. In particular we focus on a multi-level organization of the register file, which provides low latency and simple bypass logic. We propose several caching policies and prefetching strategies and demonstrate the potential of this multiple-banked organization. For instance, we show that a two-level organization degrades IPC by 10% and 2% with respect to a non-pipelined single-banked register file, for SpecInt95 and SpecFP95 respectively, but it increases performance by 87% and 92% when the register file access time is factored in.'],\n",
       "   '2000']],\n",
       " [[['Instruction path coprocessors.',\n",
       "    \"This paper presents the concept of an Instruction Path Coprocessor (I-COP), which is a programmable on-chip coprocessor, with its own mini-instruction set, that operates on the core processor's instructions to transform them into an internal format that can be more efficiently executed. It is located off the critical path of the core processor to ensure that it does not negatively impact the core processor's cycle time or pipeline depth. An I-COP is highly versatile and can be used to implement different types of instruction transformations to enhance the IPC of the core processor. We study four potential applications of the I-COP to demonstrate the feasibility of this concept and investigate the design issues of such a coprocessor. A prototype instruction set for the I-COP is presented along with an implementation framework that facilitates achieving high I-COP performance. Initial results indicate that the I-COP is able to efficiently implement the trace cache fill unit as well as the register move, stride data prefetching and linked data structure prefetching trace optimizations.\"],\n",
       "   '2000']],\n",
       " [[['A fully associative software-managed cache design.',\n",
       "    \"As DRAM access latencies approach a thousand instruction-execution times and on-chip caches grow to multiple megabytes, it is not clear that conventional cache structures continue to be appropriate. Two key features&mdash;full associativity and software management&mdash;have been used successfully in the virtual-memory domain to cope with disk access latencies. Future systems will need to employ similar techniques to deal with DRAM latencies. This paper presents a practical, fully associative, software-managed secondary cache system that provides performance competitive with or superior to traditional caches without OS or application involvement. We see this structure as the first step toward OS- and application-aware management of large on-chip caches. This paper has two primary contributions: a practical design for a fully associative memory structure, the indirect index cache (IIC), and a novel replacement algorithm, generational replacement, that is specifically designed to work with the IIC. We analyze the behavior of an IIC with generational replacement as a drop-in, transparent substitute for a conventional secondary cache. We achieve miss rate reductions from 8% to 85% relative to a 4-way associative LRU organization, matching or beating a (practically infeasible) fully associative true LRU cache. Incorporating these miss rates into a rudimentary timing model indicates that the IIC/generational replacement cache could be competitive with a conventional cache at today's DRAM latencies, and will outperform a conventional cache as these CPU-relative latencies grow.\"],\n",
       "   '2000']],\n",
       " [[['Trace preconstruction.',\n",
       "    \"Trace caches enable high bandwidth, low latency instruction supply, but have a high miss penalty and relatively large working sets. Consequently, their performance may suffer due to capacity and compulsory misses. Trace preconstruction augments a trace cache by performing a function analogous to prefetching. The trace preconstruction mechanism observes the processor's instruction dispatch stream to detect opportunities for jumping ahead of the processor. After doing so, the preconstruction mechanism fetches static instructions from the predicted future region of the program, and constructs a set of traces in advance of when they are needed. Trace preconstruction can significantly increase both the performance of the trace cache and the robustness of the trace cache to varying workloads. All but one of the SPECint95 benchmarks see a notable reduction in trace cache miss rates from preconstruction. The three benchmarks that have the largest working set (gee, go and vortex) see a 30% to 80% reduction in trace cache misses. We also consider the integration of preconstruction with another trace-specific mechanism (preprocessing) to produce a high performance frontend. When combined, preconstruction and trace preprocessing produce an average speedup of 14% for the SPECint95 benchmarks.\"],\n",
       "   '2000']],\n",
       " [[['Selective, accurate, and timely self-invalidation using last-touch prediction.',\n",
       "    \"Communication in cache-coherent distributed shared memory (DSM) often requires invalidating (or writing back) cached copies of a memory block, incurring high overheads. This paper proposes Last-Touch Predictors (LTPs) that learn and predict the &ldquo;last touch&rdquo; to a memory block by one processor before the block is accessed and subsequently invalidated by another. By predicting a last-touch and (self-)invalidating the block in advance, an LTP hides the invalidation time, significantly reducing the coherence overhead. The key behind accurate last-touch prediction is trace-based correlation, associating a last-touch with the sequence of instructions (i.e., a trace) touching the block from a coherence miss until the block is invalidated. Correlating instructions enables an LTP to identify a last-touch to a memory block uniquely throughout an application's execution. In this paper, we use results from running shared-memory applications on a simulated DSM to evaluate LTPs. The results indicate that: (1) our base case LTP design, maintaining trace signatures on a per-block basis, substantially improves prediction accuracy over previous self-invalidation schemes to an average of 79%; (2) our alternative LTP design, maintaining a global trace signature table, reduces storage overhead but only achieves an average accuracy of 58%; (3) last-touch prediction based on a single instruction only achieves an average accuracy of 41% due to instruction reuse within and across computation; and (4) LTP enables selective, accurate, and timely self-invalidation in DSM, speeding up program execution on average by 11%.\"],\n",
       "   '2000']],\n",
       " [[['Smart Memories: a modular reconfigurable architecture.',\n",
       "    'Trends in VLSI technology scaling demand that future computing devices be narrowly focused to achieve high performance and high efficiency, yet also target the high volumes and low costs of widely applicable general purpose designs. To address these conflicting requirements, we propose a modular reconfigurable architecture called Smart Memories, targeted at computing needs in the 0.1&mgr; technology generation. A Smart Memories chip is made up of many processing tiles, each containing local memory, local interconnect, and a processor core. For efficient computation under a wide class of possible applications, the memories, the wires, and the computational model can all be altered to match the applications. To show the applicability of this design, two very different machines at opposite ends of the architectural spectrum, the Imagine stream processor and the Hydra speculative multiprocessor, are mapped onto the Smart Memories computing substrate. Simulations of the mappings show that the Smart Memories architecture can successfully map these architectures with only modest performance degradation.'],\n",
       "   '2000']],\n",
       " [[['On the value locality of store instructions.',\n",
       "    'Value locality, a recently discovered program attribute that describes the likelihood of the recurrence of previously-seen program values, has been studied enthusiastically in the recent published literature. Much of the energy has focused on refining the initial efforts at predicting load instruction outcomes, with the balance of the effort examining the value locality of either all register-writing instructions, or a focused subset of them. Surprisingly, there has been very little published characterization of or effort to exploit the value locality of data words stored to memory by computer programs. This paper presents such a characterization, proposes both memory-centric (based on message passing) and producer-centric (based on program structure) prediction mechanisms for stored data values, introduces the concept of silent stores and new definitions of multiprocessor false sharing based on these observations, and suggests new techniques for aligning cache coherence protocols and microarchitectural store handling techniques to exploit the value locality of stores. We find that realistic implementations of these techniques can significantly reduce multiprocessor data bus traffic and are more effective at reducing address bus traffic than the addition of Exclusive state to a MSI coherence protocol. We also show that squashing of silent stores can provide uniprocessor speedups greater than the addition of store-to-load forwarding.'],\n",
       "   '2000']],\n",
       " [[['HLS: combining statistical and symbolic simulation to guide microprocessor designs.',\n",
       "    'As microprocessors continue to evolve, many optimizations reach a point of diminishing returns. We introduce HLS, a hybrid processor simulator which uses statistical models and symbolic execution to evaluate design alternatives. This simulation methodology allows for quick and accurate contour maps to be generated of the performance space spanned by design parameters. We validate the accuracy of HLS through correlation with existing cycle-by-cycle simulation techniques and current generation hardware. We demonstrate the power of HLS by exploring design spaces defined by two parameters: code properties and value prediction. These examples motivate how HLS can be used to set design goals and individual component performance targets. Additionally, these traces are not as susceptible to transient behavior because they are restricted to frequently executed code. Empirical results show that on average this mechanism can achieve better instruction fetch rates using only 12KB of hardware than a trace cache requiring 15KB of hardware, while producing long, persistent traces more suited to optimization.'],\n",
       "   '2000']],\n",
       " [[['Completion time multiple branch prediction for enhancing trace cache performance.',\n",
       "    'The need for multiple branch prediction is inherent to wide instruction fetching. This paper presents a completion time multiple branch predictor called the Tree-based Multiple Branch Predictor (TMP) that builds on previous single branch prediction techniques. It employs a tree structure of branch predictors, or tree-node predictors, and achieves accurate multiple branch prediction by leveraging the high accuracies of the individual branch predictors. A highly-efficient TMP design uses the 2-bit saturating counters for the tree-node predictors. To achieve higher prediction rate, the TMP employs two-level schemes for the tree-node predictors resulting in a three-level TMP design. Placing the TMP at completion time reduces the critical latency in the front-end of the pipeline; the resultant longer update latency does not significantly impact the overall performance. In this paper the TMP is applied to a trace cache design and shown to be very effective in increasing its performance. Results: A realistic-size TMP (72KB) can predict 1, 2, 3, and 4 consecutive blocks with compounded prediction accuracies of 96%, 93%, 87%, and 82%, respectively. The block-based trace cache with this TMP achieves 4.75 IPC for SPECint95 on an idealized machine, which is a 20% performance improvement over the original design [1]. This improved performance is 8% above that of a conventional I-cache design with perfect single branch prediction.'],\n",
       "   '2000']],\n",
       " [[['Reconfigurable caches and their application to media processing.',\n",
       "    'High performance general-purpose processors are increasingly being used for a variety of application domains - scientific, engineering, databases, and more recently, media processing. It is therefore important to ensure that architectural features that use a significant fraction of the on-chip transistors are applicable across these different domains. For example, current processor designs often devote the largest fraction of on-chip transistors (up to 80%) to caches. Many workloads, however, do not make effective use of large caches; e.g., media processing workloads which often have streaming data access patterns and large working sets. This paper proposes a new reconfigurable cache design. This design enables the cache SRAM arrays to be dynamically divided into multiple partitions that can be used for different processor activities. These activities can benefit applications that would otherwise not use the storage allocated to large conventional caches. Our design involves relatively few modifications to conventional cache design, and analysis using a modification of the CACTI analytical model shows a small impact on cache access time. We evaluate one representative use of reconfigurable caches - instruction reuse for media processing. We find this use gives IPC improvements ranging from 1.04X to 1.20X in simulation across eight media processing benchmarks.'],\n",
       "   '2000']],\n",
       " [[['Memory access scheduling.',\n",
       "    'The bandwidth and latency of a memory system are strongly dependent on the manner in which accesses interact with the &ldquo;3-D&rdquo; structure of banks, rows, and columns characteristic of contemporary DRAM chips. There is nearly an order of magnitude difference in bandwidth between successive references to different columns within a row and different rows within a bank. This paper introduces memory access scheduling, a technique that improves the performance of a memory system by reordering memory references to exploit locality within the 3-D memory structure. Conservative reordering, in which the first ready reference in a sequence is performed, improves bandwidth by 40% for traces from five media benchmarks. Aggressive reordering, in which operations are scheduled to optimize memory bandwidth, improves bandwidth by 93% for the same set of applications. Memory access scheduling is particularly important for media processors where it enables the processor to make the most efficient use of scarce memory bandwidth.'],\n",
       "   '2000']],\n",
       " [[['Recency-based TLB preloading.',\n",
       "    'Caching and other latency tolerating techniques have been quite successful in maintaining high memory system performance for general purpose processors. However, TLB misses have become a serious bottleneck as working sets are growing beyond the capacity of TLBs. This work presents one of the first attempts to hide TLB miss latency by using preloading techniques. We present results for traditional next-page TLB miss preloading - an approach shown to cut some of the misses. However, a key contribution of this work is a novel TLB miss prediction algorithm based on the concept of &ldquo;recency&rdquo;, and we show that it can predict over 55% of the TLB misses for the five commercial applications considered.'],\n",
       "   '2000']],\n",
       " [[['A scalable approach to thread-level speculation.',\n",
       "    'While architects understand how to build cost-effective parallel machines across a wide spectrum of machine sizes (ranging from within a single chip to large-scale servers), the real challenge is how to easily create parallel software to effectively exploit all of this raw performance potential. One promising technique for overcoming this problem is Thread-Level Speculation (TLS), which enables the compiler to optimistically create parallel threads despite uncertainty as to whether those threads are actually independent. In this paper, we propose and evaluate a design for supporting TLS that seamlessly scales to any machine size because it is a straightforward extension of writeback invalidation-based cache coherence (which itself scales both up and down). Our experimental results demonstrate that our scheme performs well on both single-chip multiprocessors and on larger-scale machines where communication latencies are twenty times larger.'],\n",
       "   '2000']],\n",
       " [[['Energy-driven integrated hardware-software optimizations using SimplePower.',\n",
       "    'With the emergence of a plethora of embedded and portable applications, energy dissipation has joined throughput, area, and accuracy/precision as a major design constraint. Thus, designers must be concerned with both optimizing and estimating the energy consumption of circuits, architectures, and software. Most of the research in energy optimization and/or estimation has focused on single components of the system and has not looked across the interacting spectrum of the hardware and software. The novelty of our new energy estimation framework, SimplePower, is that it evaluates the energy considering the system as a whole rather than just as a sum of parts, and that it concurrently supports both compiler and architectural experimentation. We present the design and use of the SimplePower framework that includes a transition-sensitive, cycle-accurate datapath energy model that interfaces with analytical and transition sensitive energy models for the memory and bus subsystems, respectively. We analyzed the energy consumption of ten codes from the multidimensional array domain, a domain that is important for embedded video and signal processing systems, after applying different compiler and architectural optimizations. Our experiments demonstrate that early estimates from the SimplePower energy estimation framework can help identify the system energy hotspots and enable architects and compiler designers to focus their efforts on these areas.'],\n",
       "   '2000']],\n",
       " [[['CHIMAERA: a high-performance architecture with a tightly-coupled reconfigurable functional unit.',\n",
       "    'Reconfigurable hardware has the potential for significant performance improvements by providing support for application-specific operations. We report our experience with Chimaera, a prototype system that integrates a small and fast reconfigurable functional unit (RFU) into the pipeline of an aggressive, dynamically-scheduled superscalar processor. Chimaera is capable of performing 9-input/1-output operations on integer data. We discuss the Chimaera C compiler that automatically maps computations for execution in the RFU. Chimaera is capable of: (1) collapsing a set of instructions into RFU operations, (2) converting control-flow into RFU operations, and (3) supporting a more powerful fine-grain data-parallel model than that supported by current multimedia extension instruction sets (for integer operations). Using a set of multimedia and communication applications we show that even with simple optimizations, the Chimaera C compiler is able to map 22% of all instructions to the RFU on the average. A variety of computations are mapped into RFU operations ranging from as simple as add/sub-shift pairs to operations of more than 10 instructions including several branches. Timing experiments demonstrate that for a 4-way out-of-order superscalar processor Chimaera results in average performance improvements of 21%, assuming a very aggressive core processor design (most pessimistic RFU latency model) and communication overheads from and to the RFU.'],\n",
       "   '2000']],\n",
       " [[['Understanding the backward slices of performance degrading instructions.',\n",
       "    \"For many applications, branch mispredictions and cache misses limit a processor's performance to a level well below its peak instruction throughput. A small fraction of static instructions, whose behavior cannot be anticipated using current branch predictors and caches, contribute a large fraction of such performance degrading events. This paper analyzes the dynamic instruction stream leading up to these performance degrading instructions to identify the operations necessary to execute them early. The backward slice (the subset of the program that relates to the instruction) of these performance degrading instructions, if small compared to the whole dynamic instruction stream, can be pre-executed to hide the instruction's latency. To overcome conservative dependance assumptions that result in large slices, speculation can be used, resulting in speculative slices. This paper provides an initial characterization of the backward slices of L2 data cache misses and branch mispredictions, and shows the effectiveness of techniques, including memory dependence prediction and control independence, for reducing the size of these slices. Through the use of these techniques, many slices can be reduced to less than one tenth of the full dynamic instruction stream when considering the 512 instructions before the performance degrading instruction.\"],\n",
       "   '2000']],\n",
       " [[['Cycle-accurate energy consumption measurement and analysis: case study of ARM7TDMI.',\n",
       "    'We introduce an energy consumption analysis of complex digital systems through a case study of ARM7TDMI RISC processor by using a new energy measurement technique. We developed a cycle-accurate energy consumption measurement system based on charge transfer which is robust to spiky noise and is capable of collecting a range of power consumption profiles in real time. The relative energy variation of the RISC core is measured by changing the opcode, the instruction fetch address, the register number, in each pipeline stage, respectively. We demonstrated energy characterization of a pipelined RISC processor for high-level power reduction.'],\n",
       "   '2000']],\n",
       " [[['Power minimization of functional units partially guarded computation.',\n",
       "    'This paper deals with power minimization problem for data-dominated applications based on a novel concept called partially guarded computation. We divide a functional unit into two parts - MSP (Most Significant Part) and LSP (Least Significant Part) - and allow the functional unit to perform only the LSP computation if the range of output data can be covered by LSP. We dynamically disable MSP computation to remove unnecessary transitions thereby reducing power consumption. We also propose a systematic approach for determining optimal location of the boundary between the two parts during high-level synthesis. Experimental results show about 10~44% power reduction with about 30~36% area overhead and less than 3% delay overhead in functional units.'],\n",
       "   '2000']],\n",
       " [[['Operating-system directed power reduction.',\n",
       "    'this paper presents a new approach for power reduction by taking a global, software-centric view. It analyzes the sources of power consumption: tasks that require services from hardware components. When a component is not used by any task, it can enter a sleeping state to save power. Operating systems have detailed information about tasks; therefore, OS is the best place for identifying hardware idleness and shutting down unused components. We implement this technique in Linux and show that it can save more than 50% power compared to traditional hardware-centric shutdown techniques.'],\n",
       "   '2000']],\n",
       " [[['Voltage scheduling in the IpARM microprocessor system.',\n",
       "    'Microprocessors represent a significant portion of the energy conðsumed in portable electronic devices. Dynamic Voltage Scaling (DVS) allows a device to reduce energy consumption by lowering its processor speed at run-time, allowing a corresponding reduction in processor voltage and energy. A voltage scheduler determines the appropriate operating voltage by analyzing application conðstraints and requirements. A complete software implementation, including both applications and the underlying operating system, shows that DVS is effective at reducing the energy consumed withðout requiring extensive software modification.'],\n",
       "   '2000']],\n",
       " [[['Incremental physical design.',\n",
       "    'Incremental modification and optimization in VLSI Computer-Aided Design (CAD) is of fundamental importance. However, it has not been investigated as a discipline. Previous research and development effort is very unfocused and incomplete. Comprehensive study of incremental algorithms and solutions in the context of CAD tool development is an open area of research with a great deal of potential. Full understanding and focused participation in research and development in the area of incremental and dynamic would help us cope with the complexity of present day VLSI systems and facilitates concurrent optimization. In this paper we formulate and survey fundamental problems in incremental physical design. Preliminary solutions to a subset of these problems will be outlined.'],\n",
       "   '2000']],\n",
       " [[['Pseudo pin assignment with crosstalk noise control.',\n",
       "    'This paper presents a new pseudo pin assignment (PPA) algorithm with crosstalk noise control in multi-layer gridless general-area routing. We propose a two-step approach that considers obstacles and minimizes the estimated number of vias under crosstalk noise constraints. Without crosstalk noise control in PPA, the average noise after detailed routing of our test cases is 0.13-0.22 VDD with up to 8% of nets larger than 0.3 VDD. However, if the noise constraint of each net is set of 0.3 VDD in PPA, the average noise reduces 15% -31% to 0.11-0.15 VDD with no crosstalk noise violations. Even without rip-out and reroute, the detailed routing completion rate is 95%-99% and the ratio of vias to nets is only 0.7-1.2.'],\n",
       "   '2000']],\n",
       " [[['DUNE: a multi-layer gridless routing system with wire planning.',\n",
       "    'In this paper, we present a multi-layer gridless detailed routing system with wire planning for deep sub-micron (DSM) physical designs. It includes a multi-layer gridless detailed routing engine that can efficiently route point-to-point connections, and a wire planning algorithm that uses exact gridless design rules (variable width and variable spacing) to accurately estimate the routing resources and distribute nets into routing regions. The wire planning method also enables efficient rip-up and reroute in gridless routing. Unlike previous approaches which explore alternatives of blocked nets by gradually tightening the design rules, our planning based approach can take the exact gridless rules and resolve the congestion and blockage at a higher level. Our experimental result shows that using the wire planning algorithm in our detailed routing system can improve the routability and also speed up the run time by 3-17 times.'],\n",
       "   '2000']],\n",
       " [[['Classical floorplanning harmful?',\n",
       "    'Classical floorplanning formulations may lead researchers to solve the wrong problems. This paper points out several examples, including (i) the preoccupation with packing-driven, as opposed to connectivity-driven, problem formulations and benchmarking standards; (ii) the preoccupation with rectangular (and L or T shaped) block shapes; and (iii) the lack of attention to algorithm scalability, fixed-die layout requirements, and the overall RTL-down methodology context. The right problem formulations must match the purpose and context of prevailing RTL-down design methodologies, and must be neither overconstrained nor underconstrained. The right solution ingredients are those which are scalable while delivering good solution quality according to relevant metrics. We also describe new problem formulations and solution ingredients, notably a perfect rectilinear floorplanning formulation that seeks zero-whitespace, perfectly packed rectilinear floorplans in a fixed-die regime. The paper closes with a list of questions for future research.'],\n",
       "   '2000']],\n",
       " [[['Wire packing: a strong formulation of crosstalk-aware chip-level track/layer assignment with an efficient integer programming solution.',\n",
       "    'By focusing on chip-wide slices of the global routing grid, making a few mild geometric assumptions about layer use, and suitably abstracting pin details, we derive an extremely efficient integer linear programming (ILP) formulation for track/layer assignment. The key technical insight is to model all constraints - both geometric and crosstalk - as cliques in an appropriate conflict graph; these cliques can be extracted quickly from the interval structures of wires in a slice. We develop a \"strong\" linear relaxation of this problem that almost always yields the integral optimum; this solution gives us directly the maximum number of wires that can pack legally without crosstalk risk. Experiments on synthetic netlists that match statistics of wire layouts from industrial 0.25um designs demonstrate that we can pack 100 - 1000 wires optimally, or with at worst a very few overflows, in seconds.'],\n",
       "   '2000']],\n",
       " [[['Routability-driven repeater block planning for interconnect-centric floorplanning.',\n",
       "    'In this paper, we present a repeater block planning algorithm for interconnect-centric floorplanning. We introduce the concept of independent feasible regions for repeaters and derive an analytical formula for their computation. We develop a routability-driven repeater clustering algorithm to perform repeater block planning based on iterative deletion. The goal is to obtain a high quality solution for the repeater block locations so that performance-driven interconnect synthesis at the routing stage can be carried out with ease, while minimizing the chip area. Experimental results show that our method increases the percentage of all global nets that meet their target delays from 67.5% in [8] to 85%. Meanwhile, our approach is able to minimize the expected routing congestion, making it easier for performance-driven routers to synthesize global nets that require the insertion of repeaters to meet timing constraints.'],\n",
       "   '2000']],\n",
       " [[['On Effective IDDQ Testing of Low Voltage CMOS Circuits Using Leakage Control Techniques.',\n",
       "    'The use of low-threshold devices in low0voltage CMOS circuits leads to an exponential increase in the intrinsic leakage current. This threatens the effectiveness of IDDQ testing for such low-voltage circuits because it is difficult to differentiate a defect-free circuit from defective circuits. Recently, several leakage control techniques have been proposed to reduce intrinsic leakage current, which may benefit IDDQ testing. In this paper, we investigate the possibilities of applying different leakage control techniques to improve the fault coverage of IDDQ testing. Results on a large number of benchmarks indicate that dual-threshold and vector control techniques can be very effective in improving fault coverage for IDDQ testing for some circuits.'],\n",
       "   '2000']],\n",
       " [[['Deciding linear-trigonometric problems.',\n",
       "    'In this paper, we present a decision procedure for certain linear-trigonometric problems for the reals and integers formalized in a suitable first-order language. The inputs are restricted to formulas, where all but one of the quantified variables occur linearly and at most one occurs both linearly and in a specific trigonometric function. Moreover we may allow in addition the integer-part operation in formulas. Besides ordinary quantifiers, we allow also counting quantifiers. Furthermore we also determine the qualitative structure of the connected components of the satisfaction set of the mixed linear-trigonometric variable. We also consider the decision of these problems in subfields of the real algebraic numbers.'],\n",
       "   '2000']],\n",
       " [[['Integer Smith form via the valence: experience with large sparse matrices from homology.',\n",
       "    'We present a new algorithm to compute the Integer Smith normal form of large sparse matrices. We reduce the computation of the Smith form to independent, and therefore parallel, computations modulo powers of word-size primes. Consequently, the algorithm does not suffer from coefficient growth. We have implemented several variants of this algorithm (Elimination and/or Black-Box techniques) since practical performance depends strongly on the memory available. Our method has proven useful in algebraic topology for the computation of the homology of some large simplicial complexes.'],\n",
       "   '2000']],\n",
       " [[['Computing an equidimensional decomposition of an algebraic variety by means of geometric resolutions.',\n",
       "    'Let &fnof;1, &hellip; , &fnof;s be polynomials in n variables over a field of characteristic zero and d be the maximum of their total degree. We propose a new probabilistic algorithm for computing a geometric resolution of each equidimensional part of the variety defined by the system &fnof;1 = &middot;&middot;&middot; = &fnof;s = 0. The returned resolutions are encoded by means of Straight-Line Programs and the complexity of the algorithm is polynomial in a geometric degree of the system. In the worst case this complexity is asymptotically polynomial in sdn.'],\n",
       "   '2000']],\n",
       " [[[\"On the design and implementation of Brown's algorithm over the integers and number fields.\",\n",
       "    \"We study the design and implementation of the dense modular GCD algorithm of Brown applied to bivariate polynomial GCDs over the integers and number fields. We present an improved design of Brown's algorithm and compare it asymptotically with Brown's original algorithm, with GCD-HEU, the heuristic GCD algorithm, and with the EEZGCD algorithm. We also make an empirical comparison based on Maple implementations of the algorithms. Our findings show that a careful implementation of our improved version of Brown's algorithm is much better than the other algorithms in theory and in practice.\"],\n",
       "   '2000']],\n",
       " [[['Solving projective complete intersection faster.',\n",
       "    'In this paper, we present a new method for solving square polynomial systems with no zero at infinity. We analyze its complexity, which indicates substantial improvements, compared with the previously known methods for solving such systems. We describe a framework for symbolic and numeric computations, developed in C++, in which we have implemented this algorithm. We mention the techniques that are involved in order to build efficient codes and compare with existing softwares. We end by some applications of this method, considering in particular an autocalibration problem in Computer Vision and an identification problem in Signal Processing, and report on the results of our first implementation.'],\n",
       "   '2000']],\n",
       " [[['A Joint Power/Performance Optimization Algorithm for Multiprocessor Systems using a Period Graph Construct.',\n",
       "    'A critical challenge in synthesis techniques for iterative applications is the efficient analysis of performance in the presence of communication resource contention. To address this challenge, we introduce the concept of the period graph. The period graph is constructed from the output of a simulation of the system, with idle states included in the graph, and its maximum cycle mean is used to estimate overall system throughput. As an example of the utility of the period graph, we demonstrate its use in a joint power/performance optimization solution that uses either a nested genetic algorithm, or a simulated annealing algorithm. We analyze the fidelity of this estimator, and quantify the speedup and optimization accuracy obtained compared to simulation.'],\n",
       "   '2000']],\n",
       " [[['Low Power Storage Cycle Budget Distribution Tool Support for Hierarchical Graphs.',\n",
       "    \"In data dominated applications, like multi-media and telecom applications, data storage and transfers are the most important factors in terms of energy consumption, area and system performance. Several steps which optimize these costs are present in our systematic Data Transfer and Storage Exploration methodology. In the important step discussed in this paper, the cycle budget available for background storage transfers is globally distributed over the application's memory accesses that are typically grouped in the loop and function hierarchy. This is crucial for meeting the real-time constraints with a customized memory organisation without counteracting the memory size and energy budget optimizations achieved by earlier steps in our script.This paper proves the effectiveness of the prototype tool on driver applications of several application domains. It clearly shows the tradeoff between power, area and speed.\"],\n",
       "   '2000']],\n",
       " [[['Hierarchical Conditional Dependency Graphs as a Unifying Design Representation in the CODESIS High-Level Synthesis System.',\n",
       "    'In high-level hardware synthesis (HLS) there is a gap on the quality of the synthesized results between data-flow and control-flow dominated behavioral descriptions. Heuristics destined for the former usually perform poorly on the latter. To close this gap, the CODESIS interactive HLS tool relies on a unifying intermediate design representation and adapted heuristics that are able to accommodate both types of designs as well as designs of a mixed data-flow and control-flow nature. Preliminary experimental results in mutual exclusiveness detection and in efficiently scheduling conditional behaviors, are encouraging and prompt for more extensive experimentation.'],\n",
       "   '2000']],\n",
       " [[['Code Generation for Embedded Processors.',\n",
       "    \"The increasing use of programmable processors as IP blocks in embedded system design creates a need for C/C++ compilers capable of generating efficient machine code. Many of today's compilers for embedded processors suffer from insufficient code quality in terms of code size and performance. This violates the tight chip area and real-time constraints often imposed on embedded systems. The reason is that embedded processors typically show architectural features which are not well handled by classical compiler technology. This paper provides a survey of methods and techniques dedicated to efficient code generation for embedded processors. Emphasis is put on DSP and multimedia processors, for which better compiler technology is definitely required. In addition, some frontend aspects and recent trends in research and industry are briefly covered. The goal of these recent efforts in embedded code generation is to facilitate the step from assembly to high-level language programming of embedded systems, so as to provide higher productivity, dependability, and portability of embedded software.\"],\n",
       "   '2000']],\n",
       " [[['Instruction-based System-level Power Evaluation of System-On-A-Chip Peripheral Cores.',\n",
       "    'Various system-level core-based power evaluation approaches for core types like microprocessors, caches, main memories, and buses, have been proposed in the past. Approaches for other types of components have been based either on the gate-level, register-transfer level, or behavioral-level. We propose a new technique, suitable for a variety of cores like peripheral cores, that is the first to combine gate-level power data with a system-level simulation model written in C++ or Java. For that purpose, we investigated peripheral cores and decomposed their functionality into so-called instructions. Our technique addresses a core-based system design paradigm. We show that our technique is sufficiently accurate for making power-related system-level design decisions, and that its computation time is orders of magnitude smaller than lower-level simulation approaches.'],\n",
       "   '2000']],\n",
       " [[['Putting static analysis to work for verification: A case study.',\n",
       "    \"A method for finding bugs in code is presented. For given small numbers j and k, the code of a procedure is translated into a rela-tional formula whose models represent all execution traces that involve at most j heap cells and k loop iterations. This formula is conjoined with the negation of the procedure's specification. The models of the resulting formula, obtained using a constraint solver, are counterexamples: executions of the code that violate the specification.The method can analyze millions of executions in seconds, and thus rapidly expose quite subtle flaws. It can accommodate calls to procedures for which specifications but no code is avail-able. A range of standard properties (such as absence of null pointer dereferences) can also be easily checked, using prede-fined specifications.\"],\n",
       "   '2000']],\n",
       " [[['Automated Testing of Classes.',\n",
       "    \"Programs developed with object technologies have unique features that often make traditional testing methods inadequate. Consider, for instance, the dependence between the state of an object and the behavior of that object: The outcome of a method executed by an object often depends on the state of the object when the method is invoked. It is therefore crucial that techniques for testing of classes exercise class methods when the method's receiver is in different states. The state of an object at any given time depends on the sequence of messages received by the object up to that time. Thus, methods for testing object-oriented software should identify sequences of method invocations that are likely to uncover potential defects in the code under test. However, testing methods for traditional software do not provide this kind of information.In this paper, we use data flow analysis, symbolic execution, and automated deduction to produce sequences of method invocations exercising a class under test. Since the static analysis techniques that we use are applied to different subproblems, the method proposed in this paper can automatically generate information relevant to testing even when symbolic execution and automated deduction cannot be completed successfully.\"],\n",
       "   '2000']],\n",
       " [[['A framework for testing database applications.',\n",
       "    \"Database systems play an important role in nearly every modern organization, yet relatively little research effort has focused on how to test them. This paper discusses issues arising in testing database systems and presents an approach to testing database applications. In testing such applications, the state of the database before and after the user's operation plays an important role, along with the user's input and the system output. A tool for populating the database with meaningful data that satisfy database constraints has been prototyped. Its design and its role in a larger database application testing tool set are discussed.\"],\n",
       "   '2000']],\n",
       " [[['Which pointer analysis should I use?',\n",
       "    'During the past two decades many different pointer analysis algorithms have been published. Although some descriptions include measurements of the effectiveness of the algorithm, qualitative comparisons among algorithms are difficult because of varying infrastructure, benchmarks, and performance metrics. Without such comparisons it is not only difficult for an implementor to determine which pointer analysis is appropriate for their application, but also for a researcher to know which algorithms should be used as a basis for future advances.This paper describes an empirical comparison of the effectiveness of five pointer analysis algorithms on C programs. The algorithms vary in their use of control flow information (flow-sensitivity) and alias data structure, resulting in worst-case complexity from linear to polynomial. The effectiveness of the analyses is quantified in terms of compile-time precision and efficiency. In addition to measuring the direct effects of pointer analysis, precision is also reported by determining how the information computed by the five pointer analyses affects typical client analyses of pointer information: Mod/Ref analysis, live variable analysis and dead assignment identification, reaching definitions analysis, dependence analysis, and conditional constant propagation and unreachable code identification. Efficiency is reported by measuring analysis time and memory consumption of the pointer analyses and their clients.'],\n",
       "   '2000']],\n",
       " [[['Comparison of delivered reliability of branch, data flow and operational testing: A case study.',\n",
       "    'Many analytical and empirical studies of software testing effectiveness have used the probability that a test set exposes at least one fault as the measure of effectiveness. That measure is useful for evaluating testing techniques when the goal of testing is to gain confidence that the program is free from faults. However, if the goal of testing is to improve the reliability of the program (by discovering and removing those faults that are most likely to cause failures when the software is in the field) then the measure of test effectiveness must distinguish between those faults that are likely to cause failures and those that are unlikely to do so. Delivered reliability was previously introduced as a means of comparing testing techniques in that setting. This paper empirically compares reliability delivered by three testing techniques, branch testing, the all-uses data flow testing criterion, and operational testing. The subject program is a moderate-sized C-program (about 10,000 LOC) produced by professional programmers and containing naturally occurring faults.'],\n",
       "   '2000']],\n",
       " [[['Prioritizing test cases for regression testing.',\n",
       "    'Test case prioritization techniques schedule test cases in an order that increases their effectiveness in meeting some performance goal. One performance goal, rate of fault detection, is a measure of how quickly faults are detected within the testing process; an improved rate of fault detection can provide faster feedback on the system under test, and let software engineers begin locating and correcting faults earlier than might otherwise be possible. In previous work, we reported the results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: (1) can prioritization techniques be effective when aimed at specific modified versions; (2) what tradeoffs exist between fine granularity and coarse granularity prioritization techniques; (3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? This paper reports the results of new experiments addressing these questions.'],\n",
       "   '2000']],\n",
       " [[['Slicing concurrent programs.',\n",
       "    'Slicing is a well-known program analysis technique for analyzing sequential programs and found useful in debugging, testing and reverse engineering. This paper extends the notion of slicing to concurrent programs with shared memory, interleaving semantics and mutual exclusion. Interference among concurrent threads or processes complicates the computation of slices of concurrent programs. Further, unlike slicing of sequential programs, a slicing algorithm for concurrent programs needs to differentiate between loop-independent data dependence and certain loop-carried data dependences.We show why previous methods do not give precise solutions in the presence of nested threads and loops and describe our solution that correctly and efficiently computes precise slices. Though the complexity of this algorithm is exponential on the number of threads, a number of optimizations are suggested. Using these optimizations, we are able to get near linear behavior for many practical concurrent programs.'],\n",
       "   '2000']],\n",
       " [[['Requirements-based monitors for real-time systems.',\n",
       "    'Before designing safety- or mission-critical real-time systems, a specification of the required behavior of the system should be produced and reviewed by domain experts. After the system has been implemented, it should be thoroughly tested to ensure that it behaves correctly. This is best done using a monitor, a system that observes the behavior of a target system and reports if that behavior is consistent with the requirements. Such a monitor can be used both as an oracle during testing and as a supervisor during operation. Monitors should be based on the documented requirements of the system. If the target system is required to monitor or control real-valued quantities, then the requirements, which are expressed in terms of the monitored and controlled quantities, will allow a range of behaviors to account for errors and imprecision in observation and control of these quantities. Even if the controlled variables are discrete valued, the requirements must specify the timing tolerance. Because of the limitations of the devices used by the monitor to observe the environmental quantities, there is unavoidable potential for false reports, both negative and positive. This paper discusses design of monitors for real-time systems, and examines the conditions under which a monitor will produce false reports. We describe the conclusions that can be drawn when using a monitor to observe system behavior.'],\n",
       "   '2000']],\n",
       " [[['Black-box test reduction using input-output analysis.',\n",
       "    'Test reduction is an important issue in black-box testing. The number of possible black-box tests for any non-trivial software application is extremely large. For the class of programs with multiple inputs and outputs, the number of possible tests grows very rapidly as combinations of input test data are considered. In this paper, we introduce an approach to test reduction that uses automated input-output analysis to identify relationships between program inputs and outputs. Our initial experience with the approach has shown that it can significantly reduce the number of black-box tests.'],\n",
       "   '2000']],\n",
       " [[['OMEN: A strategy for testing object-oriented software.',\n",
       "    'This paper presents a strategy for structural testing of object-oriented software systems with possibly unknown clients and unknown information about invoked methods. By exploiting the combined points-to and escape analysis developed for compiler optimization, our testing paradigm does not require a whole program representation to be in memory simultaneously for testing analysis. Potential effects from outside the component under test are easily identified and reported to the tester. As client and server methods become known, the graph representation of object relationships is easily extended, allowing the computation of test tuples to be performed in a demand-driven manner, without requiring unnecessary computation of test tuples based on predictions of potential clients.'],\n",
       "   '2000']],\n",
       " [[['Testability, fault size and the domain-to-range ratio: An eternal triangle.',\n",
       "    'A number of different concepts have been proposed that, loosely speaking, revolve around the notion of software testability. Indeed, the concept of testability itself has been interpreted in a variety of ways by the software community. One interpretation is concerned with the extent of the modifications a program component requires, in terms of its input and output variables, so that the entire behaviour of the component is observable and controllable. Another interpretation is the ease with which faults, if present in a program, can be revealed by the testing process and the propagation, infection and execution (PIE) model has been proposed as a method of estimating this. It has been suggested that this particular interpretation of testability might be linked with the metric domain-to-range ratio (DRR), i.e. the ratio of the cardinality of the set of all inputs (the domain) to the cardinality of the set of all outputs (the range). This paper reports work in progress exploring some of the connections between the concepts mentioned. In particular, a simple mathematical link is established between domain-to-range ratio and the observability and controllability aspects of testability. In addition, the PIE model is re-considered and a relationship with fault size is observed. This leads to the suggestion that it might be more straightforward to estimate PIE testability by an adaptation of traditional mutation analysis. The latter suggestion exemplifies the main goals of the work described here, namely to seek greater understanding of testability in general and, ultimately, to find easier ways of determining it.'],\n",
       "   '2000']],\n",
       " [[['Wearable Visual Robots.',\n",
       "    'Research work reported in the literature in wearable visual computing has used exclusively static (or non-active) cameras, making the imagery and image measurements dependent on the wearer&rsquo;s posture and motions. It is assumed that the camera is pointing in a good direction to view relevant parts of the scene at best by virtue of being mounted on the wearer&rsquo;s head, or at worst wholly by chance. Even when pointing in roughly the correct direction, any visual processing relying on feature correspondence from a passive camera is made more difficult by the large, uncontrolled inter-image movements which occur when the wearer moves, or even breathes. This paper presents a wearable active visual sensor which is able to achieve a level of decoupling of camera movement from the wearer&rsquo;s posture and motions by a combination of inertial and visual sensor feedback and active control. The issues of sensor placement, robot kinematics and their relation to wearability are discussed. The performance of the prototype robot is evaluated for some essential visual tasks. The paper also discusses potential applications for this kind of wearable robot.'],\n",
       "   '2000']],\n",
       " [[['Instructible information agents for Web mining.',\n",
       "    'Information agents are intended to assist their users in locating relevant information in vast collections of documents like the WWW. In many cases, e.g., when trying to integrate pieces of information from previously unrelated sources, it is not sufficient to merely identify documents containing relevant data. Instead, information agents have to identify the interesting portions of these documents and make them available for further use. This paper deals with the problem of training an information agent to identify and extract interesting pieces of information from online documents.'],\n",
       "   '2000']],\n",
       " [[['Guiding people to information: providing an interface to a digital library using reference as a basis for indexing.',\n",
       "    'We describe Rosetta, a digital library system for scientific literature. Rosetta makes it easy for people to find the information for which they are looking even when using short, imprecise queries. Rosetta indexes research articles based on the way they have been described when cited in other documents. The concise descriptions that occur in citations are similar to the short queries people typically form when searching; therefore, citations make a better basis for indexing than do the words used within a research article itself. Using this indexing technique we are able to provide a user interface that presents users with an automatically generated directory of the information space surrounding a query. Our objective with this interface is to present people with the information for which they have asked as well as the information for which they may have intended to ask.'],\n",
       "   '2000']],\n",
       " [[['Data exploration across temporal contexts.',\n",
       "    \"The ability to quickly explore and compare multiple scenarios is an important component of exploratory data analysis. Yet today's interfaces cannot represent alternative exploration paths as a branching history, forcing the user to recognize conceptual branch points in a linear history. Further, the interface can only show information from one state at a time, forcing the user to use her memory to compare scenarios. Our system includes a tree-structured visualization for navigating across time and scenarios. The visualization also allows browsing the history and selectively undoing/redoing events within a scenario or across scenarios. It uses the AI formalism of contexts to maintain multiple, possibly mutually inconsistent, knowledge base states. Cross-context formulas can be written for explicit scenario comparison, including visualizations of scenario differences.\"],\n",
       "   '2000']],\n",
       " [[['Adaptation in automated user-interface design.',\n",
       "    'Design problems involve issues of stylistic preference and flexible standards of success; human designers often proceed by intuition and are unaware of following any strict rule-based procedures. These features make design tasks especially difficult to automate. Adaptation is proposed as a means to overcome these challenges. We describe a system that applies an adaptive algorithm to automated user interface design within the framework of the MOBI-D (Model-Based Interface Designer) interface development environment. Preliminary experiments indicate that adaptation improves the performance of the automated user interface design system.'],\n",
       "   '2000']],\n",
       " [[['VITE: a visual interface supporting the direct manipulation of structured data using two-way mappings.',\n",
       "    \"Information processed by computers is frequently stored and organized for the computer's, rather than for the user's, convenience. For example, information stored in a database is normalized and indexed so computers can efficiently access, process, and retrieve it. However, it is not natural for people to manipulate such formal/prescriptive representations. Instead, people frequently sort items by rough notions of association or categorization. One natural organizational process has been found to center around manipulations of objects in spatial arrangements. Examples of this range from the organization of documents and other items on a regular office desktop to the use of 3&Prime;&times;5&Prime; cards to organize a conference program. Using visual cues and spatial proximity, people change the categorizations of and relationships between objects. Without the help of indices or perfect memory people can still interpret, locate, and manipulate the information represented by the items and the higher-level visual structures they form. The VITE system presented here is an intuitive interface for people to manipulate information in their own way and at their own pace. VITE provides for configurable visualizations of structured data sets so users can design their own &ldquo;perspectives&rdquo; and a direct manipulation interface allowing editing of and manipulation on the structured data.\"],\n",
       "   '2000']],\n",
       " [[['SUITOR: an attentive information system.',\n",
       "    \"Attentive systems pay attention to what users do so that they can attend to what users need. Such systems track user behavior, model user interests, and anticipate user desires and actions. Because the general class of attentive systems is broad &mdash; ranging from human butlers to web sites that profile users &mdash; we have focused specifically on attentive information systems, which observe user actions with information resources, model user information states, and suggest information that might be helpful to users. In particular, we describe an implemented system, Simple User Interest Tracker (Suitor), that tracks computer users through multiple channels &mdash; gaze, web browsing, application focus &mdash; to determine their interests and to satisfy their information needs. By observing behavior and modeling users, Suitor finds and displays potentially relevant information that is both timely and non-disruptive to the users' ongoing activities.\"],\n",
       "   '2000']],\n",
       " [[['Margin notes: building a contextually aware associative memory.',\n",
       "    \"Both the Human Computer Interaction and Information Retrieval fields have developed techniques to allow a searcher to find the information they seek quickly. However, these techniques are designed to augment one's direct-recall memory, where the searcher is actively trying to find information. Associative memory, in contrast, happens automatically and continuously, triggering memories that relate to the observed world. This paper presents design techniques and heuristics for building &ldquo;remembrance agents,&rdquo; applications that watch a user's context and proactively suggest information that may be of use. General design issues are discussed and illuminated by a description of Margin Notes, an automatic just-in-time information system for the Web.\"],\n",
       "   '2000']],\n",
       " [[['Mining asynchronous periodic patterns in time series data.',\n",
       "    'Periodicy detection in time series data is a challenging problem of great importance in many applications. Most previous work focused on mining synchronous periodic patterns and did not recognize the misaligned presence of a pattern due to the intervention of random noise. In this paper, we propose a more flexible model of asynchronous periodic pattern that may be present only within a subsequence and whose occurrences may be shifted due to disturbance. Two parameters min_rep and max_dis are employed to specify the minimum number of repetitions that is required within each segment of nondisrupted pattern occurrences and the maximum allowed disturbance between any two successive valid segments. Upon satisfying these two requirements, the longest valid subsequence of a pattern is returned. A two-phase algorithm is devised to first generate potential periods by distance-based pruning followed by an iterative procedure to derive and validate candidate patterns and locate the longest valid subsequence. We also show that this algorithm cannot only provide linear time complexity with respect to the length of the sequence but also achieve space efficiency.'],\n",
       "   '2000']],\n",
       " [[['Uniform semantic treatment of default and autoepistemic logic.',\n",
       "    'We revisit the issue of epistemological and semantic foundations for autoepistemic and default logics, two leading formalisms in nonmonotonic reasoning. We develop a general semantic approach to autoepistemic and default logics that is based on the notion of a belief pair and that exploits the lattice structure of the collection of all belief pairs. For each logic, we introduce a monotone operator on the lattice of belief pairs. We then show that a whole family of semantics can be defined in a systematic and principled way in terms of fixpoints of this operator (or as fixpoints of certain closely related operators). Our approach elucidates fundamental constructive principles in which agents form their belief sets, and leads to approximation semantics for autoepistemic and default logics. It also allows us to establish a precise one-to-one correspondence between the family of semantics for default logic and the family of semantics for autoepistemic logic. The correspondence exploits the modal interpretation of a default proposed by Konolige. Our results establish conclusively that default logic can be viewed as a fragment of autoepistemic logic, a result that has been long anticipated. At the same time, they explain the source of the difficulty to formally relate the semantics of default extensions by Reiter and autoepistemic expansions by Moore. These two semantics occupy different locations in the corresponding families of semantics for default and autoepistemic logics.'],\n",
       "   '2000']],\n",
       " [[['Subresultants Revisited.',\n",
       "    'Subresultants and polynomial remainder sequences are an important tool in polynomial computer algebra. In this survey, we sketch the history, formalize a unified framework for the various notions, derive a number of results from the early 1970s within our framework, and report on implementations.'],\n",
       "   '2000']],\n",
       " [[['On the Expressivity and Complexity of Quantitative Branching-Time Temporal Logics.',\n",
       "    'We investigate extensions of CTL allowing to express quantitative requirements about an abstract notion of time in a simple discrete-time framework, and study the expressive power of several relevant logics.When only subscripted modalities are used, polynomial-time model checking is possible even for the largest logic we consider, while the introduction of freeze quantifiers leads to a complexity blow-up.'],\n",
       "   '2000']],\n",
       " [[['Undecidable Problems in Unreliable Computations.',\n",
       "    'Lossy counter machines are defined as Minsky counter machines where the values in the counters can spontaneously decrease at any time. While termination is decidable for lossy counter machines, structural termination (termination for every input) is undecidable. This undecidability result has far-reaching consequences. Lossy counter machines can be used as a general tool to prove the undecidability of many problems, for example: (1) The verification of systems that model communication through unreliable channels (e.g., model checking lossy fifo-channel systems and lossy vector addition systems). (2) Several problems for reset Petri nets, like structural termination, boundedness and structural boundedness. (3) Parameterized problems like fairness of broadcast communication protocols.'],\n",
       "   '2000']],\n",
       " [[['Definability and Compression.',\n",
       "    \"A compression algorithm takes a finite structure of a class K as input and produces a finite structure of a different class K' as output. Given a property P on the class K defined in a logic L, we study the definability of property P on the class K'. We consider two compression schemes on unary ordered structures (strings), compression by run-length encoding and the classical Lempel-Ziv-78 scheme.First-order properties of strings are first-order on run-length compressed strings, but this fails for images, i.e. 2-dimensional strings. We present simple first-order properties of strings which are not first-order definable on strings compressed with the Lempel-Ziv-78 compression scheme.We show that all properties of strings that are first-order definable on strings are definable on Lempel-Ziv compressed strings in FO(TC), the extension of first-order logic with the transitive closure operator. We define a subclass F of the first-order properties of strings such that if P is a property in F, it is also first-order definable on the Lempel-Ziv compressed strings. Monadic second-order properties of strings, i.e. regular languages, are dyadic second-order definable on Lempel-Ziv compressed strings.\"],\n",
       "   '2000']],\n",
       " [[['Models for Name-Passing Processes: Interleaving and Causal.',\n",
       "    'We study syntax-free models for name-passing processes. For interleaving semantics, we identify the indexing structure required of an early labelled transition system to support the usual π-calculus operations, defining Indexed Labelled Transition Systems. For non-interleaving causal semantics we define Indexed Labelled Asynchronous Transition Systems, smoothly generalizing both our interleaving model and the standard Asynchronous Transition Systems model for CCS-like calculi. In each case we relate a denotational semantics to an operational view, for bisimulation and causal bisimulation respectively. We establish completeness properties of, and adjunctions between, categories of the two models. Alternative indexing structures and possible applications are also discussed. These are first steps towards a uniform understanding of the semantics and operations of name-passing calculi.'],\n",
       "   '2000']],\n",
       " [[['Probabilistic Game Semantics.',\n",
       "    'A category of HO/N-style games and probabilistic strategies is developed where the possible choices of a strategy are quantified so as to give a measure of the likelihood of seeing a given play. A two-sided die is shown to be universal in this category, in the sense that any strategy breaks down into a composition between some deterministic strategy and that die. The interpretative power of the category is then demonstrated by delineating a Cartesian closed subcategory that provides a fully abstract model of a probabilistic extension of Idealized Algol.'],\n",
       "   '2000']],\n",
       " [[['Approximating Labeled Markov Processes.',\n",
       "    'Labelled Markov processes are probabilistic versions of labelled transition systems. In general, the state space of a labelled Markov process may be a continuum. In this paper, we study approximation techniques for continuousstate labelled Markov processes.We show that the collection of labelled Markov processes carries a Polish-space structure with a countable basis given by finite-state Markov chains with rational probabilities: thus permitting the approximation of quantitative observations (e.g., an integral of a continuous function) of a continuous-state labelled Markov process by the observations on finite-state Markov chains. The primary technical tools that we develop to reach these results are • A variant of a finite-model theorem for the modal logic used to characterize bisimulation, and • an isomorphism between the poset of Markov processes (ordered by simulation) with the ω-continuous dcpo Proc (defined as the solution of the recursive domain equation Proc = ΠL PPr(Proc)). The isomorphism between labelled Markov processes and Proc can be independently viewed as a full-abstraction result relating an operational (labelled Markov process) and a denotational (Proc) model and yields a logic complete for reasoning about simulation for continuous-state processes.'],\n",
       "   '2000']],\n",
       " [[['Back and Forth between Guarded and Modal Logics.',\n",
       "    'Guarded fixed-point logic &mu;GF extends the guarded fragment by means of least and greatest fixed points, and thus plays the same role within the domain of guarded logics as the modal &mu;-calculus plays within the modal domain. We provide a semantic characterization of &mu;GF within an appropriate fragment of second-order logic, in terms of invariance under guarded bisimulation. The corresponding characterization of the modal &mu;-calculus, due to Janin and Walukiewicz, is lifted from the modal to the guarded domain by means of model theoretic translations. Guarded second-order logic, the fragment of second-order logic which is introduced in the context of our characterization theorem, captures a natural and robust level of expressiveness with several equivalent characterizations. For a wide range of issues in guarded logics it may take up a role similar to that of monadic second-order in relation to modal logics. At the more general methodological level, the translations between the guarded and modal domains make the intuitive analogy between guarded and modal logics available as a tool in the further analysis of the model theory of guarded logics.'],\n",
       "   '2000']],\n",
       " [[['A Theory of Bisimulation for a Fragment of Concurrent ML with Local Names.',\n",
       "    \"Concurrent ML is an extension of Standard ML with π-calculus-like primitives for multithreaded programming. CML has a reduction semantics, but to date there has been no labelled transition system semantics provided for the entire language. In this paper, we present a labelled transition semantics for a fragment of CML called µvCML which includes features not covered before: dynamically generated local channels and thread identifiers. We show that weak bisimilarity for µvCML is a congruence, and coincides with barbed bisimulation congruence. We also provide a variant of Sangiorgi's normal bisimulation for µvCML, and show that this too coincides with bisimilarity.\"],\n",
       "   '2000']],\n",
       " [[['Efficient and Flexible Matching of Recursive Types.',\n",
       "    'Equality and subtyping of recursive types were studied in the 1990s by Amadio and Cardelli; Kozen, Palsberg, and Schwartzbach; Brandt and Henglein; and others. Potential applications include automatic generation of bridge code for multilanguage systems and type-based retrieval of software modules from libraries. In this paper, we present an efficient decision procedure for a notion of type equality that includes unfolding of recursive types, and associativity and commutativity of product types. Advocated by Auerbach, Barton, and Raghavachari, these properties enable flexible matching of types. For two types of size at most n, our algorithm takes O (n) iterations each of which takes O (n) time, for a total of O (n2) time. 2001 Elsevier Science'],\n",
       "   '2000']],\n",
       " [[['Assigning Types to Processes.',\n",
       "    'In wide area distributed systems it is now common for higher-order code to be transferred from one domain to another; the receiving host may initialise parameters and then execute the code in its local environment. In this paper we propose a fine-grained typing system for a higher-order -calculus which can be used to control the effect of such migrating code on local environments. Processes may be assigned different types depending on their intended use. This is in contrast to most of the previous work on typing processes where all processes are typed by a unique constant type, indicating essentially that they are well typed relative to a particular environment. Our fine-grained typing facilitates the management of access rights and provides host protection from potentially malicious behaviour. Our process type takes the form of an interface limiting the resources to which it has access and the types at which they may be used. Allowing resource names to appear both in process types and process terms, as interaction ports, complicates the typing system considerably. For the development of a coherent typing system, we use a kinding technique, similar to that used by the subtyping of the system F, and order-theoretic properties of our subtyping relation. Various examples of this paper illustrate the usage of our fine-grained process types in distributed systems.'],\n",
       "   '2000']],\n",
       " [[['An Expectant Chat About Script Maturity.',\n",
       "    \"Using scripts to automate common administrative tasks is a ubiquitous practice. Powerful scripting languages and approaches support seemingly 'efficient' scripting practices that actually compromise the robustness of our scripts, as well as indirectly detracting from the stability and maturity of our support infrastructure. This is especially true for scripts that automate complex interactive processes using the scripting tools Expect or Chat. I present a formal methodology for the design and implementation of interactive scripting that, with a little more effort than writing a simple Expect script, produces scripts with substantially improved robustness and permanence. My scripting tool Babble interprets a detailed structural description of an interactive session as a script. Using this declarative, fourth-generation language, one can craft interactive scripts that are easier to perfect, inherently more robust, easier to maintain over time, and self-documenting.\"],\n",
       "   '2000']],\n",
       " [[['Querying Inconsistent Databases.',\n",
       "    \"In this paper we consider the problem of answering queries consistently in the presence of inconsistent data, i.e. data violating integrity constraints. We propose a technique based on the rewriting of integrity constraints into disjunctive rules with two different forms of negation (negation as failure and classical negation). The disjunctive program can be used i) to generate 'repairs' for the database and ii) to produce consistent answers, i.e. maximal set of atoms which do not violate the constraints. We show that our technique is sound, complete and more general than techniques previously proposed.\"],\n",
       "   '2000']],\n",
       " [[['Binding-Time Analysis by Constraint Solving. A Modular and Higher-Order Approach for Mercury.',\n",
       "    'In this paper we present a binding-time analysis for the logic programming language Mercury. Binding-time analysis is a key analysis needed to perform off-line program specialisation. Our analysis deals with the higher-order aspects of Mercury, and is formulated by means of constraint normalisation. This allows (at least part of) the analysis to be performed on a modular basis.'],\n",
       "   '2000']],\n",
       " [[['Region Analysis and a pi-Calculus wiht Groups.',\n",
       "    \"We show that the typed region calculus of Tofte and Talpin can be encoded in a typed &pi;-calculus equipped with name groups and a novel effect analysis. In the region calculus, each boxed value has a statically determined region in which it is stored. Regions are allocated and de-allocated according to a stack discipline, thus improving memory management. The idea of name groups arose in the typed ambient calculus of Cardelli, Ghelli, and Gordon. There, and in our &pi;-calculus, each name has a statically determined group to which it belongs. Groups allow for type-checking of certain mobility properties, as well as effect analyses. Our encoding makes precise the intuitive correspondence between regions and groups. We propose a new formulation of the type preservation property of the region calculus, which avoids Tofte and Talpin's rather elaborate co-inductive formulation. We prove the encoding preserves the static and dynamic semantics of the region calculus. Our proof of the correctness of region de-allocation shows it to be a specific instance of a general garbage collection principle for the &pi;-calculus with effects. We propose new equational laws for letregion, analogous to scope mobility laws in the &pi;-calculus, and show them sound in our semantics.\"],\n",
       "   '2000']],\n",
       " [[['Reducing the Number of Solutions of NP Functions.',\n",
       "    'We study whether one can prune solutions from NP functions. Though it is known that, unless surprising complexity class collapses occur, one cannot reduce the number of accepting paths of NP machines, we nonetheless show that it often is possible to reduce the number of solutions of NP functions. For finite cardinality types, we give a sufficient condition for such solution reduction. We also give absolute and conditional necessary conditions for solution reduction, and in particular we show that in many cases solution reduction is impossible unless the polynomial hierarchy collapses.'],\n",
       "   '2000']],\n",
       " [[['Optimal Satisfiability for Propositional Calculi and Constraint Satisfaction Problems.',\n",
       "    'We consider the problems of finding the lexicographically minimal (or maximal) satisfying assignment of propositional formulas for different restricted classes of formulas. It turns out that for each class from our framework, these problems are either polynomial time solvable or complete for OptP. We also consider the problem of deciding if in the optimal assignment the largest variable gets value 1. We show that this problem is either in P or PNP complete.'],\n",
       "   '2000']],\n",
       " [[['A Hierarchy Result for Read-Once Branching Programs with Restricted Parity Nondeterminism.',\n",
       "    'Restricted branching programs are considered in complexity theory in order to study the space complexity of sequential computations and in applications as a data structure for Boolean functions. In this paper (⊕, k)-branching programs and (∨, k)-branching programs are considered, i.e., branching programs starting with a ⊕- (or ∨-)node with a fan-out of k whose successors are k read-once branching programs. This model is motivated by the investigation of the power of nondeterminism in branching programs and of similar variants that have been considered as a data structure. Lower bound methods and hierarchy results for polynomial size (⊕, k)- and (∨, k)-branching programs with respect to k are presented.'],\n",
       "   '2000']],\n",
       " [[['Distributed Object Implementations for Interactive Applications.',\n",
       "    \"As computers become pervasive in the home and community and homes become better connected, new applications will be deployed over the Internet. Interactive Distributed Applications involve users in multiple locations, across a wide area network, who interact and cooperate by manipulating shared objects. A timely response to user actions, which can potentially update the state of the objects, is an important requirement of interactive applications. Because of the inherent heterogeneity of the environment, distributed applications are built using technologies like distributed objects. Central server based implementations of distributed objects cannot meet the response time needs of interactive users because invocations are always subject to communication latencies. Our approach is to extend these technologies with aggressive caching and replication mechanisms to provide interactive response time and to improve scalability. A flexible caching framework is presented, where objects can be cached in an application specific manner. It provides multiple consistency protocols that enable tradeoffs between the consistency of a cached object's state at a particular client, and the communication resources available to the client. At runtime, clients can specify their consistency requirements via a Quality of Service specification interface that is meaningful at the application level. This paper presents the caching framework, its implementation and some preliminary performance results.\"],\n",
       "   '2000']],\n",
       " [[['Evaluating strategies and systems for content based indexing of person images on the Web.',\n",
       "    'Content based indexing of multimedia has always been a challenging task. The enormity and the diversity of the multimedia content on the web adds another dimension to this challenge. In this paper, we examine ways of combining visual and textual information for content based indexing of multimedia on the web. In particular, we examine different methods of combining evidences due to face detection, Text/HTML analysis and face recognition for identifying person images. We provide experimental evaluation of the following strategies: i) Face detection on the image followed by Text/HTML analysis of the containing page; ii) face detection followed by face recognition; iii) face detection followed by a linear combination of evidences due to text/HTML analysis and face recognition; and iv) face detection followed by a Dempster-Shafer combination of evidences due to text/HTML analysis and face recognition. These strategies were implemented in an automatic web search agent named Diogenes1 and compared against some well known web image search engines. The latter includes commercial systems such as Alta Vista, Lycos and Ditto, and a research prototype, WebSEEk. We report the results of our experimental retrievals where Diogenes outperformed these search engines for celebrity image queries in terms of average precision.'],\n",
       "   '2000']],\n",
       " [[['Virtual 3D camera composition from frame constraints.',\n",
       "    \"We have designed a graphical interface that enables 3D visual artists or developers of interactive 3D virtual environments to efficiently define sophisticated camera compositions by creating storyboard frames, indicating how a desired shot should appear. These storyboard frames are then automatically encoded into an extensive set of virtual camera constraints that capture the key visual composition elements of the storyboard frame. Visual composition elements include the size and position of a subject in a camera shot. A recursive heuristic constraint solver then searches the space of a given 3D virtual environment to determine camera parameter values which produce a shot closely matching the one in the given storyboard frame. The search method uses given ranges of allowable parameter values expressed by each constraint to reduce the size of the 7 Degree of Freedom search space of possible camera positions, aim direction vectors, and field of view angles. In contrast, some existing methods of automatically positioning cameras in 3D virtual environments rely on pre-defined camera placements that cannot account for unanticipated configurations and movement of objects or use program-like scripts to define constraint-based camera shots. For example, it is more intuitive to directly manipulate an object's size in the frame rather than editing a constraint script to specify that the object should cover 10% of the frame's area.\"],\n",
       "   '2000']],\n",
       " [[['Temporal links: recording and replaying virtual environments.',\n",
       "    'Virtual reality (VR) currently lacks the kinds of sophisticated production technologies that are commonly available for established media such as video and audio. This paper introduces the idea of temporal links, which provide a flexible mechanism for replaying past or recent recordings of virtual environments within other real-time virtual environments. Their flexibility arises from a combination of temporal, spatial and presentational properties. Temporal properties determine the relationship between time in a live environment and time in a recording, including the apparent speed and direction of replay. Spatial properties determine the spatial relationship between the environment and the recording. Presentational properties determine the appearance of the recording within the environment. These properties may be fixed, dynamically varied by an application, or directly controlled in real-time by users. Consequently, temporal links have a wide variety of potential uses, including supporting post-production tools for virtual environments, post-exercise debriefing in training simulators, and asynchranous communication such as VR email, as well as providing new forms of content for virtual worlds that refer to past activity. We define temporal links and their properties and describe their implementation in the MASSIVE-3 Collaborative Virtual Environment (CVE) system, focusing on the underlying record and replay mechanisms. We also demonstrate applications for adding new content to an existing virtual world, and a VR post-production editor.'],\n",
       "   '2000']],\n",
       " [[['A practical query-by-humming system for a large music database.',\n",
       "    \"A music retrieval system that accepts hummed tunes as queries is described in this paper. This system uses similarity retrieval because a hummed tune may contain errors. The retrieval result is a list of song names ranked according to the closeness of the match. Our ultimate goal is that the correct song should be first on the list. This means that eventually our system's similarity retrieval should allow for only one correct answer. The most significant improvement our system has over general query-by-humming systems is that all processing of musical information is done based on beats instead of notes. This type of query processing is robust against queries generated from erroneous input. In addition, acoustic information is transcribed and converted into relative intervals and is used for making feature vectors. This increases the resolution of the retrieval system compared with other general systems, which use only pitch direction information. The database currently holds over 10,000 songs, and the retrieval time is at most one second. This level of performance is mainly achieved through the use of indices for retrieval. In this paper, we also report on the results of music analyses of the songs in the database. Based on these results, new technologies for improving retrieval accuracy, such as partial feature vectors and or'ed retrieval among multiple search keys, are proposed. The effectiveness of these technologies is evaluated quantitatively, and it is found that the retrieval accuracy increases by more than 20% compared with the previous system [9]. Practical user interfaces for the system are also described.\"],\n",
       "   '2000']],\n",
       " [[['IRM: integrated region matching for image retrieval.',\n",
       "    'Content-based image retrieval using region segmentation has been an active research area. We present IRM (Integrated Region Matching), a novel similarity measure for region-based image similarity comparison. The targeted image retrieval systems represent an image by a set of regions, roughly corresponding to objects, which are characterized by features reflecting color, texture, shape, and location properties. The IRM measure for evaluating overall similarity between images incorporates properties of all the regions in the images by a region-matching scheme. Compared with retrieval based on individual regions, the overall similarity approach reduces the influence of inaccurate segmentation, helps to clarify the semantics of a particular region, and enables a simple querying interface for region-based image retrieval systems. The IRM has been implemented as a part of our experimental SIMPLIcity image retrieval system. The application to a database of about 200,000 general-purpose images shows exceptional robustness to image alterations such as intensity variation, sharpness variation, color distortions, shape distortions, cropping, shifting, and rotation. Compared with several existing systems, our system in general achieves more accurate retrieval at higher speed.'],\n",
       "   '2000']],\n",
       " [[['A situated computing framework for mobile and ubiquitous multimedia access using small screen and composite devices.',\n",
       "    'In recent years, small screen devices, such as cellular phones or Personal Digital Assistants (PDAs), enjoy phenomenal popularity. PDAs can be used to complement traditional computing systems to access personal multimedia information beyond the usage as digital organizers. However, due to the physical limitations accessing rich multimedia contents and diverse services using a single PDA is more difficult. Hence, the Situated Computing Framework (SCF) research project at Siemens Corporate Research (SCR) aims to develop a ubiquitous computing infrastructure that facilitates nomadic users to access rich multimedia contents using small screen devices. This paper describes a new distributed computing concept, the Small Screen/Composite Device (SS/CD) framework, which offers mobile users new classes of ubiquitous and mobile multimedia services without to limit the diversity and the richness of the provided services.'],\n",
       "   '2000']],\n",
       " [[['Giving meanings to WWW images.',\n",
       "    'Images are increasingly being embedded in HTML documents on the WWW. Such documents over the WWW essentially provides a rich source of image collection from which user can query. Interestingly, the semantics of these images are typically described by their surrounding text. Unfortunately, most WWW image search engines fail to exploit these image semantics and give rise to poor recall and precision performance. In this paper, we propose a novel image representation model called Weight ChainNet. Weight ChainNet is based on lexical chain that represents the semantics of an image from its nearby text. A new formula, called list space model, for computing semantic similarities is also introduced. To further improve the retrieval effectiveness, we also propose two relevance feedback mechanisms. We conducted an extensive performance study on a collection of 5000 images obtained from documents identified by more than 2000 URLs. Our results show that our models and methods outperform existing technique. Moreover, the relevant feedback mechanisms can lead to significantly better retrieval effectiveness.'],\n",
       "   '2000']],\n",
       " [[['Application performance in the QLinux multimedia operating system.',\n",
       "    'In this paper, we argue that conventional operating systems need to be enhanced with predictable resource management mechanisms to meet the diverse performance requirements of emerging multimedia and web applications. We present QLinux&mdash;a multimedia operating system based on the Linux kernel that meets this requirement. QLinux employs hierarchical schedulers for fair, predictable allocation of processor, disk and network bandwidth, and accounting mechanisms for appropriate charging of resource usage. We experimentally evaluate the efficacy of these mechanisms using benchmarks and real-world applications. Our experimental results show that (i) emerging applications can indeed benefit from predictable allocation of resources, and (ii) the overheads imposed by the resource allocation mechanisms in QLinux are small. For instance, we show that the QLinux CPU scheduler can provide predictable performance guarantees to applications such as web servers and MPEG players, albeit at the expense of increasing the scheduling overhead. We conclude from our experiments that the benefits due to the resource management mechanisms in QLinux outweigh their increased overheads, making them a practical choice for conventional operating systems.'],\n",
       "   '2000']],\n",
       " [[['A video-based rendering acceleration algorithm for interactive walkthroughs.',\n",
       "    'We present a new approach for faster rendering of large synthetic environments using video-based representations. We decompose the large environment into cells and pre-compute video based impostors using MPEG compression to represent sets of objects that are far from each cell. At runtime, we decode the MPEG streams and use rendering algorithms that provide nearly constant-time random access to any frame. The resulting system has been implemented and used for an interactive walkthrough of a model of a house with 260,000 polygons and realistic lighting and textures. It is able to render this model at 16 frames per second (an eightfold improvement over simpler algorithms) on average on a Pentium II PC with an off-the-shelf graphics card.'],\n",
       "   '2000']],\n",
       " [[['A window-based congestion control for reliable multicast based on TCP dynamics.',\n",
       "    \"The limitation of the current multicast model led to the development of embedded network assist and new service models for reliable transport. We show a viable solution for one of the hardest problems in reliable multicast &mdash; congestion control &mdash; through the deployment of a new forwarding service model, &ldquo;Breadcrumb forwarding service&rdquo; (BCFS). Our proposed reliable multicast transport, &ldquo;Rainbow&rdquo;, is built on top of this model. In our approach, each receiver maintains its own congestion window and individually runs window control modeled after TCP. To enhance Rainbow's scalability and support asynchronous receiver subscriptions, Rainbow utilizes Digital Fountain at the source. This allows receivers to exercise asynchronous and autonomous behavior while simultaneously enjoying the performance benefit of synchronous multicast communication with fast group establishment of BCFS. In this paper, we detail the congestion control of Rainbow and demonstrate its efficiency and scalability through simulation and analysis. According to simulation results, Rainbow shows more TCP-fair behavior than RLC, which is a TCP friendly congestion control scheme based on layered multicast.\"],\n",
       "   '2000']],\n",
       " [[['Advanced feature extraction for Keyblock-based image retrieval.',\n",
       "    'Keyblock, which is a new framework we proposed for content-based image retrieval, is a generalization of the text-based information retrieval technology in the image domain. In this framework, keyblocks, which are analogous to keywords in text document retrieval, can be constructed by exploiting the Vector Quantization (VQ) method which has been used for image compression. Then an image can be represented as a code matrix in which the elements are the indices of keyblocks in a codebook. Based on this image representation, information retrieval and database analysis techniques developed in the text domain can be generalized to image retrieval. In this paper, we propose new models named N-block models which are the generalization of the N-gram models in language modeling to extract comprehensive image features. The effort to capture context in a text document motivated the N-gram models. Similarly, the attempt to capture the content in an image motivates us to consider the correlations of keyblocks within an image. By comparing the performance of our approach with conventional techniques using color feature and wavelet texture feature, the experimental results demonstrate the effectiveness of these N-block models.'],\n",
       "   '2000']],\n",
       " [[['A scalable low-latency cache invalidation strategy for mobile environments.',\n",
       "    'Caching frequently accessed data items on the client side is an effective technique for improving performance in a mobile environment. Classical cache invalidation strategies are not suitable for mobile environments due to frequent disconnections and mobility of the clients. One attractive cache invalidation technique is based on invalidation reports (IRs). However, the IR-based cache invalidation solution has two major drawbacks, which have not been addressed in previous research. First, there is a long query latency associated with this solution since a client cannot answer the query until the next IR interval. Second, when the server updates a hot data item, all clients have to query the server and get the data from the server separately, which wastes a large amount of bandwidth. In this paper, we propose an IR-based cache invalidation algorithm, which can significantly reduce the query latency and efficiently utilize the broadcast bandwidth. Detailed analytical analysis and simulation experiments are carried out to evaluate the proposed methodology. Compared to previous IR-based schemes, our scheme can significantly improve the throughput and reduce the query latency, the number of uplink request, and the broadcast bandwidth requirements.'],\n",
       "   '2000']],\n",
       " [[['Directed diffusion: a scalable and robust communication paradigm for sensor networks.',\n",
       "    'Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is datacentric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.'],\n",
       "   '2000']],\n",
       " [[['Downlink scheduling in CDMA data networks.',\n",
       "    'Packet data is expected to dominate third generation wireless networks, unlike current generation voice networks. This opens up new and interesting problems. Physical and link layer issues have been studied extensively, while resource allocation and scheduling issues have not been addressed satisfactorily.In this work, we address resource management on the downlink of CDMA packet data networks. Network performance (for example, capacity) has been addressed, but user centric performance has not received much attention. Recently, various non-traditional scheduling schemes based on new metrics have been proposed, and target user performance (mostly without reference to wireless). We adapt these metrics to the CDMA context, and establish some new results for the offline scheduling problem. In addition, we modify a large class of online algorithms to work in our setup and conduct a wide range of experiments. Based on detailed simulations, we infer that: Algorithms which exploit &ldquo;request sizes&rdquo; seem to outperform those that do not. Among these, algorithms that also exploit channel conditions provide significantly higher network throughput.Depending on continuous or discretized bandwidth conditions, either pure time multiplexing or a combination of time and code multiplexing strikes an excellent balance between user satisfaction and network performance.Discrete bandwidth conditions can lead to degraded user level performance without much impact on network performance. We argue that the discretization needs to be fine tuned to address this shortcoming.'],\n",
       "   '2000']],\n",
       " [[['GPSR: greedy perimeter stateless routing for wireless networks.',\n",
       "    \"We present Greedy Perimeter Stateless Routing (GPSR), a novel routing protocol for wireless datagram networks that uses the positions of routers and a packet's destination to make packet forwarding decisions. GPSR makes greedy forwarding decisions using only information about a router's immediate neighbors in the network topology. When a packet reaches a region where greedy forwarding is impossible, the algorithm recovers by routing around the perimeter of the region. By keeping state only about the local topology, GPSR scales better in per-router state than shortest-path and ad-hoc routing protocols as the number of network destinations increases. Under mobility's frequent topology changes, GPSR can use local topology information to find correct new routes quickly. We describe the GPSR protocol, and use extensive simulation of mobile wireless networks to compare its performance with that of Dynamic Source Routing. Our simulations demonstrate GPSR's scalability on densely deployed wireless networks.\"],\n",
       "   '2000']],\n",
       " [[['A scalable location service for geographic ad hoc routing.',\n",
       "    \"GLS is a new distributed location service which tracks mobile node locations. GLS combined with geographic forwarding allows the construction of ad hoc mobile networks that scale to a larger number of nodes than possible with previous work. GLS is decentralized and runs on the mobile nodes themselves, requiring no fixed infrastructure. Each mobile node periodically updates a small set of other nodes (its location servers) with its current location. A node sends its position updates to its location servers without knowing their actual identities, assisted by a predefined ordering of node identifiers and a predefined geographic hierarchy. Queries for a mobile node's location also use the predefined identifier ordering and spatial hierarchy to find a location server for that node. Experiments using the ns simulator for up to 600 mobile nodes show that the storage and bandwidth requirements of GLS grow slowly with the size of the network. Furthermore, GLS tolerates node failures well: each failure has only a limited effect and query performance degrades gracefully as nodes fail and restart. The query performance of GLS is also relatively insensitive to node speeds. Simple geographic forwarding combined with GLS compares favorably with Dynamic Source Routing (DSR): in larger networks (over 200 nodes) our approach delivers more packets, but consumes fewer network resources.\"],\n",
       "   '2000']],\n",
       " [[['A new model for packet scheduling in multihop wireless networks.',\n",
       "    'The goal of packet scheduling disciplines is to achieve fair and maximum allocation of channel bandwidth. However, these two criteria can potentially be in conflict in a generic-topology multihop wireless network where a single logical channel is shared among multiple contending flows and spatial reuse of the channel bandwidth is possible. In this paper, we propose a new model for packet scheduling that addresses this conflict. The main results of this paper are the following: (a) a two-tier service model that provides a minimum &ldquo;fair&rdquo; allocation of the channel bandwidth for each packet flow and additionally maximizes spatial reuse of bandwidth, (b) an ideal centralized packet scheduling algorithm that realizes the above service model, and (c) a practical distributed backoff-based channel contention mechanism that approximates the ideal service within the framework of the CSMA/CA protocol.'],\n",
       "   '2000']],\n",
       " [[['Mitigating routing misbehavior in mobile ad hoc networks.',\n",
       "    \"This paper describes two techniques that improve throughput in an ad hoc network in the presence of nodes that agree to forward packets but fail to do so. To mitigate this problem, we propose categorizing nodes based upon their dynamically measured behavior. We use a watchdog that identifies misbehaving nodes and a pathrater that helps routing protocols avoid these nodes. Through simulation we evaluate watchdog and pathrater using packet throughput, percentage of overhead (routing) transmissions, and the accuracy of misbehaving node detection. When used together in a network with moderate mobility, the two techniques increase throughput by 17% in the presence of 40% misbehaving nodes, while increasing the percentage of overhead transmissions from the standard routing protocol's 9% to 17%. During extreme mobility, watchdog and pathrater can increase network throughput by 27%, while increasing the overhead transmissions from the standard routing protocol's 12% to 24%.\"],\n",
       "   '2000']],\n",
       " [[['Tracking mobile users with uncertain parameters.',\n",
       "    'A method of reducing the wireless cost of tracking mobile users with uncertain parameters is developed in this paper. Such uncertainty arises naturally in wireless networks, since an efficient user tracking is based on a prediction of its future call and mobility parameters. The conventional approach based on dynamic tracking is not reliable in the sense that inaccurate prediction of the user mobility parameters may significantly reduce the tracking efficiency. Unfortunately, such uncertainty is unavoidable for mobile users, especially for a bursty mobility pattern. The two main contributions of this study are a novel method for topology-independent distance tracking, and a combination of a distance-based tracking with a distance-sensitive timer that guarantees both efficiency and robustness. The expected wireless cost of tracking under the proposed method is significantly reduced, in comparison to the existing methods currently used in cellular networks. Furthermore, as opposed to other tracking methods, the worst case tracking cost is bounded from above and governed by the system, such that it outperforms the existing methods. The proposed strategy can be easily implemented, and it does not require a significant computational power from the user.'],\n",
       "   '2000']],\n",
       " [[['Using semantic caching to manage location dependent data in mobile computing.',\n",
       "    'Location-dependent applications are becoming very popular in mobile environments. To improve system performance and facilitate disconnection, caching is crucial to such applications. In this paper, a semantic caching scheme is used to access location dependent data in mobile computing. We first develop a mobility model to represent the moving behaviors of mobile users and formally define location dependent queries. We then investigate query processing and cache management strategies. The performance of the semantic caching scheme and its replacement strategy FAR is evaluated through a simulation study. Our results show that semantic caching is more flexible and effective for use in LDD applications than page caching, whose performance is quite sensitive to the database physical organization. We also notice that the semantic cache replacement strategy FAR, which utilizes the semantic locality in terms of locations, performs robustly under different kinds of workloads.'],\n",
       "   '2000']],\n",
       " [[['Dynamic power management for portable systems.',\n",
       "    'Portable systems require long battery lifetime while still delivering high performance. Dynamic power management (DPM) policies trade off the performance for the power consumption at the system level in portable devices. In this work we present the time-indexed SMDP model (TISMDP) that we use to derive optimal policy for DPM in portable systems. TISMDP model is needed to handle the non-exponential user request interarrival times we observed in practice. We use our policy to control power consumption on three different devices: the SmartBadge portable device [18], the Sony Vaio laptop hard disk and WLAN card. Simulation results show large savings for all three devices when using our algorithm. In addition, we measured the power consumption and performance of our algorithm and compared it with other DPM algorithms for laptop hard disk and WLAN card. The algorithm based on our TISMDP model has 1.7 times less power consumption as compared to the default Windows timeout policy for the hard disk and three times less power consumption as compared to the default algorithm for the WLAN card.'],\n",
       "   '2000']],\n",
       " [[['An end-to-end approach to host mobility.',\n",
       "    'We present the design and implementation of an end-to-end architecture for Internet host mobility using dynamic updates to the Domain Name System (DNS) to track host location. Existing TCP connections are retained using secure and efficient connection migration, enabling established connections to seamlessly negotiate a change in endpoint IP addresses without the need for a third party. Our architecture is secure&mdash;name updates are effected via the secure DNS update protocol, while TCP connection migration uses a novel set of Migrate options&mdash;and provides a pure end-system alternative to routing-based approaches such as Mobile IP. Mobile IP was designed under the principle that fixed Internet hosts and applications were to remain unmodified and only the underlying IP substrate should change. Our architecture requires no changes to the unicast IP substrate, instead modifying transport protocols and applications at the end hosts. We argue that this is not a hindrance to deployment; rather, in a significant number of cases, it allows for an easier deployment path than Mobile IP, while simultaneously giving better performance. We compare and contrast the strengths of end-to-end and network-layer mobility schemes, and argue that end-to-end schemes are better suited to many common mobile applications. Our performance experiments show that hand-off times are governed by TCP migrate latencies, and are on the order of a round-trip time of the communicating peers.'],\n",
       "   '2000']],\n",
       " [[['W-mail: an electronic mail system for wearable computing environments.',\n",
       "    \"This paper describes an e-mail system for wearable computing environments. In this system, we extend the conventional mail format and the server/client(browser) architecture by considering the specific features of wearable computing environments, i.e., full time operation, hands-free use of computer, and close relationship to our daily life. A mail author can specify the behavior of his/her mail by embedding several useful commands in the mail. A user can specify in the mail various conditions as commands such as time, location of the recipient, and status of various sensors to allow an adaptive behavior in the mail. We also describe other features of the system to support user's wearable computing life.\"],\n",
       "   '2000']],\n",
       " [[['Distributed fair scheduling in a wireless LAN.',\n",
       "    'Fairness is an important issue when accessing a shared wireless channel. With fair scheduling, it is possible to allocate bandwidth in proportion to weightsof the packet flows sharing the channel. This paper presents a fully distributed algorithm for fair scheduling in a wireless LAN. The algorithm can be implemented without using a centralized coordinator to arbitrate medium access. The proposed protocol is derived from the Distributed Coordination Function in the IEEE 802.11 standard. Simulation results show that the proposed algorithm is able to schedule transmission such that the bandwidth allocated to different flows is proportional to their weights. An attractive feature of the proposed approach is that it can be implemented with simple modifications to the IEEE 802.11 standard.'],\n",
       "   '2000']],\n",
       " [[['UPPAAL - Now, Next, and Future.',\n",
       "    'UPPAAL is a tool for modeling, simulation and verification of real-time systems, developed jointly by BRICS at Aalborg University and the Department of Computer Systems at Uppsala University. The tool is appropriate for systems that can be modeled as a collection of non-deterministic processes with finite control structure and real-valued clocks, communicating through channels or shared variables. Typical application areas include real-time controllers and communication protocols, in particular those where timing aspects are critical.This paper reports on the currently available version and summarizes developments during the last two years. We report on new directions that extends UPPAAL with cost-optimal exploration, parametric modeling, stop-watches, probablistic modeling, hierachical modeling, executable timed automata, and a hybrid automata animator. We also report on recent work to improve the efficiency of the tool. In particular, we outline Clock Difference Diagrams (CDDs), new compact representations of states, a distributed version of the tool, and application of dynamic partitioning.UPPAAL has been applied in a number of academic and industrial case studies. We describe a selection of the recent case studies.'],\n",
       "   '2000']],\n",
       " [[['Testing Transition Systems: An Annotated Bibliography.',\n",
       "    'Labelled transition system based test theory has made remarkable progress over the past 15 years. From a theoretically interesting approach to the semantics of reactive systems it has developed into a field where testing theory is (slowly) narrowing the gap with testing practice. In particular, new test generation algorithms are being designed that can be used in realistic situations whilst maintaining a sound theoretical basis. In this paper we present an annotated bibliography of labelled transition system based test theory and its applications covering the main developments.'],\n",
       "   '2000']],\n",
       " [[['HMSCs as Partial Specifications ... with PNs as Completions.',\n",
       "    'The paper presents ongoing work aiming at understanding the nature of specifications given by High Level Message Sequence Charts and the ways in which they can be put into effective use. Contrarily to some authors, we do not set finite state restrictions on HMSCs as we feel such restrictions do not fit in with the type of distributed systems encountered today in the field of telecommunications. The talk presents first a series of undecidability results about general HMSCs following from corresponding undecidability results on rational sets in product monoids. These negative results which with one exception do not appear yet in the literature on HMSCs do indicate that the sole way in which general HMSCs may be usefully handed as behavioural specifications is to interpret their linear extensions as minimal languages, to be approximated from above in any realization. The problem is then to investigate frameworks in which these incomplete specifications may be given a meaning by a closure operation. The second part of the paper presents a closure operation relative to Petri net languages. This closure operation is an effective procedure that relies on semilinear properties of HMSCs languages. We finally present some decidability results for the distribution and verification of HMSCs transformed into Petri nets.'],\n",
       "   '2000']],\n",
       " [[['Mobile Processes: A Commented Bibliography.',\n",
       "    'We propose a short bibliographic survey of calculi for mobile processes. Contrasting with other similar exercises, we consider two related, but distinct, notions of mobile processes, namely labile processes, which can exhibit dynamic changes in their interaction structure, as modelled in the π-calculus of Milner, Parrow and Walker for example, and motile processes, which can exhibit motion, as modelled in the ambient calculus of Cardelli and Gordon. A common characteristic of the algebraic frameworks presented in this paper is the use of names as first class values and the support for the dynamic generation of new, fresh names.'],\n",
       "   '2000']],\n",
       " [[['Model Checking: A Tutorial Overview.',\n",
       "    'We survey principles of model checking techniques for the automatic analysis of reactive systems. The use of model checking is exemplified by an analysis of the Needham-Schroeder public key protocol. We then formally define transition systems, temporal logic, ω-automata, and their relationship. Basic model checking algorithms for linear- and branching-time temporal logics are defined, followed by an introduction to symbolic model checking and partial-order reduction techniques. The paper ends with a list of references to some more advanced topics.'],\n",
       "   '2000']],\n",
       " [[['Fault Model-Driven Test Derivation from Finite State Models: Annotated Bibliography.',\n",
       "    'The annotated bibliography highlights work in the area of algorithmic test generation from formal specifications with guaranteed fault coverage, i.e., fault model-driven test derivation. A fault model is understood as a triple, comprising a finite state specification, conformance relation and fault domain that is the set of possible implementations. The fault model can be specialized to Input/Output FSM, Labeled Transition System, or Input/Output Automaton and to a number of conformance relations such as FSM equivalence, reduction or quasi-equivalence, trace inclusion or trace equivalence and others. The fault domain usually reflects test assumptions, as an example, it can be the universe of all possible I/O FSMs with a given number of states, a classical fault domain in FSM-based testing. A test suite is complete with respect to a given fault model when each implementation from the fault domain passes it if and only if the postulated conformance relation holds between the implementation and its specification. A complete test suite is said to provide fault coverage guarantee for a given fault model.'],\n",
       "   '2000']],\n",
       " [[['Theorem Proving for Verification.',\n",
       "    'The challenges in using theorem proving for verification of parallel systems are to achieve adequate automation, and to allow human guidance to be expressed in terms of the system under examination rather than the mechanisms of the prover. This paper provides an overview of techniques that address these challenges.'],\n",
       "   '2000']],\n",
       " [[['Composition and Abstraction.',\n",
       "    'This article is a tutorial on advanced automated process-algebraic verification of concurrent systems, and it is organised around a case study. The emphasis is on verification methods that rely on the inherent compositionality of process algebras. The fundamental concepts of labelled transition systems, strong bisimilarity, synchronous parallel composition, hiding, renaming, abstraction, CFFD-equivalence and CFFD-preorder are presented as the case study proceeds. The necessity of presenting assumptions about the users of the example system is discussed, and it is shown how CFFD-preorder supports their modelling. The assumptions are essential for the verification of so-called liveness properties. The correctness requirements of the system are stated, presented in linear temporal logic, and distributed to a number of more \"localised\" requirements. It is shown how they can be checked with the aid of suitably chosen CFFD-abstracted views to the system. The state explosion problem that hampers automatic verification is encountered. Compositional LTS construction, interface specifications and induction are used to solve the problem and, as a result, an infinite family of systems is verified with a limited amount of effort.'],\n",
       "   '2000']],\n",
       " [[['Multicast tree construction and flooding in wireless ad hoc networks.',\n",
       "    'In an ad hoc network, each host assumes the role of a router and relays packets toward final destinations. This paper studies efficient routing mechanisms for multicast and broadcast in ad hoc wireless networks. Because a packet is broadcast to all neighboring nodes, the optimality criteria of wireless network routing is different from that of wired network routing. In this paper, we point out that the number of packet forwarding is the more important cost factor than the number of links in the ad hoc network. After we show constructing minimum cost multicast tree is hard, we propose two new flooding methods, self pruning and dominant pruning. Both methods utilize neighbor information to reduce redundant transmissions. Performance analysis shows that both methods perform significantly better than blind flooding. Especially, dominant pruning performs close to the practically achievable best performance limit.'],\n",
       "   '2000']],\n",
       " [[['Policies for using replica groups and their effectiveness over the Internet.',\n",
       "    'Replication is known to offer high availability in the presence of failures. This paper considers the case of a client making invocations on a group of replicated servers. It identifies attributes that typically characterise group invocation and replica management, and the options generally available for each attribute. A combination of options on these attributes constitutes a policy. The paper proposes an implementation framework which, by its group-oriented nature, simplifies the task of supporting these policies. It then considers a client (in UCL, London) making invocations on a replica group (in Newcastle, UK) over the Internet. It evaluates the response latencies for four policies that seem appropriate for this set-up. The evaluation takes into account the timing of server crashes with respect to client invocations; both real and virtual failures are considered, the latter being not uncommon in the Internet environment. The experiments are carried out using a CORBA compliant system called NewTop.'],\n",
       "   '2000']],\n",
       " [[['A New Approximate Maximal Margin Classification Algorithm.',\n",
       "    \"A new incremental learning algorithm is described which approximates the maximal margin hyperplane w.r.t. norm p &ge; 2 for a set of linearly separable data. Our algorithm, called ALMA_p (Approximate Large Margin algorithm w.r.t. norm p), takes O( (p-1) / (&alpha;2 &gamma;2 ) ) corrections to separate the data with p-norm margin larger than (1-&alpha;)&gamma;, where g is the (normalized) p-norm margin of the data. ALMA_p avoids quadratic (or higher-order) programming methods. It is very easy to implement and is as fast as on-line algorithms, such as Rosenblatt's Perceptron algorithm. We performed extensive experiments on both real-world and artificial datasets. We compared ALMA_2 (i.e., ALMA_p with p = 2) to standard Support vector Machines (SVM) and to two incremental algorithms: the Perceptron algorithm and Li and Long's ROMMA. The accuracy levels achieved by ALMA_2 are superior to those achieved by the Perceptron algorithm and ROMMA, but slightly inferior to SVM's. On the other hand, ALMA_2 is quite faster and easier to implement than standard SVM training algorithms. When learning sparse target vectors, ALMA_p with p > 2 largely outperforms Perceptron-like algorithms, such as ALMA_2.\"],\n",
       "   '2000']],\n",
       " [[['Performance-driven hand-drawn animation.',\n",
       "    'We present a novel method for generating performance-driven, \"hand-drawn\" animation in real-time. Given an annotated set of hand-drawn faces for various expressions, our algorithm performs multi-way morphs to generate real-time animation that mimics the expressions of a user. Our system consists of a vision-based tracking component and a rendering component. Together, they form an animation system that can be used in a variety of applications, including teleconferencing, multi-user virtual worlds, compressed instructional videos, and consumer-oriented animation kits.This paper describes our algorithms in detail and illustrates the potential for this work in a teleconferencing application. Experience with our implementation suggests that there are several advantages to our hand-drawn characters over other alternatives: (1) flexibility of animation style; (2) increased compression of expression information; and (3) masking of errors made by the face tracking system that are distracting in photorealistic animations.'],\n",
       "   '2000']],\n",
       " [[['Guava: a dialect of Java without data races.',\n",
       "    \"We introduce Guava, a dialect of Java whose rules statically guarantee that parallel threads access shared data only through synchronized methods. Our dialect distinguishes three categories of classes: (1) monitors, which may be referenced from multiple threads, but whose methods are accessed serially; (2) values, which cannot be referenced and therefore are never shared; and (3) objects, which can have multiple references but only from within one thread, and therefore do not need to be synchronized. Guava circumvents the problems associated with today's Java memory model, which must define behavior when concurrent threads access shared memory without synchronization.We present an overview of the syntax and the semantic rules of Guava. We discuss how implementations of Guava can exploit these rules to re-enable compiler optimizations inhibited by standard Java. We discuss how compilers for certain multiprocessor architectures can automatically generate certain programming idioms, such as double-check reads, as optimizations of serialized monitors.\"],\n",
       "   '2000']],\n",
       " [[['Formal specification of CORBA services: experience and lessons learned.',\n",
       "    \"CORBA is now established as one of the main contenders in object-oriented middleware. Beyond the definition of this standard for distributed object systems, the Object Management Group (OMG) has specified several object services (Common Object Services, COS) that should foster the interoperability of distributed applications. Based on experiment, the goal of this paper is to show that the OMG's style of specification of the CORBA services is not suited to guarantee that implementers will produce interoperable and substitutable implementations. To illustrate our point, we give an account of an experiment based upon the formal specification of one COS, namely the CORBA Event Service. This formal specification highlights several ambiguities and under-specifications in the OMG document. We then test several commercial and public domain implementations of the CORBA Event Service, in order to assess how the implementers have dealt with these under-specifications. We show that the choices made by the implementers lead to incompatible implementations. We finally suggest a solution to overcome the problem of specification of object services, which satisfies the views of both implementers and users. Specifically, we suggest that the specification of such services be made using a formal description technique, and that implementers be provided with test cases derived from the formal specification.\"],\n",
       "   '2000']],\n",
       " [[['An approach to safe object sharing.',\n",
       "    'It is essential for security to be able to isolate mistrusting programs from one another, and to protect the host platform from programs. Isolation is difficult in object-oriented systems because objects can easily become aliased. Aliases that cross program boundaries can allow programs to exchange information without using a system provided interface that could control information exchange. In Java, mistrusting programs are placed in distinct loader spaces but uncontrolled sharing of system classes can still lead to aliases between programs. This paper presents the object spaces protection model for an object-oriented system. The model decomposes an application into a set of spaces, and each object is assigned to one space. All method calls between objects in different spaces are mediated by a security policy. An implementation of the model in Java is presented.'],\n",
       "   '2000']],\n",
       " [[['MultiJava: modular open classes and symmetric multiple dispatch for Java.',\n",
       "    'We present MultiJava, a backward-compatible extension to Java supporting open classes and symmetric multiple dispatch. Open classes allow one to add to the set of methods that an existing class supports without creating distinct subclasses or editing existing code. Unlike the \"Visitor\" design pattern, open classes do not require advance planning, and open classes preserve the ability to add new subclasses modularly and safely. Multiple dispatch offers several well-known advantages over the single dispatching of conventional object-oriented languages, including a simple solution to some kinds of \"binary method\" problems. MultiJava\\'s multiple dispatch retains Java\\'s existing class-based encapsulation properties. We adapt previous theoretical work to allow compilation units to be statically typechecked modularly and safely, ruling out any link-time or run-time type errors. We also present a n compilation scheme that operates modularly and incurs performance overhead only where open classes or multiple dispatching are actually used.'],\n",
       "   '2000']],\n",
       " [[['Finding refactorings via change metrics.',\n",
       "    'Reverse engineering is the process of uncovering the design and the design rationale from a functioning software system. Reverse engineering is an integral part of any successful software system, because changing requirements lead to implementations that drift from their original design. In contrast to traditional reverse engineering techniques ---which analyse a single snapshot of a system--- we focus the reverse engineering effort by determining where the implementation has changed. Since changes of object-oriented software are often phrased in terms of refactorings, we propose a set of heuristics for detecting refactorings by applying lightweight, object-oriented metrics to successive versions of a software system. We validate our approach with three separate case studies of mature object-oriented software systems for which multiple versions are available. The case studies suggest that the heuristics support the reverse engineering process by focusing attention on the relevant parts of a software system.'],\n",
       "   '2000']],\n",
       " [[['An efficient class and object encoding.',\n",
       "    'An object encoding translates a language with object primitives to one without. Similarly, a class encoding translates classes into other primitives. Both are important theoretically for comparing the expressive power of languages and for transferring results from traditional languages to those with objects and classes. Both are also important foundations for the implementation of object-oriented languages as compilers typically include a phase that performs these translations.This paper describes a language with a primitive notion of classes and objects and presents an encoding of this language into one with records and functions. The encoding uses two techniques often used in compilers for single-inheritance class-based object-oriented languages: the self-application semantics and the method-table technique. To type the output of the encoding, the encoding uses a new formulation of self quantifiers that is more powerful than previous approaches.'],\n",
       "   '2000']],\n",
       " [[['PIROL: a case study for multidimensional separation of concerns in software engineering environments.',\n",
       "    'In this paper, we present our experience with applying multidimensional separation of concerns to a software engineering environment. By comparing two different designs of our system, we show the importance of separating integration issues from the implementation of the individual concerns. We present a model in which integration issues are encapsulated into rst--class connector objects and indicate how this facilitates the understandability, maintenance and evolution of the system. We identify issues of binding time, binding granularity and binding cardinality as important criteria in selecting an appropriate model for separation of concerns. We finally show how a good choice following these criteria and considering the requirements of software engineering environments leads to a system with dynamic configurability, high--level component integration and support for multiple instantiable views.'],\n",
       "   '2000']],\n",
       " [[['A study of devirtualization techniques for a Java Just-In-Time compiler.',\n",
       "    'Many devirtualization techniques have been proposed to reduce the runtime overhead of dynamic method calls for various object-oriented languages, however, most of them are less effective or cannot be applied for Java in a straightforward manner. This is partly because Java is a statically-typed language and thus transforming a dynamic call to a static one does not make a tangible performance gain (owing to the low overhead of accessing the method table) unless it is inlined, and partly because the dynamic class loading feature of Java prohibits the whole program analysis and optimizations from being applied.We propose a new technique called direct devirtualization with the code patching mechanism. For a given dynamic call site, our compiler first determines whether the call can be devirtualized, by analyzing the current class hierarchy. When the call is devirtualizable and the target method is suitably sized, the compiler generates the inlined code of the method, together with the backup code of making the dynamic call. Only the inlined code is actually executed until our assumption about the devirtualization becomes invalidated, at which time the compiler performs code patching to make the backup code executed subsequently. Since the new technique prevents some code motions across the merge point between the inlined code and the backup code, we have furthermore implemented recently-known analysis techniques, such as type analysis and preexistence analysis, which allow the backup code to be completely eliminated. We made various experiments using 16 real programs to understand the effectiveness and characteristics of the devirtualization techniques in our Java Just-In-Time (JIT) compiler. In summary, we reduced the number of dynamic calls by ranging from 8.9% to 97.3% (the average of 40.2%), and we improved the execution performance by ranging from -1% to 133% (with the geometric mean of 16%).'],\n",
       "   '2000']],\n",
       " [[['OoLALA: an object oriented analysis and design of numerical linear algebra.',\n",
       "    \"In this paper we review the design of a sequential object oriented linear algebra library, OOLALA. Several designs are proposed and used to classify existing sequential object oriented libraries. The classification is based on the way that matrices and matrix operations are represented. OOLALA's representation of matrices is capable of dealing with certain matrix operations that, although mathematically valid, are not handled correctly by existing libraries. OOLALA also enables implementations of matrix calculations at various abstraction levels ranging from the relatively low-level abstraction of a Fortran BLAS-like implementation to higher-level abstractions that hide many implementation details. OOLALA addresses a wide range of numerical linear algebra functionality while the reviewed object oriented libraries concen trate on parts of such functionality. We include some preliminary performance results for a Java implementation of OOLALA.\"],\n",
       "   '2000']],\n",
       " [[['Exclusion for composite objects.',\n",
       "    'Designing concurrent object-oriented programs is hard. Correct programs must coordinate multiple threads accessing composite objects, using low-level mechanisms such as locks and read-write sets. Efficient programs must balance the complexity and overhead of the coordination mechanisms against the increased performance possible through concurrency. A method-level algebra of exclusion provides a succinct description of the conditions under which a thread must be excluded from a component of a composite object. Using the algebra, programmers can check whether their programs meet their exclusion requirements, can eliminate redundant exclusion controls, and can remove synchronisation overhead by reducing concurrency.'],\n",
       "   '2000']],\n",
       " [[['Towards agent-oriented assistance for framework instantiation.',\n",
       "    'In this work we present a tool for assisting object-oriented framework instantiation based on Intelligent Agent technology. Differently from other approaches, the user is able to select the functionality needed for the new application, and based on this selection an agent elaborates a sequence of programming activities that should be carried out in order to implement it. In addition, the agent guides the execution of the activities according to the framework design. To enable this behavior, the framework need to be documented following the SmartBooks method, which extends traditional framework documentation with instantiation rules. In this paper we present an example of an instantiation environment built based on these ideas and the main characteristics of the SmartBooks method for documenting frameworks through instantiation knowledge rules.'],\n",
       "   '2000']],\n",
       " [[['An Aristotelian understanding of object-oriented programming.',\n",
       "    'The folklore of the object-oriented programming community at times maintains that object-oriented programming has drawn inspiration from philosophy, specifically that of Aristotle. We investigate this relation, first of all, in the hope of attaining a better understanding of object-oriented programming and, secondly, to explain aspects of Aristotelian logic to the computer science research community (since it differs from first order predicate calculus in a number of important ways). In both respects we endeavour to contribute to the theory of objects, albeit in a more philosophical than mathematical fashion.'],\n",
       "   '2000']],\n",
       " [[['Safely creating correct subclasses without seeing superclass code.',\n",
       "    \"A major problem for object-oriented frameworks and class libraries is how to provide enough information about a superclass, so programmers can safely create new subclasses without giving away the superclass's code. Code inherited from the superclass can call down to methods of the subclass, which may cause nontermination or unexpected behavior. We describe a reasoning technique that allows programmers, who have no access to the code of the superclass, to determine both how to safely override the superclass's methods and when it is safe to call them. The technique consists of a set of rules and some new forms of specification. Part of the specification would be generated automatically by a tool, a prototype of which is planned for the formal specification language JML. We give an example to show the kinds of problems caused by method overrides and how our technique can be used to avoid them. We also argue why the technique is sound and give guidelines for library providers and programmers that greatly simplify reasoning about how to avoid problems caused by method overrides.\"],\n",
       "   '2000']],\n",
       " [[['Quicksilver: a quasi-static compiler for Java.',\n",
       "    'This paper presents the design and implementation of the Quicksilver1 quasi-static compiler for Java. Quasi-static compilation is a new approach that combines the benefits of static and dynamic compilation, while maintaining compliance with the Java standard, including support of its dynamic features. A quasi-static compiler relies on the generation and reuse of persistent code images to reduce the overhead of compilation during program execution, and to provide identical, testable and reliable binaries over different program executions. At runtime, the quasi-static compiler adapts pre-compiled binaries to the current JVM instance, and uses dynamic compilation of the code when necessary to support dynamic Java features. Our system allows interprocedural program optimizations to be performed while maintaining binary compatibility. Experimental data obtained using a preliminary implementation of a quasi-static compiler in the Jalape&ntilde;o JVM clearly demonstrates the benefits of our approach: we achieve a runtime compilation cost comparable to that of baseline (fast, non-optimizing) compilation, and deliver the runtime program performance of the highest optimization level supported by the Jalape&ntilde;o optimizing compiler. For the SPECjvm98 benchmark suite, we obtain a factor of 104 to 158 reduction in the runtime compilation overhead relative to the Jalape&ntilde;o optimizing compiler. Relative to the better of the baseline and the optimizing Jalape&ntilde;o compilers, the overall performance (taking into account both runtime compilation and execution costs) is increased by 9.2% to 91.4% for the SPECjvm98 benchmarks with size 100, and by 54% to 356% for the (shorter running) SPECjvm98 benchmarks with size 10.'],\n",
       "   '2000']],\n",
       " [[['Practical virtual method call resolution for Java.',\n",
       "    'This paper addresses the problem of resolving virtual method and interface calls in Java bytecode. The main focus is on a new practical technique that can be used to analyze large applications. Our fundamental design goal was to develop a technique that can be solved with only one iteration, and thus scales linearly with the size of the program, while at the same time providing more accurate results than two popular existing linear techniques, class hierarchy analysis and rapid type analysis.We present two variations of our new technique, variable-type analysis and a coarser-grain version called declared-type analysis. Both of these analyses are inexpensive, easy to implement, and our experimental results show that they scale linearly in the size of the program.We have implemented our new analyses using the Soot frame-work, and we report on empirical results for seven benchmarks. We have used our techniques to build accurate call graphs for complete applications (including libraries) and we show that compared to a conservative call graph built using class hierarchy analysis, our new variable-type analysis can remove a significant number of nodes (methods) and call edges. Further, our results show that we can improve upon the compression obtained using rapid type analysis.We also provide dynamic measurements of monomorphic call sites, focusing on the benchmark code excluding libraries. We demonstrate that when considering only the benchmark code, both rapid type analysis and our new declared-type analysis do not add much precision over class hierarchy analysis. However, our finer-grained variable-type analysis does resolve significantly more call sites, particularly for programs with more complex uses of objects.'],\n",
       "   '2000']],\n",
       " [[['Scalable propagation-based call graph construction algorithms.',\n",
       "    'Propagation-based call graph construction algorithms have been studied intensively in the 199Os, and differ primarily in the number of sets that are used to approximate run-time values of expressions. In practice, algorithms such as RTA that use a single set for the whole program scale well. The scalability of algorithms such as 0-CFA that use one set per expression remains doubtful.In this paper, we investigate the design space between RTA and 0-CFA. We have implemented various novel algorithms in the context of Jax, an application extractor for Java, and shown that they all scale to a 325,000-line program. A key property of these algorithms is that they do not analyze values on the run-time stack, which makes them efficient and easy to implement. Surprisingly, for detecting unreachable methods, the inexpensive RTA algorithm does almost as well as the seemingly more powerful algorithms. However, for determining call sites with a single target, one of our new algorithms obtains the current best tradeoff between speed and precision.'],\n",
       "   '2000']],\n",
       " [[['Parametric polymorphism in Java: an approach to translation based on reflective features.',\n",
       "    \"The introduction of parametric polymorphism in Java with translation approaches has been shown to be of considerable interest, allowing the definition of extensions of Java on top of the existing Virtual Machines. Homogeneous translations furthermore, seem to be more useful than heterogeneous, avoiding the continuous increase of library code with redundant information. At this time however, homogeneous approaches aren't as flexible as heterogeneous, with extensions failing to integrate well with base language typing. In this paper, using some of the features of the Core Reflection of Java, we introduce a homogeneous translation in which run-time information about instantiation of type-parameters is carried, allowing full integration of parame-terized types with Java typing. Performance overhead is greatly decreased using a brand new translation technique based on the deferring of the management of type information at load-time. The same power and flexibility of previous heterogeneous approaches is obtained while maintaining homogeneous translation advantages.\"],\n",
       "   '2000']],\n",
       " [[['Sealed calls in Java packages.',\n",
       "    'Determining the potential targets of virtual method invocations is essential for inter-procedural optimizations of object-oriented programs. It is generally hard to determine such targets accurately. The problem is especially difficult for dynamic languages such as Java, because additional targets of virtual calls may appear at runtime. Current mechanisms that enable inter-procedural optimizations for dynamic languages, repeatedly validate the optimizations at runtime. This paper addresses this predicament by proposing a novel technique for conservative devirtualization analysis, which applies to a significant number of virtual calls in Java programs. Unlike previous work, our technique requires neither whole program analysis nor runtime information, and incurs no runtime overhead. Our solution is very efficient to compute and is based on a newly introduced, seemingly unrelated security feature of Java file archives. On average, our analysis \"seals\" (safely devirtualizes) about 39% of the virtual calls (to non-final methods) that appear in SPECjvm98 programs, and about 29% of the calls invoked while executing these programs. In the runtime library rt.jar, about 10% of the packages contain a significant percentage (20--60%) of sealed calls, with a total average of about 8.5%. Most of these calls are also shown to be monomorphic, a fact which can be safely exploited by aggressive inter-procedural optimizations such as direct inlining. These results indicate that our technique has a strong potential for enhancing the analysis and optimization of Java programs.'],\n",
       "   '2000']],\n",
       " [[['Tolerating Transient and Intermittent Failure.',\n",
       "    'Fault tolerance is a crucial property for recent distributed systems. We propose an algorithm that solves the census problem (list all processor identifiers and their relative distance) on an arbitrary strongly connected network.This algorithm tolerates transient faults that corrupt the processors and communication links memory (it is self-stabilizing) as well as intermittent faults (fair loss, reorder, finite duplication of messages) on communication media. A formal proof establishes its correctness for the considered problem. Our algorithm leads to the construction of algorithms for any silent problems that are self-stabilizing while supporting the same communication hazards.'],\n",
       "   '2000']],\n",
       " [[['Design and Evaluation of a Continuous Consistency Model for Replicated Services.',\n",
       "    'The tradeoffs between consistency, performance, and availability are well understood. Traditionally, however, designers of replicated systems have been forced to choose from either strong consistency guarantees or none at all. This paper explores the semantic space between traditional strong and optimistic consistency models for replicated services. We argue that an important class of applications can tolerate relaxed consistency, but benefit from bounding the maximum rate of inconsistent access in an application-specific manner. Thus, we develop a set of metrics, Numerical Error, Order Error, and Staleness, to capture the consistency spectrum. We then present the design and implementation of TACT, a middleware layer that enforces arbitrary consistency bounds among replicas using these metrics. Finally, we show that three replicated applications demonstrate significant semantic and performance benefits from using our framework.'],\n",
       "   '2000']],\n",
       " [[['End-to-End Authorization.',\n",
       "    'Many boundaries impede the flow of authorization information, forcing applications that span those boundaries into hop-by-hop approaches to authorization. We present a unified approach to authorization. Our approach allows applications that span administrative, network, abstraction, and protocol boundaries to understand the end-to-end authority that justifies any given request. The resulting distributed systems are more secure and easier to audit. We describe boundaries that can interfere with end-to-end authorization, and outline our unified approach. We describe the system we built and the applications we adapted to use our unified authorization system, and measure its costs. We conclude that our system is a practical approach to the desirable goal of end-to-end authorization.'],\n",
       "   '2000']],\n",
       " [[['System Support for Bandwidth Management and Content Adaptation in Internet Applications.',\n",
       "    'This paper describes the implementation and evaluation of an operating system module, the Congestion Manager (CM), which provides integrated network flow management and exports a convenient programming interface that allows applications to be notified of, and adapt to, changing network conditions. We describe the API by which applications interface with the CM, and the architectural considerations that factored into the design. To evaluate the architecture and API, we describe our implementations of TCP; a streaming layered audio/video application; and an interactive audio application using the CM, and show that they achieve adaptive behavior without incurring much end-system overhead. All flows including TCP benefit from the sharing of congestion information, and applications are able to incorporate new functionality such as congestion control and adaptive behavior.'],\n",
       "   '2000']],\n",
       " [[['Interposed Request Routing for Scalable Network Storage.',\n",
       "    \"This paper explores interposed request routing in Slice, a new storage system architecture for high-speed networks incorporating network-attached block storage. Slice interposes a request switching filter - called a µproxy - along each client's network path to the storage service (e.g., in a network adapter or switch). The µproxy intercepts request traffic and distributes it across a server ensemble. We propose request routing schemes for I/O and file service traffic, and explore their effect on service structure. The Slice prototype uses a packet filter µproxy to virtualize the standard Network File System (NFS) protocol, presenting to NFS clients a unified shared file volume with scalable bandwidth and capacity. Experimental results from the industry-standard SPECsfs97 workload demonstrate that the architecture enables construction of powerful network-attached storage services by aggregating cost-effective components on a switched Gigabit Ethernet LAN.\"],\n",
       "   '2000']],\n",
       " [[['Processes in KaffeOS: Isolation, Resource Management, and Sharing in Java.',\n",
       "    'Single-language runtime systems, in the form of Java virtual machines, are widely deployed platforms for executing untrusted mobile code. These runtimes provide some of the features that operating systems provide: inter-application memory protection and basic system services. They do not, however, provide the ability to isolate applications from each other, or limit their resource consumption. This paper describes KaffeOS, a Java runtime system that provides these features. The KaffeOS architecture takes many lessons from operating system design, such as the use of a user/kernel boundary, and employs garbage collection techniques, such as write barriers. The KaffeOS architecture supports the OS abstraction of a process in a Java virtual machine. Each process executes as if it were run in its own virtual machine, including separate garbage collection of its own heap. The difficulty in designing KaffeOS lay in balancing the goals of isolation and resource management against the goal of allowing direct sharing of objects. Overall, KaffeOS is no more than 11% slower than the freely available JVM on which it is based, which is an acceptable penalty for the safety that it provides. Because of its implementation base, KaffeOS is substantially slower than commercial JVMs for trusted code, but it clearly outperforms those JVMs in the presence of denial-of-service attacks or misbehaving code.'],\n",
       "   '2000']],\n",
       " [[['Towards Higher Disk Head Utilization: Extracting \"Free\" Bandwidth from Busy Disk Drives.',\n",
       "    \"Freeblock scheduling is a new approach to utilizing more of a disk's potential media bandwidth. By filling rotational latency periods with useful media transfers, 20-50% of a never-idle disk's bandwidth can often be provided to background applications with no effect on foreground response times. This paper describes freeblock scheduling and demonstrates its value with simulation studies of two concrete applications: segment cleaning and data mining. Free segment cleaning often allows an LFS file system to maintain its ideal write performance when cleaning overheads would otherwise reduce performance by up to a factor of three. Free data mining can achieve over 47 full disk scans per day on an active transaction processing system, with no effect on its disk performance.\"],\n",
       "   '2000']],\n",
       " [[['How to Build a Trusted Database System on Untrusted Storage.',\n",
       "    'Some emerging applications require programs to maintain sensitive state on untrusted hosts. This paper presents the architecture and implementation of a trusted database system, TDB, which leverages a small amount of trusted storage to protect a scalable amount of untrusted storage. The database is encrypted and validated against a collision-resistant hash kept in trusted storage, so untrusted programs cannot read the database or modify it undetectably. TDB integrates encryption and hashing with a low-level data model, which protects data and metadata uniformly, unlike systems built on top of a conventional database system. The implementation exploits synergies between hashing and log-structured storage. Preliminary performance results show that TDB outperforms an off-the-shelf embedded database system, thus supporting the suitability of the TDB architecture.'],\n",
       "   '2000']],\n",
       " [[['Latency Management in Storage Systems.',\n",
       "    'Storage Latency Estimation Descriptors, or SLEDs, are an API that allow applications to understand and take advantage of the dynamic state of a storage system. By accessing data in the file system cache or high-speed storage first, total I/O workloads can be reduced and performance improved. SLEDs report estimated data latency, allowing users, system utilities, and scripts to make file access decisions based on those retrieval time estimates. SLEDs thus can be used to improve individual application performance, reduce system workloads, and improve the user experience with more predictable behavior. We have modified the Linux 2.2 kernel to support SLEDs, and several Unix utilities and astronomical applications have been modified to use them. As a result, execution times of the Unix utilities when data file sizes exceed the size of the file system buffer cache have been reduced from 50% up to more than an order of magnitude. The astronomical applications incurred 30-50% fewer page faults and reductions in execution time of 10-35%. Performance of applications which use SLEDs also degrade more gracefully as data file size grows.'],\n",
       "   '2000']],\n",
       " [[['Taming the Memory Hogs: Using Compiler-Inserted Releases to Manage Physical Memory Intelligently.',\n",
       "    'Out-of-core applications consume physical resources at a rapid rate, causing interactive applications sharing the same machine to exhibit poor response times. This behavior is the result of default resource management strategies in the OS that are inappropriate for memory-intensive applications. Using an approach that integrates compiler analysis with simple OS support and a run-time layer that adapts to dynamic conditions, we have shown that the impact of out-of-core applications on interactive ones can be greatly mitigated. A combination of prefetching pages that will soon be needed, and releasing pages no longer in use results in good throughput for the out-of-core task and good response time for the interactive one. Each class of application performs well according to the metric most important to it. In addition, the OS does not need to attempt to identify these application classes, or modify its default resource management policies in any way. We also observe that when an out-of-core application releases pages, it both improves the response time of interactive tasks, and also improves its own performance through better replacement decisions and reduced memory management overhead.'],\n",
       "   '2000']],\n",
       " [[['Proactive Recovery in a Byzantine-Fault-Tolerant System.',\n",
       "    'This paper describes an asynchronous state-machine replication system that tolerates Byzantine faults, which can be caused by malicious attacks or software errors. Our system is the first to recover Byzantine-faulty replicas proactively and it performs well because it uses symmetric rather than public-key cryptography for authentication. The recovery mechanism allows us to tolerate any number of faults over the lifetime of the system provided fewer than 1/3 of the replicas become faulty within a window of vulnerability that is small under normal conditions. The window may increase under a denial-of-service attack but we can detect and respond to such attacks. The paper presents results of experiments showing that overall performance is good and that even a small window of vulnerability has little impact on service latency.'],\n",
       "   '2000']],\n",
       " [[['Surplus Fair Scheduling: A Proportional-Share CPU Scheduling Algorithm for Symmetric Multiprocessors.',\n",
       "    'In this paper, we present surplus fair scheduling (SFS), a proportional-share CPU scheduler designed for symmetric multiprocessors. We first show that the infeasibility of certain weight assignments in multiprocessor environments results in unfairness or starvation in many existing proportional-share schedulers. We present a novel weight readjustment algorithm to translate infeasible weight assignments to a set of feasible weights. We show that weight readjustment enables existing proportional-share schedulers to significantly reduce, but not eliminate, the unfairness in their allocations. We then present surplus fair scheduling, a proportional-share scheduler that is designed explicitly for multiprocessor environments. We implement our scheduler in the Linux kernel and demonstrate its efficacy through an experimental evaluation. Our results show that SFS can achieve proportionate allocation, application isolation and good interactive performance, albeit at a slight increase in scheduling overhead. We conclude from our results that a proportional-share scheduler such as SFS is not only practical but also desirable for server operating systems.'],\n",
       "   '2000']],\n",
       " [[['Performance-Driven Processor Allocation.',\n",
       "    'In current multiprogrammed multiprocessor systems, to take into account the performance of parallel applications is critical to decide an efficient processor allocation. In this paper, we present the Performance-Driven Processor Allocation policy (PDPA). PDPA is a new scheduling policy that implements a processor allocation policy and a multiprogramming-level policy, in a coordinated way, based on the measured application performance. With regard to the processor allocation, PDPA is a dynamic policy that allocates to applications the maximum number of processors to reach a given target efficiency. With regard to the multiprogramming level, PDPA allows the execution of a new application when free processors are available and the allocation of all the running applications is stable, or if some applications show bad performance. Results demonstrate that PDPA automatically adjusts the processor allocation of parallel applications to reach the specified target efficiency, and that it adjusts the multiprogramming level to the workload characteristics. PDPA is able to adjust the processor allocation and the multiprogramming level without human intervention, which is a desirable property for self-configurable systems, resulting in a better individual application response time.'],\n",
       "   '2000']],\n",
       " [[['Knit: Component Composition for Systems Software.',\n",
       "    'Knit is a new component definition and linking language for systems code. Knit helps make C code more understandable and reusable by third parties, helps eliminate much of the performance overhead of componentization, detects subtle errors in component composition that cannot be caught with normal component type systems, and provides a foundation for developing future analyses over C-based components, such as cross-component optimization. The language is especially designed for use with component kits, where standard linking tools provide inadequate support for component configuration. In particular, we developed Knit for use with the OSKit, a large collection of components for building low-level systems. However, Knit is not OSKit-specific, and we have implemented parts of the Click modular router in terms of Knit components to illustrate the expressiveness and flexibility of our language. This paper provides an overview of the Knit language and its applications.'],\n",
       "   '2000']],\n",
       " [[['Checking System Rules Using System-Specific, Programmer-Written Compiler Extensions.',\n",
       "    'Systems software such as OS kernels, embedded systems, and libraries must obey many rules for both correctness and performance. Common examples include \"accesses to variable A must be guarded by lock B,\" \"system calls must check user pointers for validity before using them,\" and \"message handlers should free their buffers as quickly as possible to allow greater parallelism.\" Unfortunately, adherence to these rules is largely unchecked. This paper attacks this problem by showing how system implementors can use meta-level compilation (MC) to write simple, system-specific compiler extensions that automatically check their code for rule violations. By melding domain-specific knowledge with the automatic machinery of compilers, MC brings the benefits of language-level checking and optimizing to the higher, \"meta\" level of the systems implemented in these languages. This paper demonstrates the effectiveness of the MC approach by applying it to four complex, real systems: Linux, OpenBSD, the Xok exokernel, and the FLASH machine\\'s embedded software. MC extensions found roughly 500 errors in these systems and led to numerous kernel patches. Most extensions were less than a hundred lines of code and written by implementors who had a limited understanding of the systems checked.'],\n",
       "   '2000']],\n",
       " [[['Self-Securing Storage: Protecting Data in Compromised Systems.',\n",
       "    'Self-securing storage prevents intruders from undetectably tampering with or permanently deleting stored data. To accomplish this, self-securing storage devices internally audit all requests and keep old versions of data for a window of time, regardless of the commands received from potentially compromised host operating systems. Within the window, system administrators have this valuable information for intrusion diagnosis and recovery. Our implementation, called S4, combines log-structuring with journal-based metadata to minimize the performance costs of comprehensive versioning. Experiments show that self-securing storage devices can deliver performance that is comparable with conventional storage systems. In addition, analyses indicate that several weeks worth of all versions can reasonably be kept on state-of-the-art disks, especially when differencing and compression technologies are employed.'],\n",
       "   '2000']],\n",
       " [[['Scalable, Distributed Data Structures for Internet Service Construction.',\n",
       "    'This paper presents a new persistent data management layer designed to simplify cluster-based Internet service construction. This self-managing layer, called a distributed data structure (DDS), presents a conventional single-site data structure interface to service authors, but partitions and replicates the data across a cluster. We have designed and implemented a distributed hash table DDS that has properties necessary for Internet services (incremental scaling of throughput and data capacity, fault tolerance and high availability, high concurrency, consistency, and durability). The hash table uses two-phase commits to present a coherent view of its data across all cluster nodes, allowing any node to service any task. We show that the distributed hash table simplifies Internet service construction by decoupling service-specific logic from the complexities of persistent, consistent state management, and by allowing services to inherit the necessary service properties from the DDS rather than having to implement the properties themselves. We have scaled the hash table to a 128 node cluster, 1 terabyte of storage, and an in-core read throughput of 61,432 operations/s and write throughput of 13,582 operations/s.'],\n",
       "   '2000']],\n",
       " [[['Exploring Failure Transparency and the Limits of Generic Recovery.',\n",
       "    'We explore the abstraction of failure transparency in which the operating system provides the illusion of failure-free operation. To provide failure transparency, an operating system must recover applications after hardware, operating system, and application failures, and must do so without help from the programmer or unduly slowing failure-free performance. We describe two invariants that must be upheld to provide failure transparency: one that ensures sufficient application state is saved to guarantee the user cannot discern failures, and another that ensures sufficient application state is lost to allow recovery from failures affecting application state. We find that several real applications get failure transparency in the presence of simple stop failures with overhead of 0-12%. Less encouragingly, we find that applications violate one invariant in the course of upholding the other for more than 90% of application faults and 3-15% of operating system faults, rendering transparent recovery impossible for these cases.'],\n",
       "   '2000']],\n",
       " [[['Trading Capacity for Performance in a Disk Array.',\n",
       "    'A variety of performance-enhancing techniques, such as striping, mirroring, and rotational data replication, exist in the disk array literature. Given a fixed budget of disks, one must intelligently choose what combination of these techniques to employ. In this paper, we present a way of designing disk arrays that can flexibly and systematically reduce seek and rotational delay in a balanced manner. We give analytical models that can guide an array designer towards optimal configurations by considering both disk and workload characteristics. We have implemented a prototype disk array that incorporates the configuration models. In the process, we have also developed a robust disk head position prediction mechanism without any hardware support. The resulting prototype demonstrates the effectiveness of the configuration models.'],\n",
       "   '2000']],\n",
       " [[['Safe timestamps and large-scale modeling.',\n",
       "    'This paper visits issues that recur in consideration of simulation time-stamps, in the context of building very large simulation models from components developed by different groups, at different times. A key problem here is &ldquo;safety&rdquo;, loosely defined to mean that unintended model behavior does not occur due to unpredictable behavior of timestamp generation and comparisons. We revisit the problems of timestamp format and simultaneity, and then turn to the new problem of timestamp inter-operability. We describe how a C++ simulation kernel can support the concurrent evaluation of submodels that internally use heterogeneous timestamps, and evaluate the execution time costs of doing so. We find that use of a safe timestamp format that explicitly allows different time-scales costs less than 10% over a stock 64-bit integer format, whereas support for completely heterogeneous timestamps can costs as much as 50% in execution speed.'],\n",
       "   '2000']],\n",
       " [[['Parallelizing a sequential logic simulator using an optimistic framework based on a global parallel heap event queue: an experience and performance report.',\n",
       "    'We have parallelized the Iowa Logic Simulator, a gate-level fine-grained discrete-event simulator, by employing an optimistic algorithm framework based on a global event queue implemented as a parallel heap. The original code and the basic data structures of the serial simulator remained unchanged. Wrapper data structures for the logical processes (gates) and the events are created to allow rollbacks, all the earliest events at each logical processes are stored into the parallel heap, and multiple earliest events are simulated repeatedly by invoking the simulate function of the serial simulator. The parallel heap allowed extraction of hundreds to thousands of earliest events in each queue access. On a bus-based shared-memory multiprocessor, simulation of synthetic circuits with 250,000 gates yielding speedups of 3.3 employing five processors compared to the serial execution time of the Iowa Logic Simulator, and limited the number of rollbacks to within 2,000. The basic steps of parallelization are well-defined and general enough to be employable on other discrete-event simulators.'],\n",
       "   '2000']],\n",
       " [[['An empirical study of conservative scheduling.',\n",
       "    'It is well known that the critical path provides an absolute lower bound on the execution time of a conservative parallel discrete event simulation. It stands to reason that optimal execution time can only be achieved by immediately executing each event on the critical path. However, dynamically identifying the critical event is difficult, if not impossible. In this paper, we examine several heuristics that might help to determine the critical event, and conduct a performance study to determine the effectiveness of using these heuristics for preferential scheduling.'],\n",
       "   '2000']],\n",
       " [[['Non Pair-Sharing and Freeness Analysis Through Linear Refinement.',\n",
       "    \"Linear refinement is a technique for systematically constructing abstract domains for program analysis directly from a basic domain representing just the property of interest. This paper uses linear refinement to construct a domain for non pair-sharing and freeness analysis. The resulting domain is strictly more precise than the domain for sharing and freeness analysis defined by Jacobs and Langen. Moreover, it can be used for abstract compilation, while Jacobs and Langen's domain can only be used for abstract interpretation. We provide a representation of the domain, together with algorithms for the abstract operations.\"],\n",
       "   '2000']],\n",
       " [[['Using Regular Approximations for Generalisation During Partial Evalution.',\n",
       "    'On-line partial evaluation algorithms include a generalisation step, which is needed to ensure termination. In partial evaluation of logic and functional programs, the usual generalisation operation applied to computation states is the most specific generalisation (msg) of expressions. This can cause loss of information, which is especially serious in programs whose computations first build some internal data structure, which is then used to control a subsequent phase of execution - a common pattern of computation. If the size of the intermediate data is unbounded at partial evaluation time then the msg will lose almost all information about its structure. Hence the second phase of computation cannot be effectively specialised. In this paper a generalisation based on regular approximations is presented. Regular approximations are recursive descriptions of term structure closely related to tree automata. A regular approximation of computation states can be built during partial evaluation. The critical point is that when generalisation is performed, the upper bound on regular descriptions can be combined with the msg, thus preserving structural information including recursively defined structure. The domain of regular approximations is infinite and hence a widening is incorporated in the generalisation to ensure termination. An algorithm for partial evaluation of logic programs, enhanced with regular approximations, along with some examples of its use will be presented.'],\n",
       "   '2000']],\n",
       " [[['The Second Futamura Projection for Type-Directed Partial Evaluation.',\n",
       "    'A generating extension of a program specializes the program with respect to part of the input. Applying a partial evaluator to the program trivially yields a generating extension, but specializing the partial evaluator with respect to the program often yields a more efficient one. This specialization can be carried out by the partial evaluator itself&semi; in this case, the process is known as the second Futamura projection.We derive an ML implementation of the second Futamura projection for Type-Directed Partial Evaluation (TDPE). Due to the differences between &lsquo;traditional&rsquo;, syntax-directed partial evaluation and TDPE, this derivation involves several conceptual and technical steps. These include a suitable formulation of the second Futamura projection and techniques for making TDPE amenable to self-application. In the context of the second Futamura projection, we also compare and relate TDPE with conventional off-line partial evaluation.We demonstrate our technique with several examples, including compiler generation for Tiny, a prototypical imperative language.'],\n",
       "   '2000']],\n",
       " [[['Towards Partially Evaluating Reflection in Java.',\n",
       "    'Reflection plays a major role in the programming of generic applications. However, it introduces an interpretation layer which is detrimental to performance. A solution consists of relying on partial evaluation to remove this interpretation layer. This paper deals with improving a standard partial evaluator in order to handle the Java reflection API. The improvements basically consist of taking type information into account when distinguishing between static and dynamic data, as well as introducing two new specialization actions: reflection actions. Benchmarks using the serialization framework show the benefits of the approach.'],\n",
       "   '2000']],\n",
       " [[['Calculating Sized Types.',\n",
       "    \"Many program optimizations and analyses, such as array-bounds checking, termination analysis, etc., depend on knowing the size of a function's input and output. However, size information can be difficult to compute. Firstly, accurate size computation requires detecting a size relation between different inputs of a function. Secondly, different optimizations and analyses may require slightly different size information, and thus slightly different computation. Literature in size computation has mainly concentrated on size checking, instead of size inference. In this paper, we provide a generic framework on which different size variants can be expressed and computed. We also describe an effective algorithm for inferring, instead of checking, size information. Size information are expressed in terms of Presburger formulae, and our algorithm utilizes the Omega Calculator to compute as exact a size information as possible, within the linear arithmetic capability.\"],\n",
       "   '2000']],\n",
       " [[['Type-Based Useless Variable Elimination.',\n",
       "    'Useless-variable elimination is a transformation that eliminates variables whose values does not affect the result of a computation. We present a type-based method for useless-variable elimination and prove its correctness. The algorithm is a surprisingly simple extension of the usual type-reconstruction algorithm. Our method has several attractive features. First, it is simple, so that the proof of the correctness is clear and the method can be easily extended to deal with a polymorphic language. Second, it is efficient: for a simply-typed &lambda;-calculus, it runs in time almost linear in the size of an input expression. Moreover, our transformation is optimal in a certain sense among those that preserve well-typedness, both for the simply-typed language and for an ML-style polymorphically-typed language.'],\n",
       "   '2000']],\n",
       " [[['From Recursion to Iteration: What are the Optimizations?',\n",
       "    'Transforming recursion into iteration eliminates the use of stack frames during program execution. It has been studied extensively. This paper describes a powerful and systematic method, based on incrementalization, for transforming general recursion into iteration: identify an input increment, derive an incremental version under the input increment, and form an iterative computation using the incremental version. Exploiting incrementalization yields iterative computation in a uniform way and also allows additional optimizations to be explored cleanly and applied systematically, in most cases yielding iterative programs that use constant additional space, reducing additional space usage asymptotically, and run much faster. We summarize major optimizations, complexity improvements, and performance measurements.'],\n",
       "   '2000']],\n",
       " [[['A Sound Reduction Semantics for Untyped CBN Multi-stage Computation. Or, the Theory of MetaML is Non-trivial (Extended Abstract).',\n",
       "    'A multi-stage computation is one involving more than one stage of execution. MetaML is a language for programming multi-stage computations. Previous studies presented big-step semantics, categorical semantics, and sound type systems for MetaML. In this paper, we report on a confluent and sound reduction semantics for untyped call-by name (CBN) MetaML. The reduction semantics can be used to formally justify some optimization performed by a CBN MetaML implementation. The reduction semantics demonstrates that non-trivial equalities hold for object-code, even in the untyped setting. The paper also emphasizes that adding intensional analysis (that is, taking-apart object programs) to MetaML remains an interesting open problem.'],\n",
       "   '2000']],\n",
       " [[['Program Analysis with Partial Transfer Functions.',\n",
       "    'Program analyses used in compilers commonly use a transfer function (TF) to summarize the input/output behavior of a procedure or region, speeding convergence of analysis of the surrounding region or program. A partial transfer function (PTF) summarizes input/output behavior for only a subset of the possible inputs. In many cases, an exact characterization of input/output behavior is possible for the contexts occurring in a given program, even when a concise and exact total summary would not be feasible. In other cases, an approximate PTF can be used which, while not exact, is more precise than would be possible with a total approximation. As demonstrated by prior work in a flow- and context-sensitive pointer alias analysis, PTFs are effective in speeding the convergence of program analysis involving higher-order functions and complex transfer functions that cannot easily be summarized. This paper presents a formal definition of partial transfer functions and introduces a general framework that enables data flow systems with complex transfer functions to be decomposed into simpler partial transfer functions. This paper provides a foundation that will allow the further development of the PTF concept, as well as enabling application to many more program analyses.'],\n",
       "   '2000']],\n",
       " [[['Symbolic Pointer Analysis for Detecting Memory Leaks.',\n",
       "    \"It is well accepted that pointers are a common source of memory anomalies such as loosing references to dynamic records without deallocating them (also known as memory leaks). This paper presents a novel pointer analysis framework that detects memory leaks by statically analyzing the behavior of programs. Our approach is based on symbolic evaluation of programs. Symbolic evaluation is an advanced static symbolic analysis that is centered around symbolic variable values, assumptions about and constraints between variable values, and control flow information (path conditions). As part of symbolic evaluation we introduce a new symbolic heap algebra for modeling heap operations. Predicates &mdash; defined over the program's input &mdash; are derived which allow to detect memory leaks. Our approach goes beyond previous work in the field of statically detecting memory leaks by considering also path conditions which increases the accuracy of our results, symbolically modeling heap data structures and heap operations. Examples are used to illustrate the effectiveness of our approach.\"],\n",
       "   '2000']],\n",
       " [[['ABCD: eliminating array bounds checks on demand.',\n",
       "    'To guarantee typesafe execution, Java and other strongly typed languages require bounds checking of array accesses. Because array-bounds checks may raise exceptions, they block code motion of instructions with side effects, thus preventing many useful code optimizations, such as partial redundancy elimination or instruction scheduling of memory operations. Furthermore, because it is not expressible at bytecode level, the elimination of bounds checks can only be performed at run time, after the bytecode program is loaded. Using existing powerful bounds-check optimizers at run time is not feasible, however, because they are too heavyweight for the dynamic compilation setting. ABCD is a light-weight algorithm for elimination of Array Bounds Checks on Demand. Its design emphasizes simplicity and efficiency. In essence, ABCD works by adding a few edges to the SSA value graph and performing a simple traversal of the graph. Despite its simplicity, ABCD is surprisingly powerful. On our benchmarks, ABCD removes on average 45% of dynamic bound check instructions, sometimes achieving near-ideal optimization. The efficiency of ABCD stems from two factors. First, ABCD works on a sparse representation. As a result, it requires on average fewer than 10 simple analysis steps per bounds check. Second, ABCD is demand-driven. It can be applied to a set of frequently executed (hot) bounds checks, which makes it suitable for the dynamic-compilation setting, in which compile-time cost is constrained but hot statements are known.'],\n",
       "   '2000']],\n",
       " [[['Unification-based pointer analysis with directional assignments.',\n",
       "    \"This paper describes a new algorithm for flow and context insensitive pointer analysis of C programs. Our studies show that the most common use of pointers in C programs is in passing the addresses of composite objects or updateable values as arguments to procedures. Therefore, we have designed a low-cost algorithm that handles this common case accurately. In terms of both precision and running time, this algorithm lies between Steensgaard's algorithm, which treats assignments bi-directionally using unification, and Andersen's algorithm, which treats assignments directionally using subtyping. Our &ldquo;one level flow&rdquo; algorithm uses a restricted form of subtyping to avoid unification of symbols at the top levels of pointer chains in the points-to graph, while using unification elsewhere in the graph. The method scales easily to large programs. For instance, we are able to analyze a 1.4 MLOC (million lines of code) program in two minutes, using less than 200MB of memory. At the same time, the precision of our algorithm is very close to that of Andersen's algorithm. On all of the integer benchmark programs from SPEC95, the one level flow algorithm and Andersen's algorithm produce either identical or essentially identical points-to information. Therefore, we claim that our algorithm provides a method for obtaining precise flow-insensitive points-to information for large C programs.\"],\n",
       "   '2000']],\n",
       " [[['Effective synchronization removal for Java.',\n",
       "    'We present a new technique for removing unnecessary synchronization operations from statically compiled Java programs. Our approach improves upon current efforts based on escape analysis, as it can eliminate synchronization operations even on objects that escape their allocating threads. It makes use of a compact, equivalence-class-based representation that eliminates the need for fixed point operations during the analysis. We describe and evaluate the performance of an implementation in the Marmot native Java compiler. For the benchmark programs examined, the optimization removes 100% of the dynamic synchronization operations in single-threaded programs, and 0-99% in multi-threaded programs, at a low cost in additional compilation time and code growth.'],\n",
       "   '2000']],\n",
       " [[['Dynamo: a transparent dynamic optimization system.',\n",
       "    \"We describe the design and implementation of Dynamo, a software dynamic optimization system that is capable of transparently improving the performance of a native instruction stream as it executes on the processor. The input native instruction stream to Dynamo can be dynamically generated (by a JIT for example), or it can come from the execution of a statically compiled native binary. This paper evaluates the Dynamo system in the latter, more challenging situation, in order to emphasize the limits, rather than the potential, of the system. Our experiments demonstrate that even statically optimized native binaries can be accelerated Dynamo, and often by a significant degree. For example, the average performance of -O optimized SpecInt95 benchmark binaries created by the HP product C compiler is improved to a level comparable to their -O4 optimized version running without Dynamo. Dynamo achieves this by focusing its efforts on optimization opportunities that tend to manifest only at runtime, and hence opportunities that might be difficult for a static compiler to exploit. Dynamo's operation is transparent in the sense that it does not depend on any user annotations or binary instrumentation, and does not require multiple runs, or any special compiler, operating system or hardware support. The Dynamo prototype presented here is a realistic implementation running on an HP PA-8000 workstation under the HPUX 10.20 operating system.\"],\n",
       "   '2000']],\n",
       " [[['Modular interprocedural pointer analysis using access paths: design, implementation, and evaluation.',\n",
       "    'In this paper we present a modular interprocedural pointer analysis algorithm based on access-paths for C programs. We argue that access paths can reduce the overhead of representing context-sensitive transfer functions and effectively distinguish non-recursive heap objects. And when the modular analysis paradigm is used together with other techniques to handle type casts and function pointers, we are able to handle significant programs like those in the SPECcint92 and SPECcint95 suites. We have implemented the algorithm and tested it on a Pentium II 450 PC running Linux. The observed resource consumption and performance improvement are very encouraging.'],\n",
       "   '2000']],\n",
       " [[['Practicing JUDO: Java under dynamic optimizations.',\n",
       "    'A high-performance implementation of a Java Virtual Machine (JVM) consists of efficient implementation of Just-In-Time (JIT) compilation, exception handling, synchronization mechanism, and garbage collection (GC). These components are tightly coupled to achieve high performance. In this paper, we present some static anddynamic techniques implemented in the JIT compilation and exception handling of the Microprocessor Research Lab Virtual Machine (MRL VM), i.e., lazy exceptions, lazy GC mapping, dynamic patching, and bounds checking elimination. Our experiments used IA-32 as the hardware platform, but the optimizations can be generalized to other architectures.'],\n",
       "   '2000']],\n",
       " [[['A certifying compiler for Java.',\n",
       "    'This paper presents the initial results of a project to determine if the techniques of proof-carrying code and certifying compilers can be applied to programming languages of realistic size and complexity. The experiment shows that: (1) it is possible to implement a certifying native-code compiler for a large subset of the Java programming language; (2) the compiler is freely able to apply many standard local and global optimizations; and (3) the PCC binaries it produces are of reasonable size and can be rapidly checked for type safety by a small proof-checker. This paper also presents further evidence that PCC provides several advantages for compiler development. In particular, generating proofs of the target code helps to identify compiler bugs, many of which would have been difficult to discover by testing.'],\n",
       "   '2000']],\n",
       " [[['An automatic object inlining optimization and its evaluation.',\n",
       "    'Automatic object inlining [19, 20] transforms heap data structures by fusing parent and child objects together. It can improve runtime by reducing object allocation and pointer dereference costs. We report continuing work studying object inlining optimizations. In particular, we present a new semantic derivation of the correctness conditions for object inlining, and program analysis which extends our previous work. And we present an object inlining transformation, focusing on a new algorithm which optimizes class field layout to minimize code expansion. Finally, we detail a fuller evaluation on eleven programs and libraries (including Xpdf, the 25,000 line Portable Document Format (PDF) file browser) that utilizes hardware measures of impact on the memory system. We show that our analysis scales effectively to large programs, finding many inlinable fields (45 in xpdf) at acceptable cost, and we show that, on some programs, it finds nearly all fields for which object inlining is correct, and averages 40% of such fields across our benchmarks. We implement our analyses in an advanced analysis infrastructure, and we show that, compared to traditional 1-CFA, that infrastructure provides better results and lower and more scalable cost. Across all programs, analysis identified about 30% of objects as inlinable on average. Our transformation increases code size by only 20% while inlining this 30% of fields. Inlining these objects eliminated on average 28% of field reads, 58% of object creations, 12% of all loads. Further, the optimized programs have significantly improved memory reference behavior, producing 25% fewer L1 data cache misses and 25% fewer read stalls. On average the runtime improved by 14%.'],\n",
       "   '2000']],\n",
       " [[['A generational on-the-fly garbage collector for Java.',\n",
       "    'An on-the-fly garbage collector does not stop the program threads to perform the collection. Instead, the collector executes in a separate thread (or process) in parallel to the program. On-the-fly collectors are useful for multi-threaded applications running on multiprocessor servers, where it is important to fully utilize all processors and provide even response time, especially for systems for which stopping the threads is a costly operation. In this work, we report on the incorporation of generations into an on-the-fly garbage collector. The incorporation is non-trivial since an on-the-fly collector avoids explicit synchronization with the program threads. To the best of our knowledge, such an incorporation has not been tried before. We have implemented the collector for a prototype Java Virtual Machine on AIX, and measured its performance on a 4-way multiprocessor. As for other generational collectors, an on-the-fly generational collector has the potential for reducing the overall running time and working set of an application by concentrating collection efforts on the young objects. However, in contrast to other generational collectors, on-the-fly collectors do not move the objects; thus, there is no segregation between the old and the young objects. Furthermore, on-the-fly collectors do not stop the threads, so there is no extra benefit for the short pauses obtained by generational collection. Nevertheless, comparing our on-the-fly collector with and without generations, it turns out that the generational collector performs better for most applications. The best reduction in overall running time for the benchmarks we measured was 25%. However, there were some benchmarks for which it had no effect and one for which the overall running time increased by 4%.'],\n",
       "   '2000']],\n",
       " [[['Scalable context-sensitive flow analysis using instantiation constraints.',\n",
       "    'This paper shows that a type graph (obtained via polymorphic type inference) harbors explicit directional flow paths between functions. These flow paths arise from the instantiations of polymorphic types and correspond to call-return sequences in first-order programs. We show that flow information can be computed efficiently while considering only paths with well matched call-return sequences, even in the higher-order case. Furthermore, we present a practical algorithm for inferring type instantiation graphs and provide empirical evidence to the scalability of the presented techniques by applying them in the context of points-to analysis for C programs.'],\n",
       "   '2000']],\n",
       " [[['Type-based race detection for Java.',\n",
       "    'This paper presents a static race detection analysis for multithreaded Java programs. Our analysis is based on a formal type system that is capable of capturing many common synchronization patterns. These patterns include classes with internal synchronization, classes thatrequire client-side synchronization, and thread-local classes. Experience checking over 40,000 lines of Java code with the type system demonstrates that it is an effective approach for eliminating races conditions. On large examples, fewer than 20 additional type annotations per 1000 lines of code were required by the type checker, and we found a number of races in the standard Java libraries and other test programs.'],\n",
       "   '2000']],\n",
       " [[['Field analysis: getting useful and low-cost interprocedural information.',\n",
       "    'We present a new limited form of interprocedural analysis called field analysis that can be used by a compiler to reduce the costs of modern language features such as object-oriented programming, automatic memory management, and run-time checks required for type safety. Unlike many previous interprocedural analyses, our analysis is cheap, and does not require access to the entire program. Field analysis exploits the declared access restrictions placed on fields in a modular language (e.g. field access modifiers in Java) in order to determine useful properties of fields of an object. We describe our implementation of field analysis in the Swift optimizing compiler for Java, as well a set of optimizations that exploit the results of field analysis. These optimizations include removal of run-time tests, compile-time resolution of method calls, object inlining, removal of unnecessary synchronization, and stack allocation. Our results demonstrate that field analysis is efficient and effective. Speedups average 7% on a wide range of applications, with some times reduced by up to 27%. Compile time overhead of field analysis is about 10%.'],\n",
       "   '2000']],\n",
       " [[['Compiler analysis of irregular memory accesses.',\n",
       "    'Irregular array accesses are array accesses whose array subscripts do not have closed-form expressions in terms of loop indices. Traditional array analysis and loop transformation techniques cannot handle irregular array accesses. In this paper, we study two kinds of simple and common cases of irregular array accesses: single-indexed access and indirect array access. We present techniques to analyze these two cases at compile-time, and we provide experimental results showing the effectiveness of these techniques in finding more implicit loop parallelism at compile-time and improved speedups.'],\n",
       "   '2000']],\n",
       " [[['Translation validation for an optimizing compiler.',\n",
       "    'We describe a translation validation infrastructure for the GNU C compiler. During the compilation the infrastructure compares the intermediate form of the program before and after each compiler pass and verifies the preservation of semantics. We discuss a general framework that the optimizer can use to communicate to the validator what transformations were performed. Our implementation however does not rely on help from the optimizer and it is quite successful by using instead a few heuristics to detect the transformations that take place. The main message of this paper is that a practical translation validation infrastructure, able to check the correctness of many of the transformations performed by a realistic compiler, can be implemented with about the effort typically required to implement one compiler pass. We demonstrate this in the context of the GNU C compiler for a number of its optimizations while compiling realistic programs such as the compiler itself or the Linux kernel. We believe that the price of such an infrastructure is small considering the qualitative increase in the ability to isolate compilation errors during compiler testing and maintenance.'],\n",
       "   '2000']],\n",
       " [[['A single intermediate language that supports multiple implementations of exceptions.',\n",
       "    'We present mechanisms that enable our compiler-target language, C--, to express four of the best known techniques for implementing exceptions, all within a single, uniform framework. We define the mechanisms precisely, using a formal operational semantics. We also show that exceptions need not require special treatment in the optimizer; by introducing extra dataflow edges, we make standard optimization techniques work even on programs that use exceptions. Our approach clarifies the design space of exception-handling techniques, and it allows a single optimizer to handle a variety of implementation techniques. Our ultimate goal is to allow a source-language compiler the freedom to choose its exception-handling policy, while encapsulating the architecture-dependent mechanisms and their optimization in an implementation of C--that can be used by compilers for many source languages.'],\n",
       "   '2000']],\n",
       " [[['Off-line variable substitution for scaling points-to analysis.',\n",
       "    \"Most compiler optimizations and software productivity tools rely on information about the effects of pointer dereferences in a program. The purpose of points-to analysis is to compute this information safely, and as accurately as is practical. Unfortunately, accurate points-to information is difficult to obtain for large programs, because the time and space requirements of the analysis become prohibitive. We consider the problem of scaling flow- and context-insensitive points-to analysis to large programs, perhaps containing hundreds of thousands of lines of code. Our approach is based on a variable substitution transformation, which is performed off-line, i.e., before a standard points-to analysis is performed. The general idea of variable substitution is that a set of variables in a program can be replaced by a single representative variable, thereby reducing the input size of the problem. Our main contribution is a linear-time algorithm which finds a particular variable substitution that maintains the precision of the standard analysis, and is also very effective in reducing the size of the problem. We report our experience in performing points-to analysis on large C programs, including some industrial-sized ones. Experiments show that our algorithm can reduce the cost of Andersen's points-to analysis substantially: on average, it reduced the running time by 53% and the memory cost by 59%, relative to an efficient baseline implementation of the analysis.\"],\n",
       "   '2000']],\n",
       " [[['Symbolic bounds analysis of pointers, array indices, and accessed memory regions.',\n",
       "    'This article presents a novel framework for the symbolic bounds analysis of pointers, array indices, and accessed memory regions. Our framework formulates each analysis problem as a system of inequality constraints between symbolic bound polynomials. It then reduces the constraint system to a linear program. The solution to the linear program provides symbolic lower and upper bounds for the values of pointer and array index variables and for the regions of memory that each statement and procedure accesses. This approach eliminates fundamental problems associated with applying standard fixed-point approaches to symbolic analysis problems. Experimental results from our implemented compiler show that the analysis can solve several important problems, including static race detection, automatic parallelization, static detection of array bounds violations, elimination of array bounds checks, and reduction of the number of bits used to store computed values.'],\n",
       "   '2000']],\n",
       " [[['A framework for interprocedural optimization in the presence of dynamic class loading.',\n",
       "    'Dynamic class loading during program execution in the Java Programming Language is an impediment for generating code that is as efficient as code generated using static whole-program analysis and optimization. Whole-program analysis and optimization is possible for languages, such as C++, that do not allow new classes and/or methods to be loaded during program execution. One solution for performing whole-program analysis and avoiding incorrect execution after a new class is loaded is to invalidate and recompile affected methods. Runtime invalidation and recompilation mechanisms can be expensive in both space and time, and, therefore, generally restrict optimization. To address these drawbacks, we propose a new framework, called the extant analysis framework, for interprocedural optimization of programs that support dynamic class (or method)loading. Given a set of classes comprising the closed world, we perform an offline static analysis which partitions references into two categories:(1) unconditionally extant references which point only to objects whose runtime type is guaranteed to be in the closed world; and (2) conditionally extant references which point to objects whose runtime type is not guaranteed to be in the closed world. Optimizations solely dependent on the first categorycan be statically performed, and are guaranteed to be correct even with any future class/method loading. Optimizations dependent on the second category are guarded by dynamic tests, called extant safety tests, for correct execution behavior.We describe the properties for extant safety tests, and provide algorithms for their generation and placement.'],\n",
       "   '2000']],\n",
       " [[['Bitwidth analysis with application to silicon compilation.',\n",
       "    'This paper introduces Bitwise, a compiler that minimizes the bitwidth the number of bits used to represent each operand for both integers and pointers in a program. By propagating 70 static information both forward and backward in the program dataflow graph, Bitwise frees the programmer from declaring bitwidth invariants in cases where the compiler can determine bitwidths automatically. Because loop instructions comprise the bulk of dynamically executed instructions, Bitwise incorporates sophisticated loop analysis techniques for identifying bitwidths. We find a rich opportunity for bitwidth reduction in modern multimedia and streaming application workloads. For new architectures that support sub-word data-types, we expect that our bitwidth reductions will save power and increase processor performance. This paper also applies our analysis to silicon compilation, the translation of programs into custom hardware, to realize the full benefits of bitwidth reduction. We describe our integration of Bitwise with the DeepC Silicon Compiler. By taking advantage of bitwidth information during architectural synthesis, we reduce silicon real estate by 15 - 86%, improve clock speed by 3 - 249%, and reduce power by 46 - 73%. The next era of general purpose and reconfigurable architectures should strive to capture a portion of these gains.'],\n",
       "   '2000']],\n",
       " [[['Optimal instruction scheduling using integer programming.',\n",
       "    \"This paper presents a new approach to local instruction scheduling based on integer programming that produces optimal instruction schedules in a reasonable time, even for very large basic blocks. The new approach first uses a set of graph transformations to simplify the data-dependency graph while preserving the optimality of the final schedule. The simplified graph results in a simplified integer program which can be solved much faster. A new integer-programming formulation is then applied to the simplified graph. Various techniques are used to simplify the formulation, resulting in fewer integer-program variables, fewer integer-program constraints and fewer terms in some of the remaining constraints, thus reducing integer-program solution time. The new formulation also uses certain adaptively added constraints (cuts) to reduce solution time. The proposed optimal instruction scheduler is built within the Gnu Compiler Collection (GCC) and is evaluated experimentally using the SPEC95 floating point benchmarks. Although optimal scheduling for the target processor is considered intractable, all of the benchmarks' basic blocks are optimally scheduled, including blocks with up to 1000 instructions, while total compile time increases by only 14%.\"],\n",
       "   '2000']],\n",
       " [[['Safety checking of machine code.',\n",
       "    'We show how to determine statically whether it is safe for untrusted machine code to be loaded into a trusted host system. Our safety-checking technique operates directly on the untrusted machine-code program, requiring only that the initial inputs to the untrusted program be annotated with typestate information and linear constraints. This approach opens up the possibility of being able to certify code produced by any compiler from any source language, which gives the code producers more freedom in choosing the language in which they write their programs. It eliminates the dependence of safety on the correctness of the compiler because the final product of the compiler is checked. It leads to the decoupling of the safety policy from the language in which the untrusted code is written, and consequently, makes it possible for safety checking to be performed with respect to an extensible set of safety properties that are specified on the host side. We have implemented a prototype safety checker for SPARC machine-language programs, and applied the safety checker to several examples. The safety checker was able to either prove that an example met the necessary safety conditions, or identify the places where the safety conditions were violated. The checking times ranged from less than a second to 14 seconds on an UltraSPARC machine.'],\n",
       "   '2000']],\n",
       " [[['Transforming loops to recursion for multi-level memory hierarchies.',\n",
       "    'Recently, there have been several experimental and theoretical results showing significant performance benefits of recursive algorithms on both multi-level memory hierarchies and on shared-memory systems. In particular, such algorithms have the data reuse characteristics of a blocked algorithm that is simultaneously blocked at many different levels. Most existing applications, however, are written using ordinary loops. We present a new compiler transformation that can be used to convert loop nests into recursive form automatically. We show that the algorithm is fast and effective, handling loop nests with arbitrary nesting and control flow. The transformation achieves substantial performance improvements for several linear algebra codes even on a current system with a two level cache hierarchy. As a side-effect of this work, we also develop an improved algorithm for transitive dependence analysis (a powerful technique used in the recursion transformation and other loop transformations)that is much faster than the best previously known algorithm in practice.'],\n",
       "   '2000']],\n",
       " [[['Improved spill code generation for software pipelined loops.',\n",
       "    'Software pipelining is a loop scheduling technique that extracts parallelism out of loops by overlapping the execution of several consecutive iterations. Due to the overlapping of iterations, schedules impose high register requirements during their execution. A schedule is valid if it requires at most the number of registers available in the target architecture. If not, its register requirements have to be reduced either by decreasing the iteration overlapping or by spilling registers to memory. In this paper we describe a set of heuristics to increase the quality of register-constrained modulo schedules. The heuristics decide between the two previous alternatives and define criteria for effectively selecting spilling candidates. The heuristics proposed for reducing the register pressure can be applied to any software pipelining technique. The proposals are evaluated using a register-conscious software pipeliner on a workbench composed of a large set of loops from the Perfect Club benchmark and a set of processor configurations. Proposals in this paper are compared against a previous proposal already described in the literature. For one of these processor configurations and the set of loops that do not fit in the available registers (32), a speed-up of 1.68 and a reduction of the memory traffic by a factor of 0.57 are achieved with an affordable increase in compilation time. For all the loops, this represents a speed-up of 1.38 and a reduction of the memory traffic by a factor of 0.7.'],\n",
       "   '2000']],\n",
       " [[['Fast deterministic consensus in a noisy environment.',\n",
       "    \"It is well known that the consensus problem cannot be solved deterministically in an asynchronous environment, but that randomized solutions are possible. We propose a new model, called noisy scheduling, in which an adversarial schedule is perturbed randomly, and show that in this model randomness in the environment can substitute for randomness in the algorithm. In particular, we show that a simplified, deterministic version of Chandra's wait-free shared-memory consensus algorithm [Chandra, in: Proceedings of the Fifteenth Annual ACM Symposium on Principles of Distributed Computing, Philadelphia, PA, USA, 23-26 May, 1996, pp. 166-175] solves consensus in time at most logarithmic in the number of active processes. The proof of termination is based on showing that a race between independent delayed renewal processes produces a winner quickly. In addition, we show that the protocol finishes in constant time using quantum and priority-based scheduling on a uniprocessor, suggesting that it is robust against the choice of model over a wide range.\"],\n",
       "   '2000']],\n",
       " [[['Adaptive and efficient mutual exclusion (extended abstract).',\n",
       "    \"A distributed algorithm is adaptive if its performance depends on k, the number of processes that are concurrently active during the algorithm execution (rather than on n, the total number of processes). This paper presents adaptive algorithm for mutual exclusion using only read and write operations. The worst case step complexity cannot be a measure for the performance of mutual exclusion algorithms, because it is always unbounded in the presence of contention. Therefore, a number of different parameters are used to measure the algorithm's performance: The remote step complexity is the maximal number of steps performed by a process where a wait is counted as one step. The system response time is the time interval between subsequent entries to the critical section, where one time unit is the minimal interval in which every active process performs at least one step. The algorithm presented here has O(k) remote step complexity and O(log k) system response time, where k is the point contention. The space complexity of this algorithm is O(nN), where N is the range of processes' names. The space complexity of all previously known adaptive algorithms for various long-lived problems depends on N. We present a technique that reduces the space complexity of our algorithm to be a function of n, while preserving the other performance measures of the algorithm.\"],\n",
       "   '2000']],\n",
       " [[['Clock synchronization with faults and recoveries (extended abstract).',\n",
       "    \"We present a convergence-function based clock synchronization algorithm, which is simple, efficient and fault-tolerant. The algorithm is tolerant of failures and allows recoveries, as long as less than a third of the processors are faulty 'at the same time'. Arbitrary (Byzantine) faults are tolerated, without requiring awareness of failure or recovery. In contrast, previous clock synchronization algorithms limited the total number of faults throughout the execution, which is not realistic, or assumed fault detection. The use of our algorithm ensures secure and reliable time services, a requirement of many distributed systems and algorithms. In particular, secure time is a fundamental assumption of proactive secure mechanisms, which are also designed to allow recovery from (arbitrary) faults. Therefore, our work is crucial to realize these mechanisms securely.\"],\n",
       "   '2000']],\n",
       " [[['1/k phase stamping for continuous shared data (extended abstract).',\n",
       "    \"Interactive distributed applications are a relatively new class of applications that are enabled by sharing continuously evolving data across distributed sites (and users). The characteristics of application data include very fine-grained updates that can atomically access a subset of the shared data, masking of update effects, and irregular locality and contention for access. Existing programming approaches are not appropriate for programming such continuous shared data in a wide-area environment. We define an object-set abstraction, where a set is replicated at sites interested in the objects, and is modified using add, delete and update operations. The key features of this abstraction are issue-time access information for update operations, and the potential for replicating the computation associated with updates. Ordering of operations is an important problem, and we present a fast, scalable ordering algorithm, 1/k phase stamping, that uses distributed tokens that correspond to objects in the set. This algorithm provides substantially better performance than alternative algorithms, is deadlock and abort free, and requires no queuing at the tokens. Therefore, tokens can be located inside a programmable 'active' network. The precise stamps generated with 1/k phase stamping enable a dynamic communication optimization technique, effect and stamp merging, which can reduce communication and allow utilization of best-effort message delivery. These algorithms have been implemented in the RAGA system which can tolerate crash failures.\"],\n",
       "   '2000']],\n",
       " [[['Random oracles in constantipole: practical asynchronous Byzantine agreement using cryptography (extended abstract).',\n",
       "    'Byzantine agreement requires a set of parties in a distributed system to agree on a value even if some parties are corrupted. A new protocol for Byzantine agreement in a completely asynchronous network is presented that makes use of cryptography, specifically of threshold signatures and coin-tossing protocols. These cryptographic protocols have practical and provably secure implementations in the &ldquo;random oracle&rdquo; model. In particular, a coin-tossing protocol based on the Diffie-Hellman problem is presented and analyzed. The resulting asynchronous Byzantine agreement protocol is both practical and nearly matches the known theoretical lower bounds. More precisely, it tolerates the maximum number of corrupted parties, runs in constant expected time, has message and communication complexity close to the maximum, and uses a trusted dealer only in a setup phase, after which it can process a virtually unlimited number of transactions. Novel dual-threshold variants of both cryptographic protocols are used. The protocol is formulated as a transaction processing service in a cryptographic security model, which differs from the standard information-theoretic formalization and may be of independent interest.'],\n",
       "   '2000']],\n",
       " [[['Compact roundtrip routing in directed networks (extended abstract).',\n",
       "    'The first sublinear average space universal compact routing schemes for directed networks are presented. For each integer k &ge; 1, they use O(k log n) size addresses; O(kn 1/k+1)-sized routing tables on average at each node; and achieve roundtrip routes of stretch at most 2k+1-1 in any (weighted) directed network. We extend our results to yield universal compact roundtrip routing schemes with the stronger requirement that they use sublinear maximum space at every node. These schemes also use O(k log n) size addresses and achieve roundtrip routes of stretch at most 2k+1 - 1 in any (weighted) directed network, and they bound the maximum sized table at each node by O(kn 3k+1/2&middot;3k).'],\n",
       "   '2000']],\n",
       " [[['Assigning labels in unknown anonymous networks (extended abstract).',\n",
       "    'We consider the task of distributedly assigning distinct labels to nodes of an unknown anonymous network. A priori, nodes do not have any identities (anonymous network) and do not know the topology or the size of the network (unknown network). They execute identical algorithms, apart from a distinguished node, called the source, which starts the labeling process. Our goal is to assign short labels, as fast as possible. The quality of a labeling algorithm is measured by the range from which the algorithm picks the labels, or alternatively, the length of the assigned labels. Natural efficiency measures are the time, i.e., the number of rounds required for the label assignment, and the message and bit complexities of the label assignment protocol, i.e., the total number of messages (resp., bits) circulating in the network. We present label assignment algorithms whose time and message complexity are asymptotically optimal and which assign short labels. On the other hand, we establish inherent trade-offs between quality and efficiency for labeling algorithms.'],\n",
       "   '2000']],\n",
       " [[['The wakeup problem in synchronous broadcast systems (extended abstract).',\n",
       "    'This paper studies the differences between two levels of synchronization in a distributed broadcast system (or a multiple access channel). In the globally synchronous model, all processors have access to a global clock. In the locally synchronous model, processors have local clocks ticking at the same rate, but each clock starts individually, when the processor wakes up. We consider the fundamental problem of waking up all of n processors of a completely connected broadcast system. Some processors wake up spontaneously, while others have to be woken up. Only wake processors can send messages; a sleeping processor is woken up upon hearing a message. The processors hear a message in a given round if and only if exactly one processor sends a message in that round. Our goal is to wake up all processors as fast as possible in the worst case, assuming an adversary controls which processors wake up and when. We analyze the problem in both the globally synchronous and locally synchronous models, with or without the assumption that n is known to the processors. We propose randomized and deterministic algorithms for the problem, as well as lower bounds in some of the cases. These bounds establish a gap between the globally synchronous and locally synchronous models.'],\n",
       "   '2000']],\n",
       " [[['Indulgent algorithms (preliminary version).',\n",
       "    'Informally, an indulgent algorithm is a distributed algorithm that tolerates unreliable failure detection: the algorithm is indulgent towards its failure detector. This paper formally characterises such algorithms and states some of their interesting features. We show that indulgent algorithms are inherently safe and uniform. We also state impossibility results for indulgent solutions to divergent problems like consensus, and failure-sensitive problems like non-blocking atomic commit and terminating reliable broadcast.'],\n",
       "   '2000']],\n",
       " [[['k-set agreement with limited accuracy failure detectors.',\n",
       "    'Let the scope of the accuracy property of an unreliable failure detector be the number x of processes that may not suspect a correct process. The scope notion gives rise to new classes of failure detectors among which we consider Sx and &diam;Sx in this paper (Usual failure detectors consider an implicit scope equal to n, the total number of processes). The k-set agreement problem generalizes the consensus problem: each correct process has to decide a value in such a way that a decided value is a proposed value, and the number of decided values is bounded by k. There exist protocols that solve this problem in asynchronous distributed systems when &fnof; < k (where &fnof; is the maximum number of processes that may crash). Moreover, it has been shown that there is no solution in those systems when &fnof; &ge; k. The paper considers asynchronous distributed systems equipped with limited scope accuracy failure detectors. It studies conditions on n, &fnof;, k and x that allow to solve the k-set agreement problem in those systems and presents two protocols. The first protocol solves the k-set agreement in asynchronous distributed systems augmented with a failure detector of the class Sx. It requires &fnof; < k + x - 1. The second protocol works with any failure detector of the class &diam;Sx. It actually defines a family of protocols. This family allows to solve the k-set agreement problem when &fnof; < max(k, max1&le;&agr;&le;k(min(n - &agr;&lfloor;n/(&agr; + 1)&rfloor;, &agr; +x - 1))). We conjecture that, when &fnof; &ge; k, these conditions are necessary to solve the k-set agreement problem in asynchronous distributed systems equipped with failure detectors &egr; Sx or &diam;Sx, respectively.'],\n",
       "   '2000']],\n",
       " [[['Self-stabilizing token circulation on asynchronous uniform unidirectional rings.',\n",
       "    'In [2], J. Beauquier, M. Gradinariu and C. Johnen presented a probabilistic self-stabilizing token circulation algorithm for asynchronous uniform unidirectional rings. This paper provides an improvement on this algorithm. It also computes the (message) complexity of the stabilization period of the original algorithm, which is &THgr;(N3), and the improved version, which is &THgr;(N2 log N), where N is the number of processes plus the number of messages in transit. Such algorithms are mostly used for mutual exclusion.'],\n",
       "   '2000']],\n",
       " [[['Reachability and Connectivity Queries in Constraint Databases.',\n",
       "    'It is known that standard query languages for constraint databases lack the power to express connectivity properties. Such properties are important in the context of geographical databases, where one naturally wishes to ask queries about connectivity (What are the connected components of a given set?) or reachability (Is there a path from A to B that lies entirely in a given region?). No existing constraint query languages that allow closed-form evaluation can express these properties. In the first part of the paper, we show that, in principle, there is no obstacle to getting closed languages that can express connectivity and reachability queries. In fact, we show that adding any topological property to standard languages like FO + Lin and FO + Poly results in a closed language. In the second part of the paper, we look for tractable closed languages for expressing reachability and connectivity queries. We introduce path logic, which allows one to state properties of paths with respect to given regions. We show that it is closed, has polynomial time data complexity for linear and polynomial constraints, and can express a large number of teachability properties beyond simple connectivity. Query evaluation in the logic involves obtaining a discrete abstraction of a continuous path, and model-checking of temporal formulae on the discrete structure.'],\n",
       "   '2000']],\n",
       " [[['(Almost) Optimal Parallel Block Access for Range Queries.',\n",
       "    'Range queries are an important class of queries for several applications can be achieved by tiling the multi-dimensional data and distributing it among multiple disks or nodes. It has been established that schemes that achieve optimal parallel block access exist only for a few special cases. Though several schemes for the allocation of tiles to disks have been developed, no scheme with non-trivial worst-case bound is known. We establish that any range query on a 2q × 2q-block grid of blocks can be performed using k = 2t disks (t ≤ q), in at most ⌈m/k⌉ + O(logk) parallel block accesses. We achieve this result by judiciously distributing the blocks among the k nodes or disks. Experimental data show that the algorithm achieves very close to ⌈m/k⌉ performance (on average less than 0.5 away from ⌈m/k⌉, with a worst-case of 3). Although several declustering schemes for range queries have been developed, prior to our work no additive non-trivial performance bounds were known. Our scheme guarantees performance within a (small) additive deviation from ⌈m/k⌉. Subsequent to this work, Bhatia et al. [Hierarchical declustering schemes for range queries, in: Proceedings of International Conference on Extending Database Technology, Konstanz, Germany, March 2000, p. 525] have proved that such a performance bound is essentially optimal for this kind of scheme, and have also extended our results to the case where the number of disks is a product of the form k1 * k2 * ... * kt where the kis need not all be 2.'],\n",
       "   '2000']],\n",
       " [[['Indexing Moving Points.',\n",
       "    'We propose three indexing schemes for storing a set S of N points in the plane, each moving along a linear trajectory, so that any query of the following form can be answered quickly: Given a rectangle R and a real value t, report all K points of S that lie inside R at time t. We first present an indexing structure that, for any given constant ε > 0, uses O(N/B) disk blocks and answers a query in O(N/B1/2+ε + K/B)I/Os, where B is the block size. It can also report all the points of S that lie inside R during a given time interval. A point can be inserted or deleted, or the trajectory of a point can be changed, in O(log2BN) I/Os. Next, we present a general approach that improves the query time if the queries arrive in chronological order, by allowing the index to evolve over time. We obtain a tradeoff between the query time and the number of times the index needs to be updated as the points move. We also describe an indexing scheme in which the number of I/Os required to answer a query depends monotonically on the difference between the query time stamp t and the current time. Finally, we develop an efficient indexing scheme to answer approximate nearest-neighbor queries among moving points.'],\n",
       "   '2000']],\n",
       " [[['Computational Properties of Metaquerying Problems.',\n",
       "    'Metaquerying is a data mining technology by which hidden dependencies among several database relations can be discovered. This tool has already been successfully applied to several real-world applications, but only preliminary results about the complexity of metaquerying can be found in the literature. In this article, we define several variants of metaquerying that encompass, as far as we know, all the variants that have been defined in the literature. We study both the combined complexity and the data complexity of these variants. We show that under the combined complexity measure metaquerying is generally intractable (unless P = NP), lying sometimes quite high in the complexity hierarchies (as high as NPPP), depending on the characteristics of the plausibility index. Nevertheless, we are able to single out some tractable and interesting metaquerying cases, whose combined complexity is LOGCFL-complete. As for the data complexity of metaquerying, we prove that, in general, it is within TC0, but lies within AC0 in some simpler cases. Finally, we discuss the implementation of metaqueries by providing algorithms that answer them.'],\n",
       "   '2000']],\n",
       " [[['Towards Estimation Error Guarantees for Distinct Values.',\n",
       "    'We consider the problem of estimating the number of distinct values in a column of a table. For large tables without an index on the column, random sampling appears to be the only scalable approach for estimating the number of distinct values. We establish a powerful negative result stating that no estimator can guarantee small error across all input distributions, unless it examines a large fraction of the input data. In fact, any estimator must incur a significant error on at least some of a natural class of distributions. We then provide a new estimator which is provably optimal, in that its error is guaranteed to essentially match our negative result. A drawback of this estimator is that while its worst-case error is reasonable, it does not necessarily give the best possible error bound on any given distribution. Therefore, we develop heuristic estimators that are optimized for a class of typical input distributions. While these estimators lack strong guarantees on distribution-independent worst-case error, our extensive empirical comparison indicate their effectiveness both on real data sets and on synthetic data sets.'],\n",
       "   '2000']],\n",
       " [[['Information Dependencies.',\n",
       "    \"This paper uses the tools of information theory to examine and reason about the information content of the attributes within a relation instance. For two sets of attributes X and Y, an information dependency measure (InD measure) characterizes the uncertainty remaining about the values for the set Y when the values for the set X are known. A variety of arithmetic inequalities (InD inequalities) are shown to hold among InD measures; InD inequalities hold in any relation instance. Numeric constraints (InD constraints) on InD measures, consistent with the InD inequalities, can be applied to relation instances. Remarkably, functional and multivalued dependencies correspond to setting certain constraints to zero, with Armstrong's axioms shown to be consequences of the arithmetic inequalities applied to constraints. As an analog of completeness, for any set of constraints consistent with the inequalities, we may construct a relation instance that approximates these constraints within any positive &egr;. InD measures suggest many valuable applications in areas such as data mining.\"],\n",
       "   '2000']],\n",
       " [[['Integrity Constraints for XML.',\n",
       "    'Integrity constraints have proved fundamentally important in database management. The ID/IDREF mechanism provided by XML DTDs relies on a simple form of constraints to describe references. Yet, this mechanism is sufficient neither for specifying references in XML documents, nor for expressing semantic constraints commonly found in databases. In this paper, we extend XML DTDs with several classes of integrity constraints and investigate the complexity of reasoning about these constraints. The constraints range over keys, foreign keys, inverse constraints as well as ID constraints for capturing the semantics of object identities. They improve semantic specifications and provide a better reference mechanism for native XML applications. They are also useful in information exchange and data integration for preserving the semantics of data originating in relational and object-oriented databases. We establish complexity and axiomatization results for the (finite) implication problems associated with these constraints. In addition, we study implication of more general constraints, such as functional, inclusion and inverse constraints defined in terms of navigation paths.'],\n",
       "   '2000']],\n",
       " [[['Linear Approximation of Planar Spatial Databases Using Transitive-Closure Logic.',\n",
       "    'We consider spatial databases in the plane that can be defined by polynomial constraint formulas. Motivated by applications in geographic information systems, we investigate linear approximations of spatial databases and study in which language they can be expressed effectively. Specifically, we show that they cannot be expressed in the standard first-order query language for polynomial constraint databases but that an extension of this first-order language with transitive closure suffices to express the approximation query in an effective manner. Furthermore, we introduce an extension of transitive-closure logic and show that this logic is complete for the computable queries on linear spatial databases. This result together with our first result implies that this extension of transitive-closure logic can express all computable topological queries on arbitrary spatial databases in the plane.'],\n",
       "   '2000']],\n",
       " [[['On the Content of Materialized Aggregate Views.',\n",
       "    'We consider the problem of rewriting queries using only materialized views. We first show that if the views subsume the query from the point of view of the information content, then the query can be rewritten using only the views, but the resulting query might be extremely inefficient. We then focus on aggregate views and queries over a single relation, which are fundamental in many applications such as data warehousing. We show that in this case, it is possible to guarantee that as soon as the views subsume the query, it can be rewritten in terms of the views in a simple query language. Our main contribution is the conception of rewriting algorithms which run in polynomial time, and the proof of their completeness which relies on combinatorial arguments. Finally, we consider the materialization of ratio views such as average and percentage, important for the design of materialized views.'],\n",
       "   '2000']],\n",
       " [[['Auditing Boolean Attributes.',\n",
       "    'We study the problem of auditing databases which support statistical sum queries to protect the security of sensitive information; we focus on the special case in which the sensitive information is Boolean. Principles and techniques developed for the security of statistical databases in the case of continuous attributes do not apply here. We prove certain strong complexity results suggesting that there is no general efficient solution for the auditing problem in this case. We propose two efficient algorithms: The first is applicable when the sum queries are one-dimensional range queries (we prove that the problem is NP-hard even in the two-dimensional case). The second is an approximate algorithm that maintains security, although it may be too restrictive. Finally, we consider a \"dual\" variant, with continuous data but an aggregate function that is combinatorial in nature. Specifically, we provide algorithms for two natural definitions of the auditing condition when the aggregate function is MAX.'],\n",
       "   '2000']],\n",
       " [[['Query Containment for Data Integration Systems.',\n",
       "    'The problem of query containment is fundamental to many aspects of database systems, including query optimization, determining independence of queries from updates, and rewriting queries using views. In the data-integration framework, however, the standard notion of query containment does not suffice. We define relative containment, which formalizes the notion of query containment relative to the sources available to the data-integration system. First, we provide optimal bounds for relative containment for several important classes of datalog queries, including the common case of conjunctive queries. Next, we provide bounds for the case when sources enforce access restrictions in the form of binding pattern constraints. Surprisingly, we show that relative containment for conjunctive queries is still decidable in this case, even though it is known that finding all answers to such queries may require a recursive datalog program over the sources. Finally, we provide tight bounds for variants of relative containment when the queries and source descriptions may contain comparison predicates.'],\n",
       "   '2000']],\n",
       " [[['Fixed-Point Query Languages for Linear Constraint Databases.',\n",
       "    'We introduce a family of query languages for linear constraint databases over the reals. The languages are defined over two-sorted structures, the first sort being the real numbers and the second sort consisting of a decomposition of the input relation into regions. The languages are defined as extensions of first-order logic by transitive closure or fixed-point operators, where the fixed-point operators are defined over the set of regions only. It is shown that the query languages capture precisely the queries definable in various standard complexity classes including PTIME.'],\n",
       "   '2000']],\n",
       " [[['Typechecking for XML Transformers.',\n",
       "    'We study the typechecking problem for XML (eXtensible Markup Language) transformers: given an XML transformation program and a DTD for the input XML documents, check whether every result of the program conforms to a specified output DTD. We model XML transformers using a novel device called a k-pebble transducer, that can express most queries without data-value joins in XML-QL, XSLT, and other XML query languages. Types are modeled by regular tree languages, a robust extension of DTDs. The main result of the paper is that typechecking for k-pebble transducers is decidable. Consequently, typechecking can be performed for a broad range of XML transformation languages, including XML-QL and a fragment of XSLT.'],\n",
       "   '2000']],\n",
       " [[['The Web as a Graph.',\n",
       "    'The pages and hyperlinks of the World-Wide Web may be viewed as nodes and edges in a directed graph. This graph has about a billion nodes today, several billion links, and appears to grow exponentially with time. There are many reasons&mdash;mathematical, sociological, and commercial&mdash;for studying the evolution of this graph. We first review a set of algorithms that operate on the Web graph, addressing problems from Web search, automatic community discovery, and classification. We then recall a number of measurements and properties of the Web graph. Noting that traditional random graph models do not explain these observations, we propose a new family of random graph models.'],\n",
       "   '2000']],\n",
       " [[['Traversing Itemset Lattice with Statistical Metric Pruning.',\n",
       "    'We study how to efficiently compute significant association rules according to common statistical measures such as a chi-squared value or correlation coefficient. For this purpose, one might consider to use of the Apriori algorithm, but the algorithm needs major conversion, because none of these statistical metrics are anti-monotone, and the use of higher support for reducing the search space cannot guarantee solutions in its the search space. We here present a method of estimating a tight upper bound on the statistical metric associated with any superset of an itemset, as well as the novel use of the resulting information of upper bounds to prune unproductive supersets while traversing itemset lattices. Experimental tests demonstrate the efficiency of this method.'],\n",
       "   '2000']],\n",
       " [[['DTD Inference for Views of XML Data.',\n",
       "    'We study the inference of Data Type Definitions (DTDs) for views of XML data, using an abstraction that focuses on document content structure. The views are defined by a query language that produces a list of documents selected from one or more input sources. The selection conditions involve vertical and horizontal navigation, thus querying explicitly the order present in input documents. We point several strong limitations in the descriptive ability of current DTDs and the need for extending them with (i) a subtyping mechanism and (ii) a more powerful specification mechanism than regular languages, such as context-free languages. With these extensions, we show that one can always infer tight DTDs, that precisely characterize a selection view on sources satisfying given DTDs. We also show important special cases where one can infer a tight DTD without requiring extension (ii). Finally we consider related problems such as verifying conformance of a view definition with a predefined DTD. Extensions to more powerful views that construct complex documents are also briefly discussed.'],\n",
       "   '2000']],\n",
       " [[['Verification of Relational Transducers for Electronic Commerce.',\n",
       "    \"Motivated by recent work of Abiteboul, Vianu, Fordham, and Yesha, we investigate the verifiability of transaction protocols specifying the interaction of multiple parties via a network. The protocols which we are concerned with typically occur in the context of electronic commerce applications and can be formalized as relational transducers. We introduce a class of powerful relational transducers based on Gurevich's abstract state machines and show that several verification problems related to electronic commerce applications are decidable for these transducers.\"],\n",
       "   '2000']],\n",
       " [[['Constraint Satisfaction and Database Theory: a Tutorial.',\n",
       "    'A large class of problems in AI and other areas of computer science can be viewed as constraint-satisfaction problems. This includes problems in machine vision, belief maintenance, scheduling, temporal reasoning, type reconstruction, graph theory, and satisfiability. In general, the constraint satisfaction-problem is NP-complete, so searching for tractable cases is an active research area. It turns out that constraint satisfaction has an intimate connection with database theory: constraint-satisfaction problems can be recast as database problems and database problems can be recast as constraint-satisfaction problems. In this tutorial, I will cover the fundamentals of constraints satisfaction and describe its intimate relationship with database theory from various perspectives.'],\n",
       "   '2000']],\n",
       " [[['Anytime, Anywhere: Modal Logics for Mobile Ambients.',\n",
       "    'The Ambient Calculus is a process calculus where processes may reside within a hierarchy of locations and modify it. The purpose of the calculus is to study mobility, which is seen as the change of spatial configurations over time. In order to describe properties of mobile computations we devise a modal logic that can talk about space as well as time, and that has the Ambient Calculus as a model.'],\n",
       "   '2000']],\n",
       " [[['Authentication Primitives and Their Compilation.',\n",
       "    'Adopting a programming-language perspective, we study the problem of implementing authentication in a distributed system. We define a process calculus with constructs for authentication and show how this calculus can be translated to a lower-level language using marshaling, multiplexing, and cryptographic protocols. Authentication serves for identity-based security in the source language and enables simplifications in the translation. We reason about correctness relying on the concepts of observational equivalence and full abstraction.'],\n",
       "   '2000']],\n",
       " [[['Transforming Out Timing Leaks.',\n",
       "    'One aspect of security in mobile code is privacy: private (or secret) data should not be leaked to unauthorised agents. Most of the work on secure information flow has until recently only been concerned with detecting direct and indirect flows. Secret information can however be leaked to the attacker also through covert channels. It is very reasonable to assume that the attacker, even as an external observer, can monitor the timing (including termination) behaviour of the program. Thus to claim a program secure, the security analysis must take also these into account. In this work we present a surprisingly simple solution to the problem of detecting timing leakages to external observers. Our system consists of a type system in which well-typed programs do not leak secret information directly, indirectly or through timing, and a transformation for removing timing leakages. For any program that is well typed according to Volpano and Smith [VS97a], our transformation generates a program that is also free of timing leaks.'],\n",
       "   '2000']],\n",
       " [[['Modular Refinement of Hierarchic Reactive Machines.',\n",
       "    'Scalable formal analysis of reactive programs demands integration of modular reasoning techniques with existing analysis tools. Modular reasoning principles such as abstraction, compositional refinement, and assume-guarantee reasoning are well understood for architectural hierarchy that describes the communication structure between component processes, and have been shown to be useful. In this paper, we develop the theory of modular reasoning for behavior hierarchy that describes control structure using hierarchic modes. From Statecharts to UML, behavior hierarchy has been an integral component of many software design languages, but only syntactically. We present the hierarchic reactive modules language that retains powerful features such as nested modes, mode reuse, exceptions, group transitions, history, and conjunctive modes, and yet has a semantic notion of mode hierarchy. We present an observational trace semantics for modes that provides the basis for mode refinement. We show the refinement to be compositional with respect to the mode constructors, and develop an assume-guarantee reasoning principle.'],\n",
       "   '2000']],\n",
       " [[['A Semantic Model of Types and Machine Instuctions for Proof-Carrying Code.',\n",
       "    'Proof-carrying code is a framework for proving the safety of machine-language programs with a machine-checkable proof. Previous PCC frameworks have defined type-checking rules as part of the logic. We show a universal type framework for proof-carrying code that will allow a code producer to choose a programming language, prove the type rules for that language as lemmas in higher-order logic, then use those lemmas to prove the safety of a particular program. We show how to handle traversal, allocation, and initialization of values in a wide variety of types, including functions, records, unions, existentials, and covariant recursive types.'],\n",
       "   '2000']],\n",
       " [[['(Optimal) Duplication is not Elementary Recursive.',\n",
       "    \"In 1998, Asperti and Mairson proved that the cost of reducing a λ-term using an optimal λ-reducer (a la Lévy) cannot be bound by any elementary function in the number of shared-beta steps. We prove in this paper that an analogous result holds for Lamping's abstract algorithm. That is, there is no elementary function in the number of shared beta steps bounding the number of duplication steps of the optimal reducer. This theorem vindicates the oracle of Lamping's algorithm as the culprit for the negative result of Asperti and Mairson. The result is obtained using as a technical tool Elementary Affine Logic.\"],\n",
       "   '2000']],\n",
       " [[['Resource Bound Certification.',\n",
       "    'Various code certification systems allow the certification and static verification of important safety properties such as memory and control-flow safety. These systems are valuable tools for verifying that untrusted and potentially malicious code is safe before execution. However, one important safety property that is not usually included is that programs adhere to specific bounds on resource consumption, such as running time. We present a decidable type system capable of specifying and certifying bounds on resource consumption. Our system makes two advances over previous resource bound certification systems, both of which are necessary for a practical system: We allow the execution time of programs and their subroutines to vary, depending on their arguments, and we provide a fully automatic compiler generating certified executables from source-level programs. The principal device in our approach is a strategy for simulating dependent types using sum and inductive kinds.'],\n",
       "   '2000']],\n",
       " [[['A Debate on Language and Tool Support for Design Patterns.',\n",
       "    \"Design patterns have earned a place in the developer's arsenal of tools and techniques for software development. They have proved so useful, in fact, that some have called for their promotion to programming language features. In turn this has rekindled the age-old debate over the mechanism that belong in programming languages versus those that are better served by tools. The debate comes full circle when one contemplates code generation and methodological tool support for patterns. The authors compare and contrast programming languages, tools, and patterns to assess their relative merits and to clarify their roles in the development process.\"],\n",
       "   '2000']],\n",
       " [[['Paths vs. Trees in Set-Based Program Analysis.',\n",
       "    'Set-based analysis of logic programs provides an accurate method for descriptive type-checking of logic programs. The key idea of this method is to upper approximate the least model of the program by a regular set of trees. In 1991, Fr&uuml;hwirth, Shapiro, Vardi and Yardeni raised the question whether it can be more efficient to use the domain of sets of paths instead, i.e., to approximate the least model by a regular set of words. We answer the question negatively by showing that type-checking for path-based analysis is as hard as the set-based one, that is DEXPTIME-complete. This result has consequences also in the areas of set constraints, automata theory and model checking.'],\n",
       "   '2000']],\n",
       " [[['Reducing Sweep Time for a Nearly Empty Heap.',\n",
       "    'Mark and sweep garbage collectors are known for using time proportional to the heap size when sweeping memory, since all objects in the heap, regardless of whether they are live or not, must be visited in order to reclaim the memory occupied by dead objects. This paper introduces a sweeping method which traverses only the live objects, so that sweeping can be done in time dependent only on the number of live objects in the heap. This allows each collection to use time independent of the size of the heap, which can result in a large reduction of overall garbage collection time in empty heaps. Unfortunately, the algorithm used may slow down overall garbage collection if the heap is not so empty. So a way to select the sweeping algorithm depending on the heap occupancy is introduced, which can avoid any significant slowdown.'],\n",
       "   '2000']],\n",
       " [[['Enforcing Trace Properties by Program Transformation.',\n",
       "    'We propose an automatic method to enforce trace properties on programs. The programmer specifies the property separately from the program; a program transformer takes the program and the property and automatically produces another &ldquo;equivalent&rdquo; pogram satisfying the property. This separation of concerns makes the program easier to develop and maintain. Our approach is both static and dynamic. It integrates static analyses in order to avoid useless transformations. On the other hand, it never rejects programs but adds dynamic checks when necessary. An important challenge is to make this dynamic enforcement as inexpensive as possible. The most obvious application domain is the enforcement of security policies. In particular, a potential use of the method is the securization of mobile code upon receipt.'],\n",
       "   '2000']],\n",
       " [[['Efficient Algorithms for pre and post on Interprocedural Parallel Flow Graphs.',\n",
       "    \"This paper is a contribution to the already existing series of work on the algorithmic principles of interprocedural analysis. We consider the generalization to the case of parallel programs. We give algorithms that compute the sets of backward resp. forward reachable configurations for parallel flow graph systems in linear time in the size of the graph viz. the program. These operations are important in dataflow analysis and in model checking. In our method, we first model configurations as terms (viz. trees) in the process algebra PA that can express call stack operations and parallelism. We then give a 'declarative' Horn-clause specification of the sets of predecessors resp. successors. The 'operational' computation of these sets is carried out using the Dowling-Gallier procedure for HornSat.\"],\n",
       "   '2000']],\n",
       " [[['Temporal Abstract Interpretation.',\n",
       "    'We study the abstract interpretation of temporal calculi and logics in a general syntax, semantics and abstraction independent setting. This is applied to the @@@@-calculus, a generalization of the &mgr;-calculus with new reversal and abstraction modalities as well as a new time-symmetric trace-based semantics. The more classical set-based semantics is shown to be an abstract interpretation of the trace-based semantics which leads to the understanding of model-checking and its application to data-flow analysis as incomplete temporal abstract interpretations. Soundness and incompleteness of the abstractions are discussed. The sources of incompleteness, even for finite systems, are pointed out, which leads to the identification of relatively complete sublogics, &agrave; la CTL.'],\n",
       "   '2000']],\n",
       " [[['Generalized Certificate Revocation.',\n",
       "    'We introduce a language for creating and manipulating certificates, that is, digitally signed data based on public key cryptography, and a system for revoking certificates. Our approach provides a uniform mechanism for secure distribution of public key bindings, authorizations, and revocation information. An external language for the description of these and other forms of data is compiled into an intermediate language with a well-defined denotational and operational semantics. The internal language is used to carry out consistency checks for security, and optimizations for efficiency. Our primary contribution is a technique for treating revocation data dually to other sorts of information using a polarity discipline in the intermediate language.'],\n",
       "   '2000']],\n",
       " [[['A New Approach to Generic Functional Programming.',\n",
       "    \"This paper describes a new approach to generic functional programming, which allows us to define functions generically for all datatypes expressible in Haskell. A generic function is one that is defined by induction on the structure of types. Typical examples include pretty printers, parsers, and comparison functions. The advanced type system of Haskell presents a real challenge: datatypes may be parameterized not only by types but also by type constructors, type definitions may involve mutual recursion, and recursive calls of type constructors can be arbitrarily nested. We show that&mdash;despite this complexity&mdash;a generic function is uniquely defined by giving cases for primitive types and type constructors (such as disjoint unions and cartesian products). Given this information a generic function can be specialized to arbitrary Haskell datatypes. The key idea of the approach is to model types by terms of the simply typed &lgr;-calculus augmented by a family of recursion operators. While conceptually simple, our approach places high demands on the type system: it requires polymorphic recursion, rank-n types, and a strong form of type constructor polymorphism. Finally, we point out connections to Haskell's class system and show that our approach generalizes type classes in some respects.\"],\n",
       "   '2000']],\n",
       " [[['Controlling Interference in Ambients.',\n",
       "    \"Two forms of interferences are individuated in Cardelli and Gordon's Mobile Ambients (MA): plain interferences, which are similar to the interferences one finds in CCS and &phgr;-calculus; and grave interferences, which are more dangerous and may be regarded as programming errors. To control interferences, the MA movement primitives are modified. On the new calculus, the Mobile Safe Ambients (SA), a type system is defined that: controls the mobility of ambients; removes all grave interferences. Other advantages of SA are: a useful algebraic theory; programs sometimes more robust (they require milder conditions for correctness) and/or simpler. These points are illustrated on several examples.\"],\n",
       "   '2000']],\n",
       " [[['Type Elaboration and Subtype Completion for Java Bytecode.',\n",
       "    'Java source code is strongly typed, but the translation from Java source to bytecode omits much of the type information originally contained within methods. Type elaboration is a technique for reconstructing strongly typed programs from incompletely typed bytecode by inferring types for local variables. There are situations where, technically, there are not enough types in the original type hierarchy to type a bytecode program. Subtype completion is a technique for adding necessary types to an arbitrary type hierarchy to make type elaboration possible for all verifiable Java bytecode. Type elaboration with subtype completion has been implemented as part of the Marmot Java compiler.'],\n",
       "   '2000']],\n",
       " [[['Semantics-Preserving Procedure Extraction.',\n",
       "    'Procedure extraction is an important program transformation that can be used to make programs easier to understand and maintain, to facilitate code reuse, and to convert &ldquo;monolithic&rdquo; code to modular or object-oriented code. Procedure extraction involves the following steps: The statements to be extracted are identified (by the programmer or by a programming tool). If the statements are not contiguous, they are moved together so that they form a sequence that can be extracted into a procedure, and so that the semantics of the original code is preserved. The statements are extracted into a new procedure, and are replaced with an appropriate call. This paper addresses step 2: in particular, the conditions under which it is possible to move a set of selected statements together so that they become &ldquo;extractable&rdquo;, while preserving semantics. Since semantic equivalence is, in general, undecidable, we identify sufficient conditions based on control and data dependences, and define an algorithm that moves the selected statements together when the conditions hold. We also include an outline of a proof that our algorithm is semantics-preserving. While there has been considerable previous work on procedure extraction, we believe that this is the first paper to provide an algorithm for semantics-preserving procedures extraction given an arbitrary set of selected statements in an arbitrary control-flow graph.'],\n",
       "   '2000']],\n",
       " [[['Shape Analysis for Mobile Ambients.',\n",
       "    'The ambient calculus is a calculus of computation that allows active processes to move between sites. We present an analysis inspired by state-of-the-art pointer analyses that safety and accurately predicts which processes may turn up at what sites during the execution of a composite system. The analysis models sets of processes by sets of regular tree grammars enhanced with context-dependent counts, and it obtains its precision by combining a powerful redex materialisation with a strong redex reduction (in the manner of the strong updates performed in pointer analyses). The underlying ideas are flexible and scale up to general tree structures admitting powerful restructuring operations.'],\n",
       "   '2000']],\n",
       " [[['Sparse Code Motion.',\n",
       "    'In this article, we add a third dimension to partial redundancy elimination by considering code size as a further optimization goal in addition to the more classical consideration of computation costs and register pressure. This results in a family of sparse code motion algorithms coming as modular extensions of the algorithms for busy and lazy code motion. Each of them optimally captures a predefined choice of priority between these three optimization goals, e.g. code size can be minimized while (1) guaranteeing at least the performance of the argument program, or (2) even computational optimality. Each of them can further be refined to simultaneously reduce the lifetimes of temporaries to a minimum. These algorithms are well-suited for size-critical application areas like smart cards and embedded systems, as they provide a handle to control the code replication problem of classical code motion techniques. In fact, we believe that our systematic, priority-based treatment of trade-offs between optimization goals may substantially decrease development costs of size-critical applications: users may &ldquo;play&rdquo; with the priorities until the algorithm automatically delivers a satisfactory solution.'],\n",
       "   '2000']],\n",
       " [[['Projection Merging: Reducing Redundancies in Inclusion Constraint Graphs.',\n",
       "    'Inclusion-based program analyses are implemented by adding new edges to directed graphs. In most analyses, there are many different ways to add a transitive edge between two nodes, namely through each different path connecting the nodes. This path redundancy limits the scalability of these analyses. We present projection merging, a technique to reduce path redundancy. Combined with cycle elimination [7], projection merging achieves orders of magnitude speedup of analysis time on programs over that of using cycle elimination alone.'],\n",
       "   '2000']],\n",
       " [[['Verifying Secrets and Relative Secrecy.',\n",
       "    'Systems that authenticate a user based on a shared secret (such as a password or PIN) normally allow anyone to query whether the secret is a given value. For example, an ATM machine allows one to ask whether a string is the secret PIN of a (lost or stolen) ATM card. Yet such queries are prohibited in any model whose programs satisfy an information-flow property like Noninterference. But there is complexity-based justification for allowing these queries. A type system is given that provides the access control needed to prove that no well-typed program can leak secrets in polynomial time, or even leak them with nonnegligible probability if secrets are of sufficient length and randomly chosen. However, there are well-typed deterministic programs in a synchronous concurrent model capable of leaking secrets in linear time.'],\n",
       "   '2000']],\n",
       " [[['A Type System for Expressive Security Policies.',\n",
       "    \"{\\\\em Certified code} is a general mechanism for enforcing security properties. In this paradigm, untrusted agent code carries annotations that allow a host to verify its trustworthiness. Before running the agent, the host checks the annotations and proves that they imply the host''s security policy. Despite the flexibility of this scheme, so far, compilers that generate proof-carrying code have focused on simple memory and control-flow safety rather than more general security properties. {\\\\em Security automata} can enforce an expressive collection of security policies including access control policies and resource bounds policies. In this paper, we show how to take specifications in the form of security automata and automatically transform them into signatures for a typed lambda calculus that will enforce the corresponding safety property. Moreover, we describe how to instrument typed source language programs with security checks and typing annotations so that the resulting programs are provably secure and can be mechanically checked. This work provides a foundation for the process of automatically generating secure certified code in a type-theoretic framework.\"],\n",
       "   '2000']],\n",
       " [[['Enhanced sharing analysis techniques: a comprehensive evaluation.',\n",
       "    '$\\\\textup{\\\\textsf{Sharing}}$, an abstract domain developed by D. Jacobs and A. Langen for the analysis of logic programs, derives useful aliasing information. It is well-known that a commonly used core of techniques, such as the integration of $\\\\textup{\\\\textsf{Sharing}}$ with freeness and linearity information, can significantly improve the precision of the analysis. However, a number of other proposals for refined domain combinations have been circulating for years. One feature that is common to these proposals is that they do not seem to have undergone a thorough experimental evaluation even with respect to the expected precision gains. In this paper we experimentally evaluate: helping $\\\\textup{\\\\textsf{Sharing}}$ with the definitely ground variables found using $\\\\textit{Pos}$, the domain of positive Boolean formulas; the incorporation of explicit structural information; a full implementation of the reduced product of $\\\\textup{\\\\textsf{Sharing}}$ and $\\\\textit{Pos}$; the issue of reordering the bindings in the computation of the abstract $\\\\mgu$; an original proposal for the addition of a new mode recording the set of variables that are deemed to be ground or free; a refined way of using linearity to improve the analysis; the recovery of hidden information in the combination of $\\\\textup{\\\\textsf{Sharing}}$ with freeness information. Finally, we discuss the issue of whether tracking compoundness allows the computation of more sharing information.'],\n",
       "   '2000']],\n",
       " [[['A precise type analysis of logic programs.',\n",
       "    'This paper presents a new type analysis for logic programs. The type information in a set of substitutions is described by a disjunction of variable typings each of which maps a variable to a non-deterministic regular type. The use of non-deterministic regular types, set union and intersection operators, and disjunctions of variable typings makes the new type analysis more precise than those found in the literature. Experimental results on the performance of the new analysis are given together with comparable results from an existing type analysis. The fundamental problem of checking the emptiness of non-deterministic regular types is more complex in the new analysis. The experimental results, however, show that careful use of tabling reduces the effect to an average of 15% of execution time on a set of benchmarks.'],\n",
       "   '2000']],\n",
       " [[['Argumentation Semantics for Defeasible Logics.',\n",
       "    'Defeasible logic is a simple but efficient rule-based nonmonotonic logic. It has powerful implementations and shows promise to be applied in the areas of legal reasoning and the modelling of business rules. So far defeasible logic has been defined only proof-theoretically. Argumentation-based semantics have become popular in the area of logic programming. In this paper we give an argumentation-based semantics for defeasible logic. Recently it has been shown that a family of approaches can be built around defeasible logic, in which different intuitions can be followed. In this paper we present an argumentation-based semantics for an ambiguity propagating logic, too. Further defeasible logics can be characterised in a similar way.'],\n",
       "   '2000']],\n",
       " [[['The Lumberjack Algorithm for Learning Linked Decision Forests.',\n",
       "    'While the decision tree is an effective representation that has been used in many domains, a tree can often encode a concept inefficiently. This happens when the tree has to represent a subconcept multiple times in different parts of the tree. In this paper we introduce a new representation based on trees, the linked decision forest, that does not need to repeat internal structure. We also introduce a supervised learning algorithm. Lumberjack, that uses the new representation. We then show empirically that Lumberjack improves generalization accuracy on hierarchically decomposable concepts.'],\n",
       "   '2000']],\n",
       " [[['Integrated constraints and inheritance in DTAC.',\n",
       "    'Inheritance and constraints are two common techniques for safely managing the complexity of large access control configurations. Inheritance is used to help factor the model, while constraints are used to help ensure that the complexity will not result in an unsafe configuration arising in the future evolution of the system. In this paper we develop an integrated mathematical approach to defining both inheritance and constraints in the dynamically typed access control (DTAC) model. In the process we identify several useful relationships among DTAC objects. The combination of DTAC and our new relationships allow us to graphically construct a greater variety and complexity of efficiently verifiable separation of duty constraints than any other model we are aware of.'],\n",
       "   '2000']],\n",
       " [[['TRBAC: a temporal role-based access control model.',\n",
       "    'Role-based access control (RBAC) models are receiving increasing attention as a generalized approach to access control. Roles may be available to users at certain time periods, and unavailable at others. Moreover, there can be temporal dependencies among roles. To tackle such dynamic aspects, we introduce Temporal-RBAC (TRBAC), an extension of the RBAC model. TRBAC supports periodic role enabling and disabling---possibly with individual exceptions for particular users---and temporal dependencies among such actions, expressed by means of role triggers. Role trigger actions may be either immediately executed, or deferred by an explicitly specified amount of time. Enabling and disabling actions may be given a priority, which is used to solve conflicting actions. A formal semantics for the specification language is provided, and a polynomial safeness check is introduced to reject ambiguous or inconsistent specifications. Finally, a system implementing TRBAC on top of a conventional DBMS is presented.'],\n",
       "   '2000']],\n",
       " [[['Process-oriented approach for role-finding to implement role-based security administration in a large industrial organization.',\n",
       "    'In this paper we describe the work in progress with a process-oriented approach for role-finding to implement Role-Based Security Administration. Our results stem from using a recently proposed role model and procedural model at Siemens AG ICN, a large industrial organization. The core of this paper presents the data model, which integrates business processes, role based security administration and access control. Moreover, a structured top-down approach is outlined which is the basis for derivation of suitable business roles from enterprise process models. A brief description is given on how these results may be used to first build the Role Catalog and then support the implementation of RBAC and a single point of administration and control, using a cross-platform administration tool.'],\n",
       "   '2000']],\n",
       " [[['Engineering authority and trust in cyberspace: the OM-AM and RBAC way.',\n",
       "    'Information systems of the future will be large-scale, highly decentralized, pervasive, span organizational boundaries and evolve rapidly. Effective security in this cyberspace will require engineering authority and trust retationships across organizations and individuals. In this paper we propose the four-layer OM-AM framework for this purpose. OM-AM comprises objective, model, architecture and mechanism layers in this sequence. The objective and model (OM) layers articulate whatthe security objective and tradeoffs are, while the architecture and mechanism (AM) layers address howto meet these requirements. The hyphen in OM-AM emphasizes the shift from what to how. These layers are roughly analogous to a network protocol stack with a many-to-many relationship between successive layers, and most certainly do not imply a top-down waterfall-style software engineering process. OM-AM is an excellent match to the policy-neutral and flexible nature of role-based access control (RBAC). This paper describes and motivates the OM-AM framework and presents a case study in applying it in a distributed RBAC application.'],\n",
       "   '2000']],\n",
       " [[['The NIST model for role-based access control: towards a unified standard.',\n",
       "    'This paper describes a unified model for role-based access control (RBAC). RBAC is a proven technology for large-scale authorization. However, lack of a standard model results in uncertainty and confusion about its utility and meaning. The NIST model seeks to resolve this situation by unifying ideas from prior RBAC models, commercial products and research prototypes. It is intended to serve as a foundation for developing future standards. RBAC is a rich and open-ended technology which is evolving as users, researchers and vendors gain experience with it. The NIST model focuses on those aspects of RBAC for which consensus is available. It is organized into four levels of increasing functional capabilities called flat RBAC, hierarchical RBAC, constrained RBAC and symmetric RBAC. These levels are cumulative and each adds exactly one new requirement. An alternate approach comprising flat and hierarchical RBAC in an ordered sequence and two unordered features&mdash;constraints and symmetry&mdash;is also presented. The paper furthermore identifies important attributes of RBAC not included in the NIST model. Some are not suitable for inclusion in a consensus document. Others require further work and agreement before standardization is feasible.'],\n",
       "   '2000']],\n",
       " [[['Voltage-Clock-Scaling Adaptive Scheduling Techniques for Low Power in Hard Real-Time Systems.',\n",
       "    'Abstract--Many embedded systems operate under severe power and energy constraints. Voltage clock scaling is one mechanism by which energy consumption may be reduced: It is based on the fact that power consumption is a quadratic function of the voltage, while the speed is a linear function. In this paper, we show how voltage scaling can be scheduled to reduce energy usage while still meeting real-time deadlines.'],\n",
       "   '2000']],\n",
       " [[['A Coordination Lanuage for Mobile Components.',\n",
       "    'Abstract In this paper we present the sigmapi coordination language, a core language for specifying dynamic networks of components. The language is inspired by the Manifold coordination language and by the pi-calculus. The main concepts of the language are components, classes, objects and channels. A program in sigmapi consists of a number of components, where each component is a collection of classes separable from its original context and re-usable in any other context. An object is an instance of a class that executes in parallel with the other objects active in the system. The sigmapi language differs from other models of object-oriented systems mainly in its treatment of communication and mobility: communication is anonymous via synchronous or asynchronous channels, while mobility is obtained by moving channels in the virtual space of linked objects. Thus, a channel is a transferable capability of communication, and objects are mobile in the sense that their communication possibilities may change during a computation. The language sigmapi itself does not impose exogenous coordination, meaning that the coordination primitives affecting each object can be executed within the computation of the object itself. However, only simple restrictions on the class-definitions of a sigmapi program suffice to enforce a separation between computation and coordination. Interaction typically occurs anonymously and under the full control of the objects involved. This make it easier to deal with Internet application where security policies must be enforced in view of the possibilities of attacks.'],\n",
       "   '2000']],\n",
       " [[['Task Scheduling using a Block Dependency DAG for Block-Oriented Sparse Cholesky Factorization.',\n",
       "    'Block-oriented sparse Cholesky factorization decomposes a sparse matrix into rectangular subblocks; each block can then be handled as a computational unit in order to increase data reuse in a hierarchical memory system. Also, the factorization method increases the degree of concurrency and reduces the overall communication volume so that it performs more efficiently on a distributed-memory multiprocessor system than the customary column-oriented factorization method. But until now, mapping of blocks to processors has been designed for load balance with restricted communication patterns. In this paper, we represent tasks using a block dependency DAG that represents the execution behavior of block sparse Cholesky factorization in a distributed-memory system. Since the characteristics of tasks for block Cholesky factorization are different from those of the conventional parallel task model, we propose a new task scheduling algorithm using a block dependency DAG. The proposed algorithm consists of two stages: early-start clustering, and affined cluster mapping (ACM). The early-start clustering stage is used to cluster tasks while preserving the earliest start time of a task without limiting parallelism. After task clustering, the ACM stage allocates clusters to processors considering both communication cost and load balance. Experimental results on a Myrinet cluster system show that the proposed task scheduling approach outperforms other processor mapping methods.'],\n",
       "   '2000']],\n",
       " [[['Towards Practical Non-interactive Public Key Cryptosystems Using Non-maximal Imaginary Quadratic Orders.',\n",
       "    'We present a new non-interactive public-key distribution system based on the class group of a non-maximal imaginary quadratic order Cl( &Delta;p). The main advantage of our system over earlier proposals based on (Z}/nZ)* [25,27] is that embedding id information into group elements in a cyclic subgroup of the class group is easy (straight-forward embedding into prime ideals suffices) and secure, since the entire class group is cyclic with very high probability. Computational results demonstrate that a key generation center (KGC) with modest computational resources can set up a key distribution system using reasonably secure public system parameters. &emsp;In order to compute discrete logarithms in the class group, the KGC needs to know the prime factorization of &Delta;p&equals;&Delta;1 p2. We present an algorithm for computing discrete logarithms in Cl(&Delta;p) by reducing the problem to computing discrete logarithms in Cl(&Delta;1) and either F*p or F*p2. Our algorithm is a specific case of the more general algorithm used in the setting of ray class groups [5]. We prove&mdash;for arbitrary non-maximal orders&mdash;that this reduction to discrete logarithms in the maximal order and a small number of finite fields has polynomial complexity if the factorization of the conductor is known.'],\n",
       "   '2000']],\n",
       " [[['Compiling Embedded Languages.',\n",
       "    \"Functional languages are particularly well-suited to the interpretive implementations of Domain-Specific Embedded Languages (DSELs). We describe an implemented technique for producing optimizing compilers for DSELs, based on Kamin's idea of DSELs for program generation. The technique uses a data type of syntax for basic types, a set of smart constructors that perform rewriting over those types, some code motion transformations, and a back-end code generator. Domain-specific optimization results from chains of domain-independent rewrites on basic types. New DSELs are defined directly in terms of the basic syntactic types, plus host language functions and tuples. This definition style makes compilers easy to write and, in fact, almost identical to the simplest embedded interpreters. We illustrate this technique with a language Pan for the computationally intensive domain of image synthesis and manipulation.\"],\n",
       "   '2000']],\n",
       " [[['Specification and Correctness of Lambda Lifting.',\n",
       "    'We present a formal and general specification of lambda lifting and prove its correctness with respect to a call-by-name operational semantics. We use this specification to prove the correctness of a lambda lifting algorithm similar to the one proposed by Johnsson. Lambda lifting is a program transformation that eliminates free variables from functions by introducing additional formal parameters to function definitions and additional actual parameters to function calls. This operation supports the transformation from a lexically-structured functional program into a set of recursive equations. Existing results provide specific algorithms and only limited correctness results. Our work provides a more general specification of lambda lifting (and related operations) that supports flexible translation strategies, which may result in new implementation techniques. Our work also supports a simple framework in which the interaction of lambda lifting and other optimizations can be studied and from which new algorithms might be obtained.'],\n",
       "   '2000']],\n",
       " [[['Tiling Imperfectly-Nested Loop Nests.',\n",
       "    'Tiling is one of the more important transformations for enhancing loca lity of reference in programs. Intuitively, tiling a set of loops achieves the effect of interleaving iterations of these loops. Tiling of perfectly-nested loop nests (which are loop nests in which all assignment statements are contained in the innermost loop) is well understood. In practice, many loop nests are imperfectly nested, so existing compilers use heuristics to try to find a sequence of transformations that convert such loop nests into perfectly-nested ones, but these heuristics do not always succeed. In this paper, we propose a novel approach to tiling imperfectly-nested loop nests. The key idea is to embed the iteration space of every statement in the imperfectly-nested loop nest into a special space called the product space which is tiled to produce the final code. We evaluate the effectiveness of this approach for dense numrical linear algebra benchmarks, relaxation codes, and the tomcatv code from the SPEC benchmarks. No other single approach in the literature can tile all these codes automatically.'],\n",
       "   '2000']],\n",
       " [[['Efficient Wire Formats for High Performance Computing.',\n",
       "    'High performance computing is being increasingly utilized in non-traditional circumstances where it must interoperate with other applications. For example, online visualization is being used to monitor the progress of applications, and real-world sensors are used as inputs to simulations. Whenever these situations arise, there is a question of what communications infrastructure should be used to link the different components. Traditional HPC-style communications systems such as MPI offer relatively high performance, but are poorly suited for developing these less tightly-coupled cooperating applications. Object-based systems and meta-data formats like XML offer substantial plug-and-play flexibility, but with substantially lower performance. We observe that the flexibility and baseline performance of all these systems is strongly determined by their \"wire format,\" or how they represent data for transmission in a heterogeneous environment. We examine the performance implications of different wire formats and present an alternative with significant advantages in terms of both performance and flexibility.'],\n",
       "   '2000']],\n",
       " [[['High Performance Visualization of Time-Varying Volume Data over a Wide-Area Network Status.',\n",
       "    'This paper presents an end-to-end, low-cost solution for visualizing time-varying volume data rendered on a parallel computer located at a remote site. Pipelining and careful grouping of processors are used to hide I/O time and to maximize processor utilization. Compression is used to significantly cut down the cost of transferring output images from the parallel computer to a display device through a wide-area network. This complete rendering pipeline makes possible highly efficient rendering and remote viewing of high-resolution time-varying data sets in the absence of high-speed network and parallel I/O support. To study the performance of this rendering pipeline and to demonstrate high-performance remote visualization, tests were conducted on a PC cluster in Japan as well as an SGI Origin 2000 operated at the NASA Ames Research Center with the display located at UC Davis.'],\n",
       "   '2000']],\n",
       " [[['The AppLeS Parameter Sweep Template: User-Level Middleware for the Grid.',\n",
       "    'The Computational Grid is a promising platform for the efficient execution of parameter sweep applications over large parameter spaces. To achieve performance on the Grid, such applications must be scheduled so that shared data files are strategically placed to maximize re-use, and so that the application execution can adapt to the deliverable performance potential of target heterogeneous, distributed and shared resources. Parameter sweep applications are an important class of applications and would greatly benefit from the development of Grid middleware that embeds a scheduler for performance and targets Grid resources transparently. In this paper we describe a user-level Grid middleware project, the AppLeS Parameter Sweep Template (APST), that uses application-level scheduling techniques and various Grid technologies to allow the efficient deployment of parameter sweep applications over the Grid. We discuss several possible scheduling algorithms and detail our software design. We then describe our current implementation of APST using systems like Globus, NetSolve and Network Weather Service, and present experimental results.'],\n",
       "   '2000']],\n",
       " [[['A Comparative Study of the NAS MG Benchmark across Parallel Languages and Architectures.',\n",
       "    \"Hierarchical algorithms such as multigrid applications form an important cornerstone for scientific computing. In this study, we take a first step toward evaluating parallel language support for hierarchical applications by comparing implementations of the NAS MG benchmark in several parallel programming languages: Co-Array Fortran, High Performance Fortran, Single Assignment C, and ZPL. We evaluate each language in terms of its portability, its performance, and its ability to express the algorithm clearly and concisely. Experimental platforms include the Cray T3E, IBM SP, SGI Origin, Sun Enterprise 5500 and a high-performance Linux cluster. Our findings indicate that while it is possible to achieve good portability, performance, and expressiveness, most languages currently fall short in at least one of these areas. We find a strong correlation between expressiveness and a language's support for a global view of computation, and we identify key factors for achieving portable performance in multigrid applications.\"],\n",
       "   '2000']],\n",
       " [[['Improving Fine-Grained Irregular Shared-Memory Benchmarks by Data Reordering.',\n",
       "    'We demonstrate that data reordering can substantially improve the performance of fine-grained irregular shared-memory benchmarks, on both hardware and software shared-memory systems. In particular, we evaluate two distinct data reordering techniques that seek to co-locate in memory objects in close proximity in the physical system modeled by the computation. The effects of these techniques are increased spatial locality and reduced false sharing. We evaluate the effectiveness of the data reordering techniques on a set of five irregular applications from SPLASH-2 and Chaos. We implement both techniques in a small library, allowing us to enable them in an application by adding less than 10 lines of code. Our results on one hardware and two software shared-memory systems show that, with data reordering during initialization, the performance of these applications is improved by 12 percent to 99 percent on the Origin 2000, 30 percent to 366 percent on TreadMarks, and 14 percent to 269 percent on HLRC.'],\n",
       "   '2000']],\n",
       " [[['Is Data Distribution Necessary in OpenMP?',\n",
       "    'This paper investigates the performance implications of data placement in OpenMP programs running on modern ccNUMA multiprocessors. Data locality and minimization of the rate of remote memory accesses are critical for sustaining high performance on these systems. We show that due to the low remote-to-local memory access latency ratio of state-of-the-art ccNUMA architectures, reasonably balanced page placement schemes-such as round-robin or random distribution of pages-incur modest performance losses. We also show that performance leaks stemming from suboptimal page placement schemes can be remedied with a smart user-level page migration engine. The main body of the paper describes how the OpenMP runtime environment can use page migration for implementing implicit data distribution and redistribution schemes without programmer intervention. Our experimental results support the effectiveness of these mechanisms and provide a proof of concept that there is no need to introduce data distribution directives in OpenMP and warrant the portability of the programming model.'],\n",
       "   '2000']],\n",
       " [[['Integrating Parallel File I/O and Database Support for High-Performance Scientific Data Management.',\n",
       "    'Many scientific applications have large I/O requirements, in terms of both the size of data and the number of files or data sets. Management, storage, efficient access, and analysis of these data present an extremely challenging task. Traditionally, two different solutions are used for this problem: file I/O or databases. File I/O can provide high performance but is tedious to use with large numbers of files and large and complex data sets. Databases can be convenient, flexible, and powerful but do not perform and scale well for parallel supercomputing applications. We have developed a software system, called Scientific Data Manager (SDM), which aims to combine the good features of both file I/O and databases. SDM provides a high-level API to the user and, internally, uses a parallel file system to store real data and a database to store appreciation-related metadata. SDM takes advantage of various I/O optimizations available in MPI-IO, such as collective I/O and noncontiguous requests, in a manner that is transparent to the user. As a result, users can write and retrieve data with the performance of parallel file I/O, without having to bother with the details of actually performing file I/O. In this paper, we describe the design and implementation of SDM. With the help of two parallel application templates, ASTRO3D and an Euler solver, we illustrate how some of the design criteria affect performance.'],\n",
       "   '2000']],\n",
       " [[['Tiling Optimizations for 3D Scientific Computations.',\n",
       "    'Compiler transformations can significantly improve data locality for many scientific programs. In this paper, we show that iterative solvers for partial differential equations (PDEs) in three dimensions require new compiler optimizations not needed for 2D codes, since reuse along the third dimension cannot fit in cache for larger problem sizes. Tiling is a program transformation compilers can apply to capture this reuse, but successful application of tiling requires selection of non-conflicting tiles and/or padding array dimensions to eliminate conflicts. We present new algorithms and cost models for selecting tiling shapes and array pads. We explain why tiling is rarely needed for 2D PDE solvers, but can be helpful for 3D stencil codes. Experimental results show tiling 3D codes can reduce miss rates and achieve performance improvements of 17-121 percent for key scientific kernels, including a 27 percent average improvement for the key computational loop nest in the SPEC/NAS benchmark mgrid.'],\n",
       "   '2000']],\n",
       " [[['A Comparison of Three Programming Models for Adaptive Applications on the Origin2000.',\n",
       "    'Adaptive applications have computational workloads and communication patterns which change unpredictably at runtime, requiring load balancing to achieve scalable performance on parallel machines. Efficient parallel implementation of such adaptive application is therefore a challenging task. In this paper, we compare the performance of and the programming effort required for two major classes of adaptive applications under three leading parallel programming models on an SGI Origin 2000 system, a machine which supports all three models efficiently. Results indicate that the three models deliver comparable performance. However, the implementations differ significantly beyond merely using explicit messages versus implicit loads/stores even though the basic parallel algorithms are similar. Compared with the message-passing (using MPI) and SHMEM programming models, the cache-coherent shared address space (CC-SAS) model provides substantial ease of programming at both the conceptual level and program orchestration levels, often accompanied by performance gains. However, CC-SAS currently has portability limitations and may suffer from poor spatial locality of physically distributed shared data on large numbers of processors.'],\n",
       "   '2000']],\n",
       " [[['Landing CG on EARTH: A Case Study of Fine-Grained Multithreading on an Evolutionary Path.',\n",
       "    'We report on our work in developing a fine-grained multithreaded solution for the communication-intensive Conjugate Gradient (CG) problem. In our recent work, we have developed a simple, yet very efficient, solution to executing matrix-vector multiply on a multithreaded system. This paper presents an effective mechanism for the reduction-broadcast phase, which is implemented and integrated with the sparse MVM resulting in a scalable implementation of the complete CG application. Three major observations from our experiments on the EARTH multithreaded testbed are: (1) The scalability of our CG implementation is impressive, e.g., speedup is 90 on 120 processors for the NAS CG class B input. (2) Our dataflow-style reduction-broadcast network based on fine-grain multithreading is twice as fast as a serial reduction scheme on the same system. (3)By slowing down the netwok by a factor of 2, no notable degradation of overall CG performance was observed.'],\n",
       "   '2000']],\n",
       " [[['Dynamic Software Testing of MPI Applications with Umpire.',\n",
       "    \"As evidenced by the popularity of MPI (Message Passing Interface), message passing is an effective programming technique for managing coarse-grained concurrency on distributed computers. Unfortunately, debugging message-passing applications can be difficult. Software complexity, data races, and scheduling dependencies can make programming errors challenging to locate with manual, interactive debugging techniques. This article describes Umpire, a new tool for detecting programming errors at runtime in message passing applications. Umpire monitors the MPI operations of an application by interposing itself between the application and the MPI runtime system using the MPI profiling layer. Umpire then checks the application's MPI behavior for specific errors. Our initial collection of programming errors includes deadlock detection, mismatched collective operations, and resource exhaustion. We present an evaluation on a variety of applications that demonstrates the effectiveness of this approach.\"],\n",
       "   '2000']],\n",
       " [[['Realizing Fault Resilience in Web-Server Cluster.',\n",
       "    'Today, a successful Internet service is absolutely critical to be up 100 percent of the time. Server clustering is the most promising approach to meet this requirement. However, the existing Web server-clustering solutions merely can provide high availability derived from their redundancy nature, but offer no guarantee about fault resilience for the service. In this paper, we address this problem by implementing an innovative mechanism which enables a Web request to be smoothly migrated and recovered on another working node in the presence of server failure. We will show that request migration and recovery could be efficiently achieved in the manner of user transparency. The achieved capability of fault resilience is important and essential for a variety of critical services (e.g., E-commerce), which are increasingly widespread used. Our approach takes an important step toward providing a highly reliable Web service.'],\n",
       "   '2000']],\n",
       " [[['On the content of materialized aggregate views.',\n",
       "    'We consider the problem of answering queries using only materialized views. We first show that if the views subsume the query from the point of view of the information content, then the query can be answered using only the views, but the resulting query might be extremely inefficient. We then focus on aggregate views and queries over a single relation, which are fundamental in many applications such as data warehousing. We show that in this case, it is possible to guarantee that as soon as the views subsume the query, it can be completely rewritten in terms of the views in a simple query language. Our main contribution is the conception of various rewriting algorithms which run in polynomial time, and the proof of their completeness which relies on combinatorial arguments. Finally, we discuss the choice of materializing or not ratio views such as average and percentage, important for the design of materialized views. We show that it has an impact on the information content, which can be used to protect data, as well as on the maintenance of views.'],\n",
       "   '2000']],\n",
       " [[['Critical path analysis of TCP transactions.',\n",
       "    'Improving the performance of data transfers in the Internet (such as Web transfers) requires a detailed understanding of when and how delays are introduced. Unfortunately, the complexity of data transfers like those using HTTP is great enough that identifying the precise causes of delays is difficult. In this paper we describe a method for pinpointing where delays are introduced into applications like HTTP by using critical path analysis. By constructing and profiling the critical path, it is possible to determine what fraction of total transfer latency is due to packet propagation, network variation (e.g., queuing at touters or route fluctuation), packet losses, and delays at the server and at the client. We have implemented our technique in a tool called tcpeval that automates critical path analysis for Web transactions. We show that our analysis method is robust enough to analyze traces taken for two different TCP implementations (Linux and FreeBSD). To demonstrate the utility of our approach, we present the results of critical path analysis for a set of Web transactions taken over 14 days under a variety of server and network conditions. The results show that critical path analysis can shed considerable light on the causes of delays in Web transfers, and can expose subtleties in the behavior of the entire end-to-end system.'],\n",
       "   '2000']],\n",
       " [[['The content and access dynamics of a busy web site: findings and implicatins.',\n",
       "    \"In this paper, we study the dynamics of the MSNBC news site, one of the busiest Web sites in the Internet today. Unlike many other efforts that have analyzed client accesses as seen by proxies, we focus on the server end. We analyze the dynamics of both the server content and client accesses made to the server. The former considers the content creation and modification process while the latter considers page popularity and locality in client accesses. Some of our key results are: (a) files tend to change little when they are modified, (b) a small set of files tends to get modified repeatedly, (c) file popularity follows a Zipf-like distribution with a parameter &agr that is much larger than reported in previous, proxy-based studies, and (d) there is significant temporal stability in file popularity but not much stability in the domains from which clients access the popular content. We discuss the implications of these findings for techniques such as Web caching (including cache consistency algorithms), and prefetching or server-based ``push'' of Web content.\"],\n",
       "   '2000']],\n",
       " [[['FIRE: Flexible intra-AS routing environment.',\n",
       "    'Current routing protocols are monolithic, specifying the algorithm used to construct forwarding tables, the metric used by the algorithm (generally some form of hop-count), and the protocol used to distribute these metrics as an integrated package. The Flexible Intra-AS Routing Environment (FIRE) is a link-state, intra-domain routing protocol that decouples these components. FIRE supports run-time-pro- grammable algorithms and metrics over a secure link-state distribution protocol. By allowing the network operator to dynamically reprogram both the information being advertised and the routing algorithm used to construct forwarding tables in Java, FIRE enables the development and deployment of novel routing algorithms without the need for a new protocol to distribute state. FIRE supports multiple concurrent routing algorithms and metrics, each constructing separate forwarding tables. By using operator-specified packet filters, separate classes of traffic are routed using completely different routing algorithms, all supported by a single routing protocol.'],\n",
       "   '2000']],\n",
       " [[['Packet Types: Abstract specifications of network protocol messages.',\n",
       "    'In writing networking code, one is often faced with the task of interpreting a raw buffer according to a standardized packet format. This is needed, for example, when monitoring network traffic for specific kinds of packets, or when unmarshaling an incoming packet for protocol processing. In such cases, a programmer typically writes C code that understands the grammar of a packet and that also performs any necessary byte-order and alignment adjustments. Because of the complexity of certain protocol formats, and because of the low-level of programming involved, writing such code is usually a cumbersome and error-prone process. Furthermore, code written in this style loses the domain-specific information, viz. the packet format, in its details, making it difficult to maintain.'],\n",
       "   '2000']],\n",
       " [[['Memory-efficient state lookups with fast updates.',\n",
       "    'Routers must do a best matching prefix lookup for every packet; solutions for Gigabit speeds are well known. As Internet link speeds higher, we seek a scalable solution whose speed scales with memory speeds while allowing large prefix databases. In this paper we show that providing such a solution requires careful attention to memory allocation and pipelining. This is because fast lookups require on-chip or off-chip SRAM which is limited by either expense or manufacturing process. We show that doing so while providing guarantees on the number of prefixes supported requires new algorithms and the breaking down of traditional abstraction boundaries between hardware and software. We introduce new problem-specific memory allocators that have provable memory utilization guarantees that can reach 100%; this is contrast to all standard allocators that can only guarantee 20% utilization when the requests can come in the range [1 ... 32]. An optimal version of our algorithm requires a new (but feasible) SRAM memory design that allows shifted access in addition to normal word access. Our techniques generalize to other IP lookup schemes and to other state lookups besides prefix lookup.'],\n",
       "   '2000']],\n",
       " [[['Examining workgroup influence on technology usage: a community of practice perspective.',\n",
       "    \"This paper develops a new theoretical model with which to examine the factors that determine employees' technology usage within organizational settings. Much of the prior IS training literature has assumed an underlying relationship between necessary conditions for technology adoption &mdash; such as user training and support resources &mdash; and actual technology use. Whereas these &ldquo;facilitating conditions&rdquo; for technology usage [38, 80, 81] have been taken for granted as factors that influence system adoption and usage, the reality of learning and working in organizational settings suggests an entirely different mode of influence on employees' technology-related behavior. Drawing from research in cognitive anthropology [35], and acknowledging the role of communities of practice that shape learning, behavior, and memory [5, 65, 87, 88], we develop an alternative theoretical framework to explain employees' adoption of technology and their degree of system usage. Contrasting this novel theoretical framework with traditional notions that presume technology usage to be directly related to the amount or perceived quality of user training and support, we evaluate both frameworks with empirical data obtained from a technology implementation initiative across five sites of one nonprofit organization. While both theories receive some support from our data, we argue for the recognition that social influences exert an effect not just on whether employees adopt IT [77], but more importantly on how and how much employees use technology for their jobs. Implications for research and practice related to IT training are provided, as well as to more general lessons for managing technology implementation.\"],\n",
       "   '2000']],\n",
       " [[['PILOT: an interactive tool for learning and grading.',\n",
       "    'We describe a Web-based interactive system, called PILOT, for testing computer science concepts. The strengths of PILOT are its universal access and platform independence, its use as an algorithm visualization tool, its ability to test algorithmic concepts, its support for graph generation and layout, its automated grading mechanism, and its ability to award partial credit to proposed solutions.'],\n",
       "   '2000']],\n",
       " [[['Yet, more Web exercises for learning C++.',\n",
       "    \"This paper describes a set of author developed interactive web exercises and a development environment designed to facilitate language acquisition in a beginning course in C++. The exercises test the students' understanding of several C++ language constructs as well as general programming concepts such as scope of variables. The environment allows students to write and test sections of code in a instructor controlled setting. Together the exercises and environment can be used to enhance computer science education for both traditional and distance learning students. The paradigm of generalization and automation of standard exercises can be extended to facilitate web education in other courses.\"],\n",
       "   '2000']],\n",
       " [[['Competitive analysis of incentive compatible on-line auctions.',\n",
       "    \"This paper studies auctions in a setting where the different bidders arrive at different times and the auction mechanism is required to make decisions about each bid as it is received. Such settings occur in computerized auctions of computational resources as well as in other settings. We call such auctions, on-line auctions.We first characterize exactly on-line auctions that are incentive compatible, i.e. where rational bidders are always motivated to bid their true valuation. We then embark on a competitive worst-case analysis of incentive compatible on-line auctions. We obtain several results, the cleanest of which is an incentive compatible on-line auction for a large number of identical items. This auction has an optimal competitive ratio, both in terms of seller's revenue and in terms of the total social efficiency obtained.\"],\n",
       "   '2000']],\n",
       " [[['Computationally feasible VCG mechanisms.',\n",
       "    \"A major achievement of mechanism design theory is a general method for the construction of truthful mechanisms called VCG (Vickrey, Clarke, Groves). When applying this method to complex problems such as combinatorial auctions, a difficulty arises: VCG mechanisms are required to compute optimal outcomes and are, therefore, computationally infeasible. However, if the optimal outcome is replaced by the results of a sub-optimal algorithm, the resulting mechanism (termed VCG-based) is no longer necessarily truthful. The first part of this paper studies this phenomenon in depth and shows that it is near universal. Specifically, we prove that essentially all reasonable approximations or heuristics for combinatorial auctions as well as a wide class of cost minimization problems yield non-truthful VCG-based mechanisms. We generalize these results for affine maximizers. The second part of this paper proposes a general method for circumventing the above problem. We introduce a modification of VCG-based mechanisms in which the agents are given a chance to improve the output of the underlying algorithm. When the agents behave truthfully, the welfare obtained by the mechanism is at least as good as the one obtained by the algorithm's output. We provide a strong rationale for truth-telling behavior. Our method satisfies individual rationality as well.\"],\n",
       "   '2000']],\n",
       " [[['Efficient image-based methods for rendering soft shadows.',\n",
       "    'We present two efficient imaged-based approaches for computation and display of high-quality soft shadows from area light sources. Our methods are related to shadow maps and provide the associated benefits. The computation time and memory requirements for adding soft shadows to an image depend on image size and the number of lights, not geometric scene complexity. We also show that because area light sources are localized in space, soft shadow computations are particularly well suited to imaged-based rendering techniques. Our first approach&mdash;layered attenuation maps&mdash;achieves interactive rendering rates, but limits sampling flexibility, while our second method&mdash;coherence-based raytracing of depth images&mdash;is not interactive, but removes the limitations on sampling and yields high quality images at a fraction of the cost of conventional raytracers. Combining the two algorithms allows for rapid previewing followed by efficient high-quality rendering.'],\n",
       "   '2000']],\n",
       " [[['As-rigid-as-possible shape interpolation.',\n",
       "    'We present an object-space morphing technique that blends the interiors of given two- or three-dimensional shapes rather than their boundaries. The morph is rigid in the sense that local volumes are least-distorting as they vary from their source to target configurations. Given a boundary vertex correspondence, the source and target shapes are decomposed into isomorphic simplicial complexes. For the simplicial complexes, we find a closed-form expression allocating the paths of both boundary and interior vertices from source to target locations as a function of time. Key points are the identification of the optimal simplex morphing and the appropriate definition of an error functional whose minimization defines the paths of the vertices. Each pair of corresponding simplices defines an affine transformation, which is factored into a rotation and a stretching transformation. These local transformations are naturally interpolated over time and serve as the basis for composing a global coherent least-distorting transformation.'],\n",
       "   '2000']],\n",
       " [[['Tangible interaction + graphical interpretation: a new approach to 3D modeling.',\n",
       "    'Construction toys are a superb medium for geometric models. We argue that such toys, suitably instrumented or sensed, could be the inspiration for a new generation of easy-to-use, tangible modeling systems&mdash;especially if the tangible modeling is combined with graphical-interpretation techniques for enhancing nascent models automatically. The three key technologies needed to realize this idea are embedded computation, vision-based acquisition, and graphical interpretation. We sample these technologies in the context of two novel modeling systems: physical building blocks that self-describe, interpret, and decorate the structures into which they are assembled; and a system for scanning, interpreting, and animating clay figures.'],\n",
       "   '2000']],\n",
       " [[['A microfacet-based BRDF generator.',\n",
       "    'A method is presented that takes as an input a 2D microfacet orientation distribution and produces a 4D bidirectional reflectance distribution function (BRDF). This method differs from previous microfacet-based BRDF models in that it uses a simple shadowing term which allows it to handle very general microfacet distributions while maintaining reciprocity and energy conservation. The generator is shown on a variety of material types.'],\n",
       "   '2000']],\n",
       " [[['Style machines.',\n",
       "    'We approach the problem of stylistic motion synthesis by learning motion patterns from a highly varied set of motion capture sequences. Each sequence may have a distinct choreography, performed in a distinct sytle. Learning identifies common choreographic elements across sequences, the different styles in which each element is performed, and a small number of stylistic degrees of freedom which span the many variations in the dataset. The learned model can synthesize novel motion data in any interpolation or extrapolation of styles. For example, it can convert novice ballet motions into the more graceful modern dance of an expert. The model can also be driven by video, by scripts or even by noise to generate new choreography and synthesize virtual motion-capture in many styles.'],\n",
       "   '2000']],\n",
       " [[['Sampling plausible solutions to multi-body constraint problems.',\n",
       "    \"Traditional collision intensive multi-body simulations are difficult to control due to extreme sensitivity to initial conditions or model parameters. Furthermore, there may be multiple ways to achieve any one goal, and it may be difficult to codify a user's preferences before they have seen the available solutions. In this paper we extend simulation models to include plausible sources of uncertainty, and then use a Markov chain Monte Carlo algorithm to sample multiple animations that satisfy constraints. A user can choose the animation they prefer, or applications can take direct advantage of the multiple solutions. Our technique is applicable when a probability can be attached to each animation, with &ldquo;good&rdquo; animations having high probability, and for such cases we provide a definition of physical plausibility for animations. We demonstrate our approach with examples of multi-body rigid-body simulations that satisfy constraints of various kinds, for each case presenting animations that are true to a physical model, are significantly different from each other, and yet still satisfy the constraints.\"],\n",
       "   '2000']],\n",
       " [[['The EMOTE model for effort and shape.',\n",
       "    'Human movements include limb gestures and postural attitude. Although many computer animation researchers have studied these classes of movements, procedurally generated movements still lack naturalness. We argue that looking only at the psychological notion of gesture is insufficient to capture movement qualities needed by animated charactes. We advocate that the domain of movement observation science, specifically Laban Movement Analysis (LMA) and its Effort and Shape components, provides us with valuable parameters for the form and execution of qualitative aspects of movements. Inspired by some tenets shared among LMA proponents, we also point out that Effort and Shape phrasing across movements and the engagement of the whole body are essential aspects to be considered in the search for naturalness in procedurally generated gestures. Finally, we present EMOTE (Expressive MOTion Engine), a 3D character animation system that applies Effort and Shape qualities to independently defined underlying movements and thereby generates more natural synthetic gestures.'],\n",
       "   '2000']],\n",
       " [[['Acquiring the reflectance field of a human face.',\n",
       "    \"We present a method to acquire the reflectance field of a human face and use these measurements to render the face under arbitrary changes in lighting and viewpoint. We first acquire images of the face from a small set of viewpoints under a dense sampling of incident illumination directions using a light stage. We then construct a reflectance function image for each observed image pixel from its values over the space of illumination directions. From the reflectance functions, we can directly generate images of the face from the original viewpoints in any form of sampled or computed illumination. To change the viewpoint, we use a model of skin reflectance to estimate the appearance of the reflectance functions for novel viewpoints. We demonstrate the technique with synthetic renderings of a person's face under novel illumination and viewpoints.\"],\n",
       "   '2000']],\n",
       " [[['Computer-generated pen-and-ink illustration of trees.',\n",
       "    'We present a method for automatically rendering pen-and-ink illustrations of trees. A given 3-d tree model is illustrated by the tree skeleton and a visual representation of the foliage using abstract drawing primitives. Depth discontinuities are used to determine what parts of the primitives are to be drawn; a hybrid pixel-based and analytical algorithm allows us to deal efficiently with the complex geometric data. Using the proposed method we are able to generate illustrations with different drawing styles and levels of abstraction. The illustrations generated are spatial coherent, enabling us to create animations of sketched environments. Applications of our results are found in architecture, animation and landscaping.'],\n",
       "   '2000']],\n",
       " [[['A simple, efficient method for realistic animation of clouds.',\n",
       "    'This paper proposes a simple and computationally inexpensive method for animation of clouds. The cloud evolution is simulated using cellular automaton that simplifies the dynamics of cloud formation. The dynamics are expressed by several simple transition rules and their complex motion can be simulated with a small amount of computation. Realistic images are then created using one of the standard graphics APIs, OpenGL. This makes it possible to utilize graphics hardware, resulting in fast image generation. The proposed method can realize the realistic motion of clouds, shadows cast on the ground, and shafts of light through clouds.'],\n",
       "   '2000']],\n",
       " [[['Conservative visibility preprocessing using extended projections.',\n",
       "    'Visualization of very complex scenes can be significantly accelerated using occlusion culling. In this paper we present a visibility preprocessing method which efficiently computes potentially visible geometry for volumetric viewing cells. We introduce novel extended projection operators, which permits efficient and conservative occlusion culling with respect to all viewpoints within a cell, and takes into account the combined occlusion effect of multiple occluders. We use extended projection of occluders onto a set of projection planes to create extended occlusion maps; we show how to efficiently test occludees against these occlusion maps to determine occlusion with respect to the entire cell. We also present an improved projection operator for certain specific but important configurations. An important advantage of our approach is that we can re-project extended projections onto a series of projection planes (via an occlusion sweep), and accumulate occlusion information from multiple blockers. This new approach allows the creation of effective occlusion maps for previously hard-to-treat scenes such as leaves of trees in a forest. Graphics hardware is used to accelerate both the extended projection and reprojection operations. We present a complete implementation demonstrating significant speedup with respect to view-frustum culling only, without the computational overhead of on-line occlusion culling.'],\n",
       "   '2000']],\n",
       " [[['Adaptively sampled distance fields: a general representation of shape for computer graphics.',\n",
       "    'Adaptively Sampled Distance Fields (ADFs) are a unifying representation of shape that integrate numerous concepts in computer graphics including the representation of geometry and volume data and a broad range of processing operations such as rendering, sculpting, level-of-detail management, surface offsetting, collision detection, and color gamut correction. Its structure is uncomplicated and direct, but is especially effective for quality reconstruction of complex shapes, e.g., artistic and organic forms, precision parts, volumes, high order functions, and fractals. We characterize one implementation of ADFs, illustrating its utility on two diverse applications: 1) artistic carving of fine detail, and 2) representing and rendering volume data and volumetric effects. Other applications are briefly presented.'],\n",
       "   '2000']],\n",
       " [[['A fast relighting engine for interactive cinematic lighting design.',\n",
       "    \"We present new techniques for interactive cinematic lighting design of complex scenes that use procedural shaders. Deep-framebuffers are used to store the geometric and optical information of the visible surfaces of an image. The geometric information is represented as collections of oriented points, and the optical information is represented as bi-directional reflection distribution functions, or BRDFs. The BRDFs are generated by procedurally defined surface texturing functions that spatially vary the surfaces' appearances. The deep-framebuffer information is rendered using a multi-pass algorithm built on the OpenGL graphics pipeline. In order to handle both physically-correct as well as non-realistic reflection models used in the film industry, we factor the BRDF into independent components that map onto both the lighting and texturing units of the graphics hardware. A similar factorization is used to control the lighting distribution. Using these techniques, lighting calculations can be evaluated 2500 times faster than previous methods. This allows lighting changes to be rendered at rates of 20Hz in static environments that contain millions of objects of with dozens of unique procedurally defined surface properties and scores of lights.\"],\n",
       "   '2000']],\n",
       " [[['Normal meshes.',\n",
       "    'Normal meshes are new fundamental surface descriptions inspired by differential geometry. A normal mesh is a multiresolution mesh where each level can be written as a normal offset from a coarser version. Hence the mesh can be stored with a single float per vertex. We present an algorithm to approximate any surface arbitrarily closely with a normal semi-regular mesh. Normal meshes can be useful in numerous applications such as compression, filtering, rendering, texturing, and modeling.'],\n",
       "   '2000']],\n",
       " [[['Illuminating micro geometry based on precomputed visibility.',\n",
       "    'Many researchers have been arguing that geometry, bump maps, and BRDFs present a hierarchy of detail that should be exploited for efficient rendering purposes. In practice however, this is often not possible due to inconsistencies in the illumination for these different levels of detail. For example, while bump map rendering often only considers direct illumination and no shadows, geometry-based rendering and BRDFs will mostly also respect shadowing effects, and in many cases even indirect illumination caused by scattered light. In this paper, we present an approach for overcoming these inconsistencies. We introduce an inexpensive method for consistently illuminating height fields and bump maps, as well as simulating BRDFs based on precomputed visibility information. With this information we can achieve a consistent illumination across the levels of detail. The method we propose offers significant performance benefits over existing algorithms for computing the light scattering in height fields and for computing a sampled BRDF representation using a virtual gonioreflectometer. The performance can be further improved by utilizing graphics hardware, which then also allows for interactive display. Finally, our method also approximates the changes in illumination when the height field, bump map, or BRDF is applied to a surface with a different curvature.'],\n",
       "   '2000']],\n",
       " [[['Illustrating smooth surfaces.',\n",
       "    'We present a new set of algorithms for line-art rendering of smooth surfaces. We introduce an efficient, deterministic algorithm for finding silhouettes based on geometric duality, and an algorithm for segmenting the silhouette curves into smooth parts with constant visibility. These methods can be used to find all silhouettes in real time in software. We present an automatic method for generating hatch marks in order to convey surface shape. We demonstrate these algorithms with a drawing style inspired by A Topological Picturebook by G. Francis.'],\n",
       "   '2000']],\n",
       " [[['Dynamically reparameterized light fields.',\n",
       "    \"This research further develops the light field and lumigraph image-based rendering methods and extends their utility. We present alternate parameterizations that permit 1) interactive rendering of moderately sampled light fields of scenes with significant, unknown depth variation and 2) low-cost, passive autostereoscopic viewing. Using a dynamic reparameterization, these techniques can be used to interactively render photographic effects such as variable focus and depth-of-field within a light field. The dynamic parameterization is independent of scene geometry and does not require actual or approximate geometry of the scene. We explore the frequency domain and ray-space aspects of dynamic reparameterization, and present an interactive rendering technique that takes advantage of today's commodity rendering hardware.\"],\n",
       "   '2000']],\n",
       " [[['Spectral compression of mesh geometry.',\n",
       "    'We show how spectral methods may be applied to 3D mesh data to obtain compact representations. This is achieved by projecting the mesh geometry onto an orthonormal basis derived from the mesh topology. To reduce complexity, the mesh is partitioned into a number of balanced submeshes with minimal interaction, each of which are compressed independently. Our methods may be used for compression and progressive transmission of 3D content, and are shown to be vastly superior to existing methods using spatial techniques, if slight loss can be tolerated.'],\n",
       "   '2000']],\n",
       " [[['Progressive geometry compression.',\n",
       "    'We propose a new progressive compression scheme for arbitrary topology, highly detailed and densely sampled meshes arising from geometry scanning. We observe that meshes consist of three distinct components: geometry, parameter, and connectivity information. The latter two do not contribute to the reduction of error in a compression setting. Using semi-regular meshes, parameter and connectivity information can be virtually eliminated. Coupled with semi-regular wavelet transforms, zerotree coding, and subdivision based reconstruction we see improvements in error by a factor four (12dB) compared to other progressive coding schemes.'],\n",
       "   '2000']],\n",
       " [[['Non-photorealistic virtual environments.',\n",
       "    'We describe a system for non-photorealistic rendering (NPR) of virtual environments. In real time, it synthesizes imagery of architectural interiors using stroke-based textures. We address the four main challenges of such a system &mdash; interactivity, visual detail, controlled stroke size, and frame-to-frame coherence &mdash; through image based rendering (IBR) methods. In a preprocessing stage, we capture photos of a real or synthetic environment, map the photos to a coarse model of the environment, and run a series of NPR filters to generate textures. At runtime, the system re-renders the NPR textures over the geometry of the coarse model, and it adds dark lines that emphasize creases and silhouettes. We provide a method for constructing non-photorealistic textures from photographs that largely avoids seams in the resulting imagery. We also offer a new construction, art-maps, to control stroke size across the images. Finally, we show a working system that provides an immersive experience rendered in a variety of NPR styles.'],\n",
       "   '2000']],\n",
       " [[['3-subdivision.',\n",
       "    'A new stationary subdivision scheme is presented which performs slower topological refinement than the usual dyadic split operation. The number of triangles increases in every step by a factor of 3 instead of 4. Applying the subdivision operator twice causes a uniform refinement with tri-section of every original edge (hence the name &radic;3-subdivision) while two dyadic splits would quad-sect every original edge. Besides the finer gradation of the hierarchy levels, the new scheme has several important properties: The stencils for the subdivision rules have minimum size and maximum symmetry. The smoothness of the limit surface is C2 everywhere except for the extraordinary points where it is C1. The convergence analysis of the scheme is presented based on a new general technique which also applies to the analysis of other subdivision schemes. The new splitting operation enables locally adaptive refinement under built-in preservation of the mesh consistency without temporary crack-fixing between neighboring faces from different refinement levels. The size of the surrounding mesh area which is affected by selective refinement is smaller than for the dyadic split operation. We further present a simple extension of the new subdivision scheme which makes it applicable to meshes with boundary and allows us to generate sharp feature lines.'],\n",
       "   '2000']],\n",
       " [[['Interactive control for physically-based animation.',\n",
       "    \"We propose the use of interactive, user-in-the-loop techniques for controlling physically-based animated characters. With a suitably designed interface, the continuous and discrete input actions afforded by a standard mouse and keyboard allow for the creation of a broad range of motions. We apply our techniques to interactively control planar dynamic simulations of a bounding cat, a gymnastic desk lamp, and a human character capable of walking, running, climbing, and various gymnastic behaviors. The interactive control techniques allows a performer's intuition and knowledge about motion planning to be readily exploited. Video games are the current target application of this work.\"],\n",
       "   '2000']],\n",
       " [[['Displaced subdivision surfaces.',\n",
       "    'In this paper we introduce a new surface representing, the displaced subdivision surface. It represents a detailed surface model as a scalar-valued displacement over a smooth domain surface. Our representation defines both the domain surface and the displacement function using a unified subdivision framework, allowing for simple and efficient evaluation of analytic surface properties. We present a simple, automatic scheme for converting detailed geometric models into such a representation. The challenge in this conversion process is to find a simple subdivision surface that still faithfully expresses the detailed model as its offset. We demonstrate that displaced subdivision surfaces offer a number of benefits, including geometry compression, editing, animation, scalability, and adaptive rendering. In particular, the encoding of fine detail as a scalar function makes the representation extremely compact.'],\n",
       "   '2000']],\n",
       " [[['Pose space deformation: a unified approach to shape interpolation and skeleton-driven deformation.',\n",
       "    'Pose space deformation generalizes and improves upon both shape interpolation and common skeleton-driven deformation techniques. This deformation approach proceeds from the observation that several types of deformation can be uniformly represented as mappings from a pose space, defined by either an underlying skeleton or a more abstract system of parameters, to displacements in the object local coordinate frames. Once this uniform representation is identified, previously disparate deformation types can be accomplished within a single unified approach. The advantages of this algorithm include improved expressive power and direct manipulation of the desired shapes yet the performance associated with traditional shape interpolation is achievable. Appropriate applications include animation of facial and body deformation for entertainment, telepresence, computer gaming, and other applications where direct sculpting of deformations is desired or where real-time synthesis of a deforming model is required.'],\n",
       "   '2000']],\n",
       " [[['The digital Michelangelo project: 3D scanning of large statues.',\n",
       "    'We describe a hardware and software system for digitizing the shape and color of large fragile objects under non-laboratory conditions. Our system employs laser triangulation rangefinders, laser time-of-flight rangefinders, digital still cameras, and a suite of software for acquiring, aligning, merging, and viewing scanned data. As a demonstration of this system, we digitized 10 statues by Michelangelo, including the well-known figure of David, two building interiors, and all 1,163 extant fragments of the Forma Urbis Romae, a giant marble map of ancient Rome. Our largest single dataset is of the David - 2 billion polygons and 7,000 color images. In this paper, we discuss the challenges we faced in building this system, the solutions we employed, and the lessons we learned. We focus in particular on the unusual design of our laser triangulation scanner and on the algorithms and software we developed for handling very large scanned models.'],\n",
       "   '2000']],\n",
       " [[['Image-based visual hulls.',\n",
       "    'In this paper, we describe an efficient image-based approach to computing and shading visual hulls from silhouette image data. Our algorithm takes advantage of epipolar geometry and incremental computation to achieve a constant rendering cost per rendered pixel. It does not suffer from the computation complexity, limited resolution, or quantization artifacts of previous volumetric approaches. We demonstrate the use of this algorithm in a real-time virtualized reality application running off a small number of video streams.'],\n",
       "   '2000']],\n",
       " [[['Accessible animation and customizable graphics via simplicial configuration modeling.',\n",
       "    \"O ur goal is to em bed free-form constraints into a graphical m odel. W ith such constraints a graphic can m aintain its visual integrity&mdash; and break rules tastefully&mdash; while being m anipulated by a casualuser. A typicalparam eterized graphic does notm eet these needs because its configuration space contains nonsense im ages in m uch higher proportion than desirable im ages, and the casual user is apt to ruin the graphic on any attem pt to m odify oranim ate it. W e therefore m odel the sm all subset of a given graphic's configuration space that m aps to desirable im ages. In our solution, the basic building block is a sim plicial complex&mdash; the m ost practical data structure able to accom m odate the variety of topologies that can arise. The configuration-space m odel can be built from a cross productofsuch com plexes. W e describe how to define the m apping from this space to the im age space. W e show how to invert that m apping, allow ing the user to m anipulate the im age without understanding the structure of the configuration-space m odel. W e also show how to extend the m apping when the originalparam eterization contains hierarchy, coordinate transform ations,and other non linearities. O ur software im plem entation applies sim plicial configuration m odeling to 2D vector graphics.\"],\n",
       "   '2000']],\n",
       " [[['Timewarp rigid body simulation.',\n",
       "    \"The traditional high-level algorithms for rigid body simulation work well for moderate numbers of bodies but scale poorly to systems of hundreds or more moving, interacting bodies. The problem is unnecessary synchronization implicit in these methods. Jefferson's timewarp algorithm [22] is a technique for alleviating this problem in parallel discrete event simulation. Rigid body dynamics, though a continuous process, exhibits many aspects of a discrete one. With modification, the timewarp algorithm can be used in a uniprocessor rigid body simulator to give substantial performance improvements for simulations with large numbers of bodies. This paper describes the limitations of the traditional high-level simulation algorithms, introduces Jefferson's algorithm, and extends and optimizes it for the rigid body case. It addresses issues particular to rigid body simulation, such as collision detection and contact group management, and describes how to incorporate these into the timewarp framework. Quantitative experimental results indicate that the timewarp algorithm offers significant performance improvements over traditional high-level rigid body simulation algorithms, when applied to systems with hundreds of bodies. It also helps pave the way to parallel implementations, as the paper discusses.\"],\n",
       "   '2000']],\n",
       " [[['Interactive multi-pass programmable shading.',\n",
       "    'Programmable shading is a common technique for production animation, but interactive programmable shading is not yet widely available. We support interactive programmable shading on virtually any 3D graphics hardware using a scene graph library on top of OpenGL. We treat the OpenGL architecture as a general SIMD computer, and translate the high-level shading description into OpenGL rendering passes. While our system uses OpenGL, the techniques described are applicable to any retained mode interface with appropriate extension mechanisms and hardware API with provisions for recirculating data through the graphics pipeline. We present two demonstrations of the method. The first is a constrained shading language that runs on graphics hardware supporting OpenGL 1.2 with a subset of the ARB imaging extensions. We remove the shading language constraints by minimally extending OpenGL. The key extensions are color range (supporting extended range and precision data types) and pixel texture (using framebuffer values as indices into texture maps). Our second demonstration is a renderer supporting the RenderMan Interface and RenderMan Shading Language on a software implementation of this extended OpenGL. For both languages, our compiler technology can take advantage of extensions and performance characteristics unique to any particular graphics hardware.'],\n",
       "   '2000']],\n",
       " [[['Toward a psychophysically-based light reflection model for image synthesis.',\n",
       "    \"In this paper we introduce a new light reflection model for image synthesis based on experimental studies of surface gloss perception. To develop the model, we've conducted two experiments that explore the relationships between the physical parameters used to describe the reflectance properties of glossy surfaces and the perceptual dimensions of glossy appearance. In the first experiment we use multidimensional scaling techniques to reveal the dimensionality of gloss perception for simulated painted surfaces. In the second experiment we use magnitude estimation methods to place metrics on these dimensions that relate changes in apparent gloss to variations in surface reflectance properties. We use the results of these experiments to rewrite the parameters of a physically-based light reflection model in perceptual terms. The result is a new psychophysically-based light reflection model where the dimensions of the model are perceptually meaningful, and variations along the dimensions are perceptually uniform. We demonstrate that the model can facilitate describing surface gloss in graphics rendering applications. This work represents a new methodology for developing light reflection models for image synthesis.\"],\n",
       "   '2000']],\n",
       " [[['Shadows for cel animation.',\n",
       "    'We present a semi-automatic method for creating shadow mattes in cel animation. In conventional cel animation, shadows are drawn by hand, in order to provide visual cues about the spatial relationships and forms of characters in the scene. Our system creates shadow mattes based on hand-drawn characters, given high-level guidance from the user about depths of various objects. The method employs a scheme for &ldquo;inflating&rdquo; a 3D figure based on hand-drawn art. It provides simple tools for adjusting object depths, coupled with an intuitive interface by which the user specifies object shapes and relative positions in a scene. Our system obviates the tedium of drawing shadow mattes by hand, and provides control over complex shadows falling over interesting shapes.'],\n",
       "   '2000']],\n",
       " [[['Surfels: surface elements as rendering primitives.',\n",
       "    'Surface elements (surfels) are a powerful paradigm to efficiently render complex geometric objects at interactive frame rates. Unlike classical surface discretizations, i.e., triangles or quadrilateral meshes, surfels are point primitives without explicit connectivity. Surfel attributes comprise depth, texture color, normal, and others. As a pre-process, an octree-based surfel representation of a geometric object is computed. During sampling, surfel positions and normals are optionally perturbed, and different levels of texture colors are prefiltered and stored per surfel. During rendering, a hierarchical forward warping algorithm projects surfels to a z-buffer. A novel method called visibility splatting determines visible surfels and holes in the z-buffer. Visible surfels are shaded using texture filtering, Phong illumination, and environment mapping using per-surfel normals. Several methods of image reconstruction, including supersampling, offer flexible speed-quality tradeoffs. Due to the simplicity of the operations, the surfel rendering pipeline is amenable for hardware implementation. Surfel objects offer complex shape, low rendering cost and high image quality, which makes them specifically suited for low-cost, real-time graphics, such as games.'],\n",
       "   '2000']],\n",
       " [[['Monte Carlo evaluation of non-linear scattering equations for subsurface reflection.',\n",
       "    'We describe a new mathematical framework for solving a wide variety of rendering problems based on a non-linear integral scattering equation. This framework treats the scattering functions of complex aggregate objects as first-class rendering primitives; these scattering functions accurately account for all scattering events inside them. We also describe new techniques for computing scattering functions from the composition of scattering objects. We demonstrate that solution techniques based on this new approach can be more efficient than previous techniques based on radiance transport and the equation of transfer and we apply these techniques to a number of problems in rendering scattering from complex surfaces.'],\n",
       "   '2000']],\n",
       " [[['Seamless texture mapping of subdivision surfaces by model pelting and texture blending.',\n",
       "    'Subdivision surfaces solve numerous problems related to the geometry of character and animation models. However, unlike on parametrised surfaces there is no natural choice of texture coordinates on subdivision surfaces. Existing algorithms for generating texture coordinates on non-parametrised surfaces often find solutions that are locally acceptable but globally are unsuitable for use by artists wishing to paint textures. In addition, for topological reasons there is not necessarily any choice of assignment of texture coordinates to control points that can satisfactorily be interpolated over the entire surface. We introduce a technique, pelting, for finding both optimal and intuitive texture mapping over almost all of an entire subdivision surface and then show how to combine multiple texture mappings together to produce a seamless result.'],\n",
       "   '2000']],\n",
       " [[['The WarpEngine: an architecture for the post-polygonal age.',\n",
       "    'We present the WarpEngine, an architecture designed for real-time imaged-based rendering of natural scenes from arbitrary viewpoints. The modeling primitives are real-world images with per-pixel depth. Currently they are acquired and stored off-line; in the near future real-time depth-image acquisition will be possible, the WarpEngine is designed to render in immediate mode from such data sources. The depth-image resolution is locally adapted by interpolation to match the resolution of the output image. 3D warping can occur either before or after the interpolation; the resulting warped/interpolated samples are forward-mapped into a warp buffer, with the precise locations recorded using an offset. Warping processors are integrated on-chip with the warp buffer, allowing efficient, scalable implementation of very high performance systems. Each chip will be able to process 100 million samples per second and provide 4.8GigaBytes per second of bandwidth to the warp buffer. The WarpEngine is significantly less complex than our previous efforts, incorporating only a single ASIC design. Small configurations can be packaged as a PC add-in card, while larger deskside configurations will provide HDTV resolutions at 50 Hz, enabling radical new applications such as 3D television. WarpEngine will be highly programmable, facilitating use as a test-bed for experimental IBR algorithms.'],\n",
       "   '2000']],\n",
       " [[['Interactive manipulation of rigid body simulations.',\n",
       "    \"Physical simulation of dynamic objects has become commonplace in computer graphics because it produces highly realistic animations. In this paradigm the animator provides few physical parameters such as the objects' initial positions and velocities, and the simulator automatically generates realistic motions. The resulting motion, however, is difficult to control because even a small adjustment of the input parameters can drastically affect the subsequent motion. Furthermore, the animator often wishes to change the end-result of the motion instead of the initial physical parameters. We describe a novel interactive technique for intuitive manipulation of rigid multi-body simulations. Using our system, the animator can select bodies at any time and simply drag them to desired locations. In response, the system computes the required physical parameters and simulates the resulting motion. Surface characteristics such as normals and elasticity coefficients can also be automatically adjusted to provide a greater range of feasible motions, if the animator so desires. Because the entire simulation editing process runs at interactive speeds, the animator can rapidly design complex physical animations that would be difficult to achieve with existing rigid body simulators.\"],\n",
       "   '2000']],\n",
       " [[['QSplat: a multiresolution point rendering system for large meshes.',\n",
       "    'Advances in 3D scanning technologies have enabled the practical creation of meshes with hundreds of millions of polygons. Traditional algorithms for display, simplification, and progressive transmission of meshes are impractical for data sets of this size. We describe a system for representing and progressively displaying these meshes that combines a multiresolution hierarchy based on bounding spheres with a rendering system based on points. A single data structure is used for view frustum culling, backface culling, level-of-detail selection, and rendering. The representation is compact and can be computed quickly, making it suitable for large data sets. Our implementation, written for use in a large-scale 3D digitization project, launches quickly, maintains a user-settable interactive frame rate regardless of object complexity or camera position, yields reasonable image quality during motion, and refines progressively when idle to a high final image quality. We have demonstrated the system on scanned models containing hundreds of millions of samples.'],\n",
       "   '2000']],\n",
       " [[['Lapped textures.',\n",
       "    'We present for creating texture over an surface mesh using an example 2D texture. The approach is to identify interesting regions (texture patches) in the 2D example, and to repeatedly paste them onto the surface until it is completely covered. We call such a collection of overlapping patches a lapped texture. It is rendered using compositing operations, either into a traditional global texture map during a preprocess, or directly with the surface at runtime. The runtime compositing approach avoids resampling artifacts and drastically reduces texture memory requirements. Through a simple interface, the user specifies a tangential vector field over the surface, providing local control over the texture scale, and for anisotropic textures, the orientation. To paste a texture patch onto the surface, a surface patch is grown and parametrized over texture space. Specifically, we optimize the parametrization of each surface patch such that the tangential vector field aligns everywhere with the standard frame of the texture patch. We show that this optimization is solved efficiently as a sparse linear system.'],\n",
       "   '2000']],\n",
       " [[['Silhouette clipping.',\n",
       "    'Approximating detailed with coarse, texture-mapped meshes results in polygonal silhouettes. To eliminate this artifact, we introduce silhouette clipping, a framework for efficiently clipping the rendering of coarse geometry to the exact silhouette of the original model. The coarse mesh is obtained using progressive hulls, a novel representation with the nesting property required for proper clipping. We describe an improved technique for constructing texture and normal maps over this coarse mesh. Given a perspective view, silhouettes are efficiently extracted from the original mesh using a precomputed search tree. Within the tree, hierarchical culling is achieved using pairs of anchored cones. The extracted silhouette edges are used to set the hardware stencil buffer and alpha buffer, which in turn clip and antialias the rendered coarse geometry. Results demonstrate that silhouette clipping can produce renderings of similar quality to high-resolution meshes in less rendering time.'],\n",
       "   '2000']],\n",
       " [[['Conservative volumetric visibility with occluder fusion.',\n",
       "    'Visibility determination is a key requirement in a wide range of graphics algorithms. This paper introduces a new approach to the computation of volume visibility, the detection of occluded portions of space as seen from a given region. The method is conservative and classifies regions as occluded only when they are guaranteed to be invisible. It operates on a discrete representation of space and uses the opaque interior of objects as occluders. This choice of occluders facilitates their extension into adjacent opaque regions of space, in essence maximizing their size and impact. Our method efficiently detects and represents the regions of space hidden by such occluders. It is the first one to use the property that occluders can also be extended into empty space provided this space is itself occluded from the viewing volume. This proves extremely effective for computing the occlusion by a set of occluders, effectively realizing occluder fusion. An auxiliary data structure represents occlusion in the scene and can then be queried to answer volume visibility questions. We demonstrate the applicability to visibility preprocessing for real-time walkthroughs and to shadow-ray acceleration for extended light sources in ray tracing, with significant acceleration in both cases.'],\n",
       "   '2000']],\n",
       " [[['Video textures.',\n",
       "    'This paper introduces a new type of medium, called a video texture, which has qualities somewhere between those of a photograph and a video. A video texture provides a continuous infinitely varying stream of images. While the individual frames of a video texture may be repeated from time to time, the video sequence as a whole is never repeated exactly. Video textures can be used in place of digital photos to infuse a static image with dynamic qualities and explicit actions. We present techniques for analyzing a video clip to extract its structure, and for synthesizing a new, similar looking video of arbitrary length. We combine video textures with view morphing techniques to obtain 3D video textures. We also introduce video-based animation, in which the synthesis of video textures can be guided by a user through high-level interactive controls. Applications of video textures and their extensions include the display of dynamic scenes on web pages, the creation of dynamic backdrops for special effects and games, and the interactive control of video-based animation.'],\n",
       "   '2000']],\n",
       " [[['Fast texture synthesis using tree-structured vector quantization.',\n",
       "    'Texture synthesis is important for many applications in computer graphics, vision, and image processing. However, it remains difficult to design an algorithm that is both efficient and capable of generating high quality results. In this paper, we present an efficient algorithm for realistic texture synthesis. The algorithm is easy to use and requires only a sample texture as input. It generates textures with perceived quality equal to or better than those produced by previous techniques, but runs two orders of magnitude faster. This permits us to apply texture synthesis to problems where it has traditionally been considered impractical. In particular, we have applied it to constrained synthesis for image editing and temporal texture generation. Our algorithm is derived from Markov Random Field texture models and generates textures through a deterministic searching process. We accelerate this synthesis process using tree-structured vector quantization.'],\n",
       "   '2000']],\n",
       " [[['Surface light fields for 3D photography.',\n",
       "    \"A surface light field is a function that assigns a color to each ray originating on a surface. Surface light fields are well suited to constructing virtual images of shiny objects under complex lighting conditions. This paper presents a framework for construction, compression, interactive rendering, and rudimentary editing of surface light fields of real objects. Generalization of vector quantization and principal component analysis are used to construct a compressed representation of an object's surface light field from photographs and range scans. A new rendering algorithm achieves interactive rendering of images from the compressed representation, incorporating view-dependent geometric level-of-detail control. The surface light field representation can also be directly edited to yield plausible surface light fields for small changes in surface geometry and reflectance properties.\"],\n",
       "   '2000']],\n",
       " [[['Animating explosions.',\n",
       "    'In this paper, we introduce techniques for animating explosions and their effects. The primary effect of an explosion is a disturbance that causes a shock wave to propagate through the surrounding medium. The disturbance determines the behavior of nearly all other secondary effects seen in explosion. We simulate the propagation of an explosion through the surrounding air using a computational fluid dynamics model based on the equations for compressible, viscous flow. To model the numerically stable formation of shocks along blast wave fronts, we employ an integration method that can handle steep pressure gradients without introducing inappropriate damping. The system includes two-way coupling between solid objects and surrounding fluid. Using this technique, we can generate a variety of effects including shaped explosive charges, a projectile propelled from a chamber by an explosion, and objects damaged by a blast. With appropriate rendering techniques, our explosion model can be used to create such visual effects as fireballs, dust clouds, and the refraction of light caused by a blast wave.'],\n",
       "   '2000']],\n",
       " [[['Latent semantic-space: iterative scaling improves precision of inter-document similarity measurement.',\n",
       "    'We present a novel algorithm that creates document vectors with reduced dimensionality. This work was motivated by an application characterizing relationships among documents in a collection. Our algorithm yielded inter-document similarities with an average precision up to 17.8% higher than that of singular value decomposition (SVD) used for Latent Semantic Indexing. The best performance was achieved with dimensional reduction rates that were 43% higher than SVD on average. Our algorithm creates basis vectors for a reduced space by iteratively &ldquo;scaling&rdquo; vectors and computing eigenvectors. Unlike SVD, it breaks the symmetry of documents and terms to capture information more evenly across documents. We also discuss correlation with a probabilistic model and evaluate a method for selecting the dimensionality using log-likelihood estimation.'],\n",
       "   '2000']],\n",
       " [[['Evaluating evaluation measure stability.',\n",
       "    'This paper presents a novel way of examining the accuracy of the evaluation measures commonly used in information retrieval experiments. It validates several of the rules-of-thumb experimenters use, such as the number of queries needed for a good experiment is at least 25 and 50 is better, while challenging other beliefs, such as the common evaluation measures are equally reliable. As an example, we show that Precision at 30 documents has about twice the average error rate as Average Precision has. These results can help information retrieval researchers design experiments that provide a desired level of confidence in their results. In particular, we suggest researchers using Web measures such as Precision at 10 documents will need to use many more than 50 queries or will have to require two methods to have a very large difference in evaluation scores before concluding that the two methods are actually different.'],\n",
       "   '2000']],\n",
       " [[['Topical locality in the Web.',\n",
       "    'Most web pages are linked to others with related content. This idea, combined with another that says that text in, and possibly around, HTML anchors describe the pages to which they point, is the foundation for a usable World-Wide Web. In this paper, we examine to what extent these ideas hold by empirically testing whether topical locality mirrors spatial locality of pages on the Web. In particular, we find that the likelihood of linked pages having similar textual content to be high; the similarity of sibling pages increases when the links from the parent are close together; titles, descriptions, and anchor text represent at least part of the target page; and that anchor text may be a useful discriminator among unseen child pages. These results show the foundations necessary for the success of many web systems, including search engines, focused crawlers, linkage analyzers, and intelligent web agents.'],\n",
       "   '2000']],\n",
       " [[['Hierarchical classification of Web content.',\n",
       "    'This paper explores the use of hierarchical structure for classifying a large, heterogeneous collection of web content. The hierarchical structure is initially used to train different second-level classifiers. In the hierarchical case, a model is learned to distinguish a second-level category from other categories within the same top level. In the flat non-hierarchical case, a model distinguishes a second-level category from all other second-level categories. Scoring rules can further take advantage of the hierarchy by considering only second-level categories that exceed a threshold at the top level. We use support vector machine (SVM) classifiers, which have been shown to be efficient and effective for classification, but not previously explored in the context of hierarchical classification. We found small advantages in accuracy for hierarchical models over flat models. For the hierarchical approach, we found the same accuracy using a sequential Boolean decision rule and a multiplicative decision rule. Since the sequential approach is much more efficient, requiring only 14%-16% of the comparisons used in the other approaches, we find it to be a good choice for classifying text into large hierarchical structures.'],\n",
       "   '2000']],\n",
       " [[['Event tracking based on domain dependency.',\n",
       "    'This paper proposes a method for event tracking on broadcast news stories based on distinction between a topic and an event. A topic and an event are identified using a simple criterion called domain dependency of words: how greatly a word features a given set of data. The method was tested on the TDT corpus which has been developed by the TDT Pilot Study and the result can be regarded as promising the usefulness of the method.'],\n",
       "   '2000']],\n",
       " [[['An investigation of linguistic features and clustering algorithms for topical document clustering.',\n",
       "    \"We investigate four hierarchical clustering methods (single-link, complete-link, groupwise-average, and single-pass) and two linguistically motivated text features (noun phrase heads and proper names) in the context of document clustering. A statistical model for combining similarity information from multiple sources is described and applied to DARPA's Topic Detection and Tracking phase 2 (TDT2) data. This model, based on log-linear regression, alleviates the need for extensive search in order to determine optimal weights for combining input features. Through an extensive series of experiments with more than 40,000 documents from multiple news sources and modalities, we establish that both the choice of clustering algorithm and the introduction of the additional features have an impact on clustering performance. We apply our optimal combination of features to the TDT2 test data, obtaining partitions of the documents that compare favorably with the results obtained by participants in the official TDT2 competition.\"],\n",
       "   '2000']],\n",
       " [[['Text filtering by boosting naive bayes classifiers.',\n",
       "    'Several machine learning algorithms have recently been used for text categorization and filtering. In particular, boosting methods such as AdaBoost have shown good performance applied to real text data. However, most of existing boosting algorithms are based on classifiers that use binary-valued features. Thus, they do not fully make use of the weight information provided by standard term weighting methods. In this paper, we present a boosting-based learning method for text filtering that uses naive Bayes classifiers as a weak learner. The use of naive Bayes allows the boosting algorithm to utilize term frequency information while maintaining probabilistically accurate confidence ratio. Applied to TREC-7 and TREC-8 filtering track documents, the proposed method obtained a significant improvement in LF1, LF2, F1 and F3 measures compared to the best results submitted by other TREC entries.'],\n",
       "   '2000']],\n",
       " [[['Partial collection replication versus caching for information retrieval systems.',\n",
       "    'The explosion of content in distributed information retrieval (IR) systems requires new mechanisms to attain timely and accurate retrieval of unstructured text. In this paper, we compare two mechanisms to improve IR system performance: partial collection replication and caching. When queries have locality, both mechanisms return results more quickly than sending queries to the original collection(s). Caches return results when queries exactly match a previous one. Partial replicas are a form of caching that return results when the IR technology determines the query is a good match. Caches are simpler and faster, but replicas can increase locality by detecting similarity between queries that are not exactly the same. We use real traces from THOMAS and Excite to measure query locality and similarity. With a very restrictive definition of query similarity, similarity improves query locality up to 15% over exact match. We use a validated simulator to compare their performance, and find that even if the partial replica hit rate increases only 3 to 6%, it will outperform simple caching under a variety of configurations. A combined approach will probably yield the best performance.'],\n",
       "   '2000']],\n",
       " [[['Automatic adaptation of proper noun dictionaries through cooperation of machine learning and probabilistic methods.',\n",
       "    'The recognition of Proper Nouns (PNs) is considered an important task in the area of Information Retrieval and Extraction. However the high performance of most existing PN classifiers heavily depends upon the availability of large dictionaries of domain-specific Proper Nouns, and a certain amount of manual work for rule writing or manual tagging. Though it is not a heavy requirement to rely on some existing PN dictionary (often these resources are available on the web), its coverage of a domain corpus may be rather low, in absence of manual updating. In this paper we propose a technique for the automatic updating of an PN Dictionary through the cooperation of an inductive and a probabilistic classifier. In our experiments we show that, whenever an existing PN Dictionary allows the identification of 50% of the proper nouns within a corpus, our technique allows, without additional manual effort, the successful recognition of about 90% of the remaining 50%.'],\n",
       "   '2000']],\n",
       " [[['The impact of database selection on distributed searching.',\n",
       "    'The proliferation of online information resources increases the importance of effective and efficient distributed searching. Distributed searching is cast in three parts &mdash; database selection, query processing, and results merging. In this paper we examine the effect of database selection on retrieval performance. We look at retrieval performance in three different distributed retrieval testbeds and distill some general results. First we find that good database selection can result in better retrieval effectiveness than can be achieved in a centralized database. Second we find that good performance can be achieved when only a few sites are selected and that the performance generally increases as more sites are selected. Finally we find that when database selection is employed, it is not necessary to maintain collection wide information (CWI), e.g. global idf. Local information can be used to achieve superior performance. This means that distributed systems can be engineered with more autonomy and less cooperation. This work suggests that improvements in database selection can lead to broader improvements in retrieval performance, even in centralized (i.e. single database) systems. Given a centralized database and a good selection mechanism, retrieval performance can be improved by decomposing that database conceptually and employing a selection step.'],\n",
       "   '2000']],\n",
       " [[['Question-answering by predictive annotation.',\n",
       "    'We present a new technique for question answering called Predictive Annotation. Predictive Annotation identifies potential answers to questions in text, annotates them accordingly and indexes them. This technique, along with a complementary analysis of questions, passage-level ranking and answer selection, produces a system effective at answering natural-language fact-seeking questions posed against large document collections. Experimental results show the effects of different parameter settings and lead to a number of general observations about the question-answering problem.'],\n",
       "   '2000']],\n",
       " [[['Link-based and content-based evidential information in a belief network model.',\n",
       "    'This work presents an information retrieval model developed to deal with hyperlinked environments. The model is based on belief networks and provides a framework for combining information extracted from the content of the documents with information derived from cross-references among the documents. The information extracted from the content of the documents is based on statistics regarding the keywords in the collection and is one of the basis for traditional information retrieval (IR) ranking algorithms. The information derived from cross-references among the documents is based on link references in a hyperlinked environment and has received increased attention lately due to the success of the Web. We discuss a set of strategies for combining these two types of sources of evidential information and experiment with them using a reference collection extracted from the Web. The results show that this type of combination can improve the retrieval performance without requiring any extra information from the users at query time. In our experiments, the improvements reach up to 59% in terms of average precision figures.'],\n",
       "   '2000']],\n",
       " [[['Document clustering using word clusters via the information bottleneck method.',\n",
       "    'We present a novel implementation of the recently introduced information bottleneck method for unsupervised document clustering. Given a joint empirical distribution of words and documents, p(x, y), we first cluster the words, Y, so that the obtained word clusters, Ytilde;, maximally preserve the information on the documents. The resulting joint distribution. p(X, Ytilde;), contains most of the original information about the documents, I(X; Ytilde;) &ap; I(X; Y), but it is much less sparse and noisy. Using the same procedure we then cluster the documents, X, so that the information about the word-clusters is preserved. Thus, we first find word-clusters that capture most of the mutual information about to set of documents, and then find document clusters, that preserve the information about the word clusters. We tested this procedure over several document collections based on subsets taken from the standard 20Newsgroups corpus. The results were assessed by calculating the correlation between the document clusters and the correct labels for these documents. Finding from our experiments show that this double clustering procedure, which uses the information bottleneck method, yields significantly superior performance compared to other common document distributional clustering algorithms. Moreover, the double clustering procedure improves all the distributional clustering methods examined here.'],\n",
       "   '2000']],\n",
       " [[['A novel method for the evaluation of Boolean query effectiveness across a wide operational range.',\n",
       "    'Traditional methods for the system-oriented evaluation of Boolean IR system suffer from validity and reliability problems. Laboratory-based research neglects the searcher and studies suboptimal queries. Research on operational systems fails to make a distinction between searcher performance and system performance. This approach is neither capable of measuring performance at standard points of operation (e.g. across R0.0-R1.0). A new laboratory-based evaluation method for Boolean IR systems is proposed. It is based on a controlled formulation of inclusive query plans, on an automatic conversion of query plans into elementary queries, and on combining elementary queries into optimal queries at standard points of operation. Major results of a large case experiment are reported. The validity, reliability, and efficiency of the method are considered in the light of empirical and analytical test data.'],\n",
       "   '2000']],\n",
       " [[['Structured translation for cross-language information retrieval.',\n",
       "    \"The paper introduces a query translation model that reflects the structure of the cross-language information retrieval task. The model is based on a structured bilingual dictionary in which the translations of each term are clustered into groups with distinct meanings. Query translation is modeled as a two-stage process, with the system first determining the intended meaning of a query term and then selecting translations appropriate to that meaning that might appear in the document collection. An implementation of structured translation based on automatic dictionary clustering is described and evaluated by using Chinese queries to retrieve English documents. Structured translation achieved an average precision that was statistically indistinguishable from Pirkola's technique for very short queries, but Pirkola's technique outperformed structured translation on long queries. The paper concludes with some observations on future work to improve retrieval effectiveness and on other potential uses of structured translation in interactive cross-language retrieval applications.\"],\n",
       "   '2000']],\n",
       " [[['Automatic generation of overview timelines.',\n",
       "    'We present a statistical model of feature occurrence over time, and develop tests based on classical hypothesis testing for significance of term appearance on a given date. Using additional classical hypothesis testing we are able to combine these terms to generate &ldquo;topics&rdquo; as defined by the Topic Detection and Tracking study. The groupings of terms obtained can be used to automatically generate an interactive timeline displaying the major events and topics covered by the corpus. To test the validity of our technique we extracted a large number of these topics from a test corpus and had human evaluators judge how well the selected features captured the gist of the topics, and how they overlapped with a set of known topics from the corpus. The resulting topics were highly rated by evaluators who compared them to known topics.'],\n",
       "   '2000']],\n",
       " [[['Improving text categorization methods for event tracking.',\n",
       "    'Automated tracking of events from chronologically ordered document streams is a new challenge for statistical text classification. Existing learning techniques must be adapted or improved in order to effectively handle difficult situations where the number of positive training instances per event is extremely small, the majority of training documents are unlabelled, and most of the events have a short duration in time. We adapted several supervised text categorization methods, specifically several new variants of the k-Nearest Neighbor (kNN) algorithm and a Rocchio approach, to track events. All of these methods showed significant improvement (up to 71% reduction in weighted error rates) over the performance of the original kNN algorithm on TDT benchmark collections, making kNN among the top-performing systems in the recent TDT3 official evaluation. Furthermore, by combining these methods, we significantly reduced the variance in performance of our event tracking system over different data collections, suggesting a robust solution for parameter optimization.'],\n",
       "   '2000']],\n",
       " [[['Feasibility of a serverless distributed file system deployed on an existing set of desktop PCs.',\n",
       "    'We consider an architecture for a serverless distributed file system that does not assume mutual trust among the client computers. The system provides security, availability, and reliability by distributing multiple encrypted replicas of each file among the client machines. To assess the feasibility of deploying this system on an existing desktop infrastructure, we measure and analyze a large set of client machines in a commercial environment. In particular, we measure and report results on disk usage and content; file activity; and machine uptimes, lifetimes, and loads. We conclude that the measured desktop infrastructure would passably support our proposed system, providing availability on the order of one unfilled file request per user per thousand days.'],\n",
       "   '2000']],\n",
       " [[['Cluster reserves: a mechanism for resource management in cluster-based network servers.',\n",
       "    'In network (e.g., Web) servers, it is often desirable to isolate the performance of different classes of requests from each other. That is, one seeks to achieve that a certain minimal proportion of server resources are available for a class of requests, independent of the load imposed by other requests. Recent work demonstrates how to achieve this performance isolation in servers consisting of a single, centralized node; however, achieving performance isolation in a distributed, cluster based server remains a problem.This paper introduces a new abstraction, the cluster reserve, which represents a resource principal in a cluster based network server. We present a design and evaluate a prototype implementation that extends existing techniques for performance isolation on a single node server to cluster based servers.In our design, the dynamic cluster-wide resource management problem is formulated as a constrained optimization problem, with the resource allocations on individual machines as independent variables, and the desired cluster-wide resource allocations as constraints. Periodically collected resource usages serve as further inputs to the problem.Experimental results show that cluster reserves are effective in providing performance isolation in cluster based servers. We demonstrate that, in a number of different scenarios, cluster reserves are effective in ensuring performance isolation while enabling high utilization of the server resources.'],\n",
       "   '2000']],\n",
       " [[['Towards application/file-level characterization of block references: a case for fine-grained buffer management.',\n",
       "    'Two contributions are made in this paper. First, we show that system level characterization of file block references is inadequate for maximizing buffer cache performance. We show that a finer-grained characterization approach is needed. Though application level characterization methods have been proposed, this is the first attempt, to the best of our knowledge, to consider file level characterizations. We propose an Application/File-level Characterization (AFC) scheme where we detect on-line the reference characteristics at the application level and then at the file level, if necessary. The results of this characterization are used to employ appropriate replacement policies in the buffer cache to maximize performance. The second contribution is in proposing an efficient and fair buffer allocation scheme. Application or file level resource management is infeasible unless there exists an allocation scheme that is efficient and fair. We propose the &Dgr;HIT allocation scheme that takes away a block from the application/file where the removal results in the smallest reduction in the number of expected buffer cache hits. Both the AFC and &Dgr;HIT schemes are on-line schemes that detect and allocate as applications execute. Experiments using trace-driven simulations show that substantial performance improvements can be made. For single application executions the hit ratio increased an average of 13 percentage points compared to the LRU policy, with a maximum increase of 59 percentage points, while for multiple application executions, the increase is an average of 12 percentage points, with a maximum of 32 percentage points for the workloads considered.'],\n",
       "   '2000']],\n",
       " [[['AMVA techniques for high service time variability.',\n",
       "    'Motivated by experience gained during the validation of a recent Approximate Mean Value Analysis (AMVA) model of modern shared memory architectures, this paper re-examines the &ldquo;standard&rdquo; AMVA approximation for non-exponential FCFS queues. We find that this approximation is often inaccurate for FCFS queues with high service time variability. For such queues, we propose and evaluate: (1) AMVA estimates of the mean residual service time at an arrival instant that are much more accurate than the standard AMVA estimate, (2) a new AMVA technique that provides a much more accurate estimate of mean center residence time than the standard AMVA estimate, and (3) a new AMVA technique for computing the mean residence time at a &ldquo;downstream&rdquo; queue which has a more bursty arrival process than is assumed in the standard AMVA equations. Together, these new techniques increase the range of applications to which AMVA may be fruitfully applied, so that for example, the memory system architecture of shared memory systems with complex modern processors can be analyzed with these computationally efficient methods.'],\n",
       "   '2000']],\n",
       " [[['Quantifying the energy consumption of a pocket computer and a Java virtual machine.',\n",
       "    \"In this paper, we examine the energy consumption of a state-of-the-art pocket computer. Using a data acquisition system, we measure the energy consumption of the Itsy Pocket Computer, developed by Compaq Computer Corporation's Palo Alto Research Labs. We begin by showing that the energy usage characteristics of the Itsy differ markedly from that of a notebook computer. Then, since we expect that flexible software environments will become increasingly prevalent on pocket computers, we consider applications running in a Java environment. In particular, we explain some of the Java design tradeoffs applicable to pocket computers, and quantify their energy costs. For the design options we considered and the three workloads we studied, we find a maximum change in energy use of 25%.\"],\n",
       "   '2000']],\n",
       " [[['Stable Internet routing without global coordination.',\n",
       "    \"The Border Gateway Protocol (BGP) allows an autonomous system (AS) to apply diverse local policies for selecting routes and propagating reachability information to other domains. However, BGP permits ASs to have conflicting policies that can lead to routing instability. This paper proposes a set of guidelines for an AS to follow in setting its routing policies, without requiring coordination with other ASs. Our approach exploits the Internet's hierarchical structure and the commercial relationships between ASs to impose a partial order on the set of routes to each destination. The guidelines conform to conventional traffic-engineering practices of ISPs, and provide each AS with significant flexibility in selecting its local policies. Furthermore, the guidelines ensure route convergence even under changes in the topology and routing policies. Drawing on a formal model of BGP, we prove that following our proposed policy guidelines guarantees route convergence. We also describe how our methodology can be applied to new types of relationships between ASs, how to verify the hierarchical AS relationships, and how to realize our policy guidelines. Our approach has significant practical value since it preserves the ability of each AS to apply complex local policies without divulging its BGP configurations to others.\"],\n",
       "   '2000']],\n",
       " [[['An analytical model of the working-set sizes in decision-support systems.',\n",
       "    'This paper presents an analytical model to study how working sets scale with database size and other applications parameters in decision-support systems (DSS). The model uses application parameters, that are measured on down-scaled database executions, to predict cache miss ratios for executions of large databases.By applying the model to two database engines and typical DSS queries we find that, even for large databases, the most performance-critical working set is small and is caused by the instructions and private data that are required to access a single tuple. Consequently, its size is not affected by the database size. Surprisingly, database data may also exhibit temporal locality but the size of its working set critically depends on the structure of the query, the method of scanning, and the size and the content of the database.'],\n",
       "   '2000']],\n",
       " [[['Memory system behavior of Java programs: methodology and analysis.',\n",
       "    \"This paper studies the memory system behavior of Java programs by analyzing memory reference traces of several SPECjvm98 applications running with a Just-In-Time (JIT) compiler. Trace information is collected by an exception-based tracing tool called JTRACE, without any instrumentation to the Java programs or the JIT compiler.First, we find that the overall cache miss ratio is increased due to garbage collection, which suffers from higher cache misses compared to the application. We also note that going beyond 2-way cache associativity improves the cache miss ratio marginally. Second, we observe that Java programs generate a substantial amount of short-lived objects. However, the size of frequently-referenced long-lived objects is more important to the cache performance, because it tends to determine the application's working set size. Finally, we note that the default heap configuration which starts from a small initial heap size is very inefficient since it invokes a garbage collector frequently. Although the direct costs of garbage collection decrease as we increase the available heap size, there exists an optimal heap size which minimizes the total execution time due to the interaction with the virtual memory performance.\"],\n",
       "   '2000']],\n",
       " [[['An efficient algorithm for finding a path subject to two additive constraints.',\n",
       "    \"One of the key issues in providing end-to-end quality-of-service guarantees in packet networks is how to determine a feasible route that satisfies a set of constraints while simultaneously maintaining high utilization of network resources. In general, finding a path subject to multiple additive constraints (e.g., delay, delay-jitter) is an NP-complete problem that cannot be exactly solved in polynomial time. Accordingly, heuristics and approximation algorithms are often used to address to this problem. Previously proposed algorithms suffer from either excessive computational cost or low performance. In this paper, we provide an efficient approximation algorithm for finding a path subject to two additive constraints. The worst-case computational complexity of this algorithm is within a logarithmic number of calls to Dijkstra's shortest path algorithm. Its average complexity is much lower than that, as demonstrated by simulation results. The performance of the proposed algorithm is justified via theoretical performance bounds. To achieve further performance improvement, several extensions to the basic algorithm are also provided at low extra computational cost. Extensive simulations are used to demonstrate the high performance of the proposed algorithm and to contrast it with other path selection algorithms.\"],\n",
       "   '2000']],\n",
       " [[['Using the exact state space of a Markov model to compute approximate stationary measures.',\n",
       "    'We present a new approximation algorithm based on an exact representation of the state space S, using decision diagrams, and of the transition rate matrix R, using Kronecker algebra, for a Markov model with K submodels. Our algorithm builds and solves K Markov chains, each corresponding to a different aggregation of the exact process, guided by the structure of the decision diagram, and iterates on their solution until their entries are stable. We prove that exact results are obtained if the overall model has a product-form solution. Advantages of our method include good accuracy, low memory requirements, fast execution times, and a high degree of automation, since the only additional information required to apply it is a partition of the model into the K submodels. As far as we know, this is the first time an approximation algorithm has been proposed where knowledge of the exact state space is explicitly used.'],\n",
       "   '2000']],\n",
       " [[['Efficient performance prediction for modern microprocessors.',\n",
       "    \"Generating an accurate estimate of the performance of a program on a given system is important to a large number of people. Computer architects, compiler writers, and developers all need insight into a machine's performance. There are a number of performance estimation techniques in use, from profile-based approaches to full machine simulation. This paper discusses a profile-based performance estimation technique that uses a lightweight instrumentation phase that runs in order number of dynamic instructions, followed by an analysis phase that runs in roughly order number of static instructions. This technique accurately predicts the performance of the core pipeline of a detailed out-of-order issue processor model while scheduling far fewer instructions than does full simulation. The difference between the predicted execution time and the time obtained from full simulation is only a few percent.\"],\n",
       "   '2000']],\n",
       " [[['Implications of proxy caching for provisioning networks and servers.',\n",
       "    'In this paper, we examine the potential benefits of web proxy caches in improving the effective capacity of servers and networks. Since networks and servers are typically provisioned based on a high percentile of the load, we focus on the effects of proxy caching on the tail of the load distribution. We find that, unlike their substantial impact on the average load, proxies have a diminished impact on the tail of the load distribution. The exact reduction in the tail and the corresponding capacity savings depend on the percentile of the load distribution chosen for provisioning networks and servers&mdash;the higher the percentile, the smaller the savings. In particular, compared to over a 50% reduction in the average load, the savings in network and server capacity is only 20-35% for the 99th percentile of the load distribution. We also find that while proxies can be somewhat useful in smoothing out some of the burstiness in web workloads; the resulting workload continues, however, to exhibit substantial burstiness and a heavy-tailed nature. We identify large objects with poor locality to be the limiting factor that diminishes the impact of proxies on the tail of load distribution. We conclude that, while proxies are immensely useful to users due to the reduction in the average response time, they are less effective in improving the capacities of networks and servers.'],\n",
       "   '2000']],\n",
       " [[['Comparing random data allocation and data striping in multimedia servers.',\n",
       "    'We compare performance of a multimedia storage server based on a random data allocation layout and block replication with traditional data striping techniques. Data striping techniques in multimedia servers are often designed for restricted workloads, e.g. sequential access patterns with CBR (constant bit rate) requirements. On the other hand, a system based on random data allocation can support virtually any type of multimedia application, including VBR (variable bit rate) video or audio, and interactive applications with unpredictable access patterns, such as 3D interactive virtual worlds, interactive scientific visualizations, etc. Surprisingly, our results show that system performance with random data allocation is competitive and sometimes even outperforms traditional data striping techniques, for the workloads for which data striping is designed to work best; i.e. streams with sequential access patterns and CBR requirements. Due to its superiority in supporting general workloads and competitive system performance, we believe that random data allocation will be the scheme of choice for next generation multimedia servers.'],\n",
       "   '2000']],\n",
       " [[['Congressional Samples for Approximate Answering of Group-By Queries.',\n",
       "    'In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex decision support queries using precomputed summary statistics, such as samples. Decision support queries routinely segment the data into groups and then aggregate the information in each group (group-by queries). Depending on the data, there can be a wide disparity between the number of data items in each group. As a result, approximate answers based on uniform random samples of the data can result in poor accuracy for groups with very few data items, since such groups will be represented in the sample by very few (often zero) tuples. In this paper, we propose a general class of techniques for obtaining fast, highly-accurate answers for group-by queries. These techniques rely on precomputed non-uniform (biased) samples of the data. In particular, we propose congressional samples, a hybrid union of uniform and biased samples. Given a fixed amount of space, congressional samples seek to maximize the accuracy for all possible group-by queries on a set of columns. We present a one pass algorithm for constructing a congressional sample and use this technique to also incrementally maintain the sample up-to-date without accessing the base relation. We also evaluate query rewriting strategies for providing approximate answers from congressional samples. Finally, we conduct an extensive set of experiments on the TPC-D database, which demonstrates the efficacy of the techniques proposed.'],\n",
       "   '2000']],\n",
       " [[['Privacy-Preserving Data Mining.',\n",
       "    'A fruitful direction for future data mining research will be the development of techniques that incorporate privacy concerns. Specifically, we address the following question. Since the primary task in data mining is the development of models about aggregated data, can we develop accurate models without access to precise information in individual data records? We consider the concrete case of building a decision-tree classifier from training data in which the values of individual records have been perturbed. The resulting data records look very different from the original records and the distribution of data values is also very different from the original distribution. While it is not possible to accurately estimate original values in individual data records, we propose a novel reconstruction procedure to accurately estimate the distribution of original data values. By using these reconstructed distributions, we are able to build classifiers whose accuracy is comparable to the accuracy of classifiers built with the original data.'],\n",
       "   '2000']],\n",
       " [[['Finding Generalized Projected Clusters In High Dimensional Spaces.',\n",
       "    'High dimensional data has always been a challenge for clustering algorithms because of the inherent sparsity of the points. Recent research results indicate that in high dimensional data, even the concept of proximity or clustering may not be meaningful. We discuss very general techniques for projected clustering which are able to construct clusters in arbitrarily aligned subspaces of lower dimensionality. The subspaces are specific to the clusters themselves. This definition is substantially more general and realistic than currently available techniques which limit the method to only projections from the original set of attributes. The generalized projected clustering technique may also be viewed as a way of trying to redefine clustering for high dimensional applications by searching for hidden subspaces with clusters which are created by inter-attribute correlations. We provide a new concept of using extended cluster feature vectors in order to make the algorithm scalable for very large databases. The running time and space requirements of the algorithm are adjustable, and are likely ta tradeoff with better accuracy.'],\n",
       "   '2000']],\n",
       " [[['LOF: Identifying Density-Based Local Outliers.',\n",
       "    'For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.'],\n",
       "   '2000']],\n",
       " [[['NiagaraCQ: A Scalable Continuous Query System for Internet Databases.',\n",
       "    \"Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system's performance and scalability.\"],\n",
       "   '2000']],\n",
       " [[['Finding Replicated Web Collections.',\n",
       "    'Many web documents (such as JAVA FAQs) are being replicated on the Internet. Often entire document collections (such as hyperlinked Linux manuals) are being replicated many times. In this paper, we make the case for identifying replicated documents and collections to improve web crawlers, archivers, and ranking functions used in search engines. The paper describes how to efficiently identify replicated documents and hyperlinked document collections. The challenge is to identify these replicas from an input data set of several tens of millions of web pages and several hundreds of gigabytes of textual data. We also present two real-life case studies where we used replication information to improve a crawler and a search engine. We report these results for a data set of 25 million web pages (about 150 gigabytes of HTML data) crawled from the web.'],\n",
       "   '2000']],\n",
       " [[['On Wrapping Query Languages and Efficient XML Integration.',\n",
       "    'Modern applications (Web portals, digital libraries, etc.) require integrated access to various information sources (from traditional DBMS to semistructured Web repositories), fast deployment and low maintenance cost in a rapidly evolving environment. Because of its flexibility, there is an increasing interest in using XML as a middleware model for such applications. XML enables fast wrapping and declarative integration. However, query processing in XML-based integration systems is still penalized by the lack of an algebra with adequate optimization properties and the difficulty to understand source query capabilities. In this paper, we propose an algebraic approach to support efficient XML query evaluation. We define a general purpose algebra suitable for semistructured on XML query languages. We show how this algebra can be used, with appropriate type information, to also wrap more structured query languages such as OQL or SQL. Finally, we develop new optimization techniques for XML-based integration systems.'],\n",
       "   '2000']],\n",
       " [[['Closest Pair Queries in Spatial Databases.',\n",
       "    'This paper addresses the problem of finding the K closest pairs between two spatial data sets, where each set is stored in a structure belonging in the R-tree family. Five different algorithms (four recursive and one iterative) are presented for solving this problem. The case of 1 closest pair is treated as a special case. An extensive study, based on experiments performed with synthetic as well as with real point data sets, is presented. A wide range of values for the basic parameters affecting the performance of the algorithms, especially the effect of overlap between the two data sets, is explored. Moreover, an algorithmic as well as an experimental comparison with existing incremental algorithms addressing the same problem is presented. In most settings, the new algorithms proposed clearly outperform the existing ones.'],\n",
       "   '2000']],\n",
       " [[['Spatial Join Selectivity Using Power Laws.',\n",
       "    'We discovered a surprising law governing the spatial join selectivity across two sets of points. An example of such a spatial join is &ldquo;find the libraries that are within 10 miles of schools&rdquo;. Our law dictates that the number of such qualifying pairs follows a power law, whose exponent we call &ldquo;pair-count exponent&rdquo; (PC). We show that this law also holds for self-spatial-joins (&ldquo;find schools within 5 miles of other schools&rdquo;) in addition to the general case that the two point-sets are distinct. Our law holds for many real datasets, including diverse environments (geographic datasets, feature vectors from biology data, galaxy data from astronomy). In addition, we introduce the concept of the Box-Occupancy-Product-Sum (BOPS) plot, and we show that it can compute the pair-count exponent in a timely manner, reducing the run time by orders of magnitude, from quadratic to linear. Due to the pair-count exponent and our analysis (Law 1), we can achieve accurate selectivity estimates in constant time (O(1)) without the need for sampling or other expensive operations. The relative error in selectivity is about 30% with our fast BOPS method, and even better (about 10%), if we use the slower, quadratic method.'],\n",
       "   '2000']],\n",
       " [[['A Data Model and Data Structures for Moving Objects Databases.',\n",
       "    'We consider spatio-temporal databases supporting spatial objects with continuously changing position and extent, termed moving objects databases. We formally define a data model for such databases that includes complex evolving spatial structures such as line networks or multi-component regions with holes. The data model is given as a collection of data types and operations which can be plugged as attribute types into any DBMS data model (e.g. relational, or object-oriented) to obtain a complete model and query language. A particular novel concept is the sliced representation which represents a temporal development as a set of units, where unit types for spatial and other data types represent certain &ldquo;simple&rdquo; functions of time. We also show how the model can be mapped into concrete physical data structures in a DBMS environment.'],\n",
       "   '2000']],\n",
       " [[['WSQ/DSQ: A Practical Approach for Combined Querying of Databases and the Web.',\n",
       "    'We present WSQ/DSQ (pronounced &ldquo;wisk-disk&rdquo;), a new approach for combining the query facilities of traditional databases with existing search engines on the Web. WSQ, for Web-Supported (Database) Queries, leverages results from Web searches to enhance SQL queries over a relational database. DSQ, for Database-Supported (Web) Queries, uses information stored in the database to enhance and explain Web searches. This paper focuses primarily on WSQ, describing a simple, low-overhead way to support WSQ in a relational DBMS, and demonstrating the utility of WSQ with a number of interesting queries and results. The queries supported by WSQ are enabled by two virtual tables, whose tuples represent Web search results generated dynamically during query execution. WSQ query execution may involve many high-latency calls to one or more search engines, during which the query processor is idle. We present a lightweight technique called asynchronous iteration that can be integrated easily into a standard sequential query processor to enable concurrency between query processing and multiple Web search requests. Asynchronous iteration has broader applications than WSQ alone, and it opens up many interesting query optimization issues. We have developed a prototype implementation of WSQ by extending a DBMS with virtual tables and asynchronous iteration; performance results are reported.'],\n",
       "   '2000']],\n",
       " [[['XTRACT: A System for Extracting Document Type Descriptors from XML Documents.',\n",
       "    \"XML is rapidly emerging as the new standard for data representation and exchange on the Web. An XML document can be accompanied by a Document Type Descriptor (DTD) which plays the role of a schema for an XML data collection. DTDs contain valuable information on the structure of documents and thus have a crucial role in the efficient storage of XML data, as well as the effective formulation and optimization of XML queries. In this paper, we propose XTRACT, a novel system for inferring a DTD schema for a database of XML documents. Since the DTD syntax incorporates the full expressive power of regular expressions, naive approaches typically fail to produce concise and intuitive DTDs. Instead, the XTRACT inference algorithms employ a sequence of sophisticated steps that involve: (1) finding patterns in the input sequences and replacing them with regular expressions to generate &ldquo;general&rdquo; candidate DTDs, (2) factoring candidate DTDs using adaptations of algorithms from the logic optimization literature, and (3) applying the Minimum Description Length (MDL) principle to find the best DTD among the candidates. The results of our experiments with real-life and synthetic DTDs demonstrate the effectiveness of XTRACT's approach in inferring concise and semantically meaningful DTD schemas for XML databases.\"],\n",
       "   '2000']],\n",
       " [[['Approximating Multi-Dimensional Aggregate Range Queries over Real Attributes.',\n",
       "    'Finding approximate answers to multi-dimensional range queries over real valued attributes has significant applications in data exploration and database query optimization. In this paper we consider the following problem: given a table of d attributes whose domain is the real numbers, and a query that specifies a range in each dimension, find a good approximation of the number of records in the table that satisfy the query. We present a new histogram technique that is designed to approximate the density of multi-dimensional datasets with real attributes. Our technique finds buckets of variable size, and allows the buckets to overlap. Overlapping buckets allow more efficient approximation of the density. The size of the cells is based on the local density of the data. This technique leads to a faster and more compact approximation of the data distribution. We also show how to generalize kernel density estimators, and how to apply them on the multi-dimensional query approximation problem. Finally, we compare the accuracy of the proposed techniques with existing techniques using real and synthetic datasets.'],\n",
       "   '2000']],\n",
       " [[['Mining Frequent Patterns without Candidate Generation.',\n",
       "    'Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns. In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.'],\n",
       "   '2000']],\n",
       " [[['Eddies: Continuously Adaptive Query Processing',\n",
       "    'In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments. In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates promising results, with eddies performing nearly as well as a static optimizer/executor in static scenarios, and providing dramatic improvements in dynamic execution environments.'],\n",
       "   '2000']],\n",
       " [[['On Effective Multi-Dimensional Indexing for Strings.',\n",
       "    'As databases have expanded in scope from storing purely business data to include XML documents, product catalogs, e-mail messages, and directory data, it has become increasingly important to search databases based on wild-card string matching: prefix matching, for example, is more common (and useful) than exact matching, for such data. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the dimensions. Traditional multi-dimensional index structures, designed with (fixed length) numeric data in mind, are not suitable for matching unbounded length string data. In this paper, we describe a general technique for adapting a multi-dimensional index structure for wild-card indexing of unbounded length string data. The key ideas are (a) a carefully developed mapping function from strings to rational numbers, (b) representing an unbounded length string in an index leaf page by a fixed length offset to an external key, and (c) storing multiple elided tries, one per dimension, in an index page to prune search during traversal of index pages. These basic ideas affect all index algorithms. In this paper, we present efficient algorithms for different types of string matching. While our technique is applicable to a wide range of multi-dimensional index structures, we instantiate our generic techniques by adapting the 2-dimensional R-tree to string data. We demonstrate the space effectiveness and time benefits of using the string R-tree both analytically and experimentally.'],\n",
       "   '2000']],\n",
       " [[['Influence Sets Based on Reverse Nearest Neighbor Queries.',\n",
       "    'Inherent in the operation of many decision support and continuous referral systems is the notion of the &ldquo;influence&rdquo; of a data point on the database. This notion arises in examples such as finding the set of customers affected by the opening of a new store outlet location, notifying the subset of subscribers to a digital library who will find a newly added document most relevant, etc. Standard approaches to determining the influence set of a data point involve range searching and nearest neighbor queries. In this paper, we formalize a novel notion of influence based on reverse neighbor queries and its variants. Since the nearest neighbor relation is not symmetric, the set of points that are closest to a query point (i.e., the nearest neighbors) differs from the set of points that have the query point as their nearest neighbor (called the reverse nearest neighbors). Influence sets based on reverse nearest neighbor (RNN) queries seem to capture the intuitive notion of influence from our motivating examples. We present a general approach for solving RNN queries and an efficient R-tree based method for large data sets, based on this approach. Although the RNN query appears to be natural, it has not been studied previously. RNN queries are of independent interest, and as such should be part of the suite of available queries for processing spatial and multimedia data. In our experiments with real geographical data, the proposed method appears to scale logarithmically, whereas straightforward sequential scan scales linearly. Our experimental study also shows that approaches based on range searching or nearest neighbors are ineffective at finding influence sets of our interest.'],\n",
       "   '2000']],\n",
       " [[['WebView Materialization.',\n",
       "    'A WebView is a web page automatically created from base data typically stored in a DBMS. Given the multi-tiered architecture behind database-backed web servers, we have the option of materializing a WebView inside the DBMS, at the web server, or not at all, always computing it on the fly (virtual). Since WebViews must be up to date, materialized WebViews are immediately refreshed with every update on the base data. In this paper we compare the three materialization policies (materialized inside the DBMS, materialized at the web server and virtual) analytically, through a detailed cost model, and quantitatively, through extensive experiments on an implemented system. Our results indicate that materializing at the web server is a more scalable solution and can facilitate an order of magnitude more users than the virtual and materialized inside the DBMS policies, even under high update workloads.'],\n",
       "   '2000']],\n",
       " [[['On-line Reorganization in Object Databases.',\n",
       "    'Reorganization of objects in an object databases is an important component of several operations like compaction, clustering, and schema evolution. The high availability requirements (24 &times; 7 operation) of certain application domains requires reorganization to be performed on-line with minimal interference to concurrently executing transactions. In this paper, we address the problem of on-line reorganization in object databases, where a set of objects have to be migrated from one location to another. Specifically, we consider the case where objects in the database may contain physical references to other objects. Relocating an object in this case involves finding the set of objects (parents) that refer to it, and modifying the references in each parent. We propose an algorithm called the Incremental Reorganization Algorithm (IRA) that achieves the above task with minimal interference to concurrently executing transactions. The IRA algorithm holds locks on at most two distinct objects at any point of time. We have implemented IRA on Brahma, a storage manager developed at IIT Bombay, and conducted an extensive performance study. Our experiments reveal that IRA makes on-line reorganization feasible, with very little impact on the response times of concurrently executing transactions and on overall system throughput. We also describe how the IRA algorithm can handle system failures.'],\n",
       "   '2000']],\n",
       " [[['Towards Self-Tuning Data Placement in Parallel Database Systems.',\n",
       "    'Parallel database systems are increasingly being deployed to support the performance demands of end-users. While declustering data across multiple nodes facilitates parallelism, initial data placement may not be optimal due to skewed workloads and changing access patterns. To prevent performance degradation, the placement of data must be reorganized, and this must be done on-line to minimize disruption to the system. In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.'],\n",
       "   '2000']],\n",
       " [[['High Speed On-line Backup When Using Logical Log Operations.',\n",
       "    'Media recovery protects a database from failures of the stable medium by maintaining an extra copy of the database, called the backup, and a media recovery log. When a failure occurs, the database is &ldquo;restored&rdquo; from the backup, and the media recovery log is used to roll forward the database to the desired time, usually the current time. Backup must be both fast and &ldquo;on-line&rdquo;, i.e. concurrent with on-going update activity. Conventional online backup sequentially copies from the stable database, almost independent of the database cache manager, but requires page-oriented log operations. But results of logical operations must be flushed to a stable database (a backup is a stable database) in a constrained order to guarantee recovery. This order is not naturally achieved for the backup by a cache manager concerned only with crash recovery. We describe a &ldquo;full speed&rdquo; backup, only loosely coupled to the cache manager, and hence similar to current online backups, but effective for general logical log operations. This requires additional logging of cached objects to guarantee media recoverability. We then show how logging can be greatly reduced when log operations have a constrained form which nonetheless provides very useful additional logging efficiency for database systems.'],\n",
       "   '2000']],\n",
       " [[['XMILL: An Efficient Compressor for XML Data.',\n",
       "    'We describe a tool for compressing XML data, with applications in data exchange and archiving, which usually achieves about twice the compression ratio of gzip at roughly the same speed. The compressor, called XMill, incorporates and combines existing compressors in order to apply them to heterogeneous XML data: it uses zlib, the library function for gzip, a collection of datatype specific compressors for simple data types, and, possibly, user defined compressors for application specific data types.'],\n",
       "   '2000']],\n",
       " [[['LH*: A High-Availability Scalable Distributed Data Structure using Reed Solomon Codes.',\n",
       "    'LH*RS is a new high-availability Scalable Distributed Data Structure (SDDS). The data storage scheme and the search performance of LH*RS are basically these of LH*. LH*RS manages in addition the parity information to tolerate the unavailability of k &gne; 1 server sites. The value of k scales with the file, to prevent the reliability decline. The parity calculus uses the Reed -Solomon Codes. The storage and access performance overheads to provide the high-availability are about the smallest possible. The scheme should prove attractive to data-intensive applications.'],\n",
       "   '2000']],\n",
       " [[['Adaptive Multi-Stage Distance Join Processing.',\n",
       "    'A spatial distance join is a relatively new type of operation introduced for spatial and multimedia database applications. Additional requirements for ranking and stopping cardinality are often combined with the spatial distance join in on-line query processing or internet search environments. These requirements pose new challenges as well as opportunities for more efficient processing of spatial distance join queries. In this paper, we first present an efficient k-distance join algorithm that uses spatial indexes such as R-trees. Bi-directional node expansion and plane-sweeping techniques are used for fast pruning of distant pairs, and the plane-sweeping is further optimized by novel strategies for selecting a sweeping axis and direction. Furthermore, we propose adaptive multi-stage algorithms for k-distance join and incremental distance join operations. Our performance study shows that the proposed adaptive multi-stage algorithms outperform previous work by up to an order of magnitude for both k-distance join and incremental distance join queries, under various operational conditions.'],\n",
       "   '2000']],\n",
       " [[['Efficient and Cost-effective Techniques for Browsing and Indexing Large Video Databases.',\n",
       "    'We present in this paper a fully automatic content-based approach to organizing and indexing video data. Our methodology involves three steps: Step 1: We segment each video into shots using a Camera-Tracking technique. This process also extracts the feature vector for each shot, which consists of two statistical variances VarBA and VarOA. These values capture how much things are changing in the background and foreground areas of the video shot. Step 2: For each video, We apply a fully automatic method to build a browsing hierarchy using the shots identified in Step 1. Step 3: Using the VarBA and VarOA values obtained in Step 1, we build an index table to support a variance-based video similarity model. That is, video scenes/shots are retrieved based on given values of VarBA and VarOA. The above three inter-related techniques offer an integrated framework for modeling, browsing, and searching large video databases. Our experimental results indicate that they have many advantages over existing methods.'],\n",
       "   '2000']],\n",
       " [[['SQLEM: Fast Clustering in SQL using the EM Algorithm.',\n",
       "    'Clustering is one of the most important tasks performed in Data Mining applications. This paper presents an efficient SQL implementation of the EM algorithm to perform clustering in very large databases. Our version can effectively handle high dimensional data, a high number of clusters and more importantly, a very large number of data records. We present three strategies to implement EM in SQL: horizontal, vertical and a hybrid one. We expect this work to be useful for data mining programmers and users who want to cluster large data sets inside a relational DBMS.'],\n",
       "   '2000']],\n",
       " [[['Density Biased Sampling: An Improved Method for Data Mining and Clustering.',\n",
       "    \"Data mining in large data sets often requires a sampling or summarization step to form an in-core representation of the data that can be processed more efficiently. Uniform random sampling is frequently used in practice and also frequently criticized because it will miss small clusters. Many natural phenomena are known to follow Zipf's distribution and the inability of uniform sampling to find small clusters is of practical concern. Density Biased Sampling is proposed to probabilistically under-sample dense regions and over-sample light regions. A weighted sample is used to preserve the densities of the original data. Density biased sampling naturally includes uniform sampling as a special case. A memory efficient algorithm is proposed that approximates density biased sampling using only a single scan of the data. We empirically evaluate density biased sampling using synthetic data sets that exhibit varying cluster size distributions finding up to a factor of six improvement over uniform sampling.\"],\n",
       "   '2000']],\n",
       " [[['A Chase Too Far?',\n",
       "    'In a previous paper we proposed a novel method for generating alternative query plans that uses chasing (and back-chasing) with logical constraints. The method brings together use of indexes, use of materialized views, semantic optimization and join elimination (minimization). Each of these techniques is known separately to be beneficial to query optimization. The novelty of our approach is in allowing these techniques to interact systematically, eg. non-trivial use of indexes and materialized views may be enabled only by semantic constraints. We have implemented our method for a variety of schemas and queries. We examine how far we can push the method in term of complexity of both schemas and queries. We propose a technique for reducing the size of the search space by &ldquo;stratifying&rdquo; the sets of constraints used in the (back)chase. The experimental results demonstrate that our method is practical (i.e., feasible and worthwhile).'],\n",
       "   '2000']],\n",
       " [[['Efficient Algorithms for Mining Outliers from Large Data Sets.',\n",
       "    'In this paper, we propose a novel formulation for distance-based outliers that is based on the distance of a point from its kth nearest neighbor. We rank each point on the basis of its distance to its kth nearest neighbor and declare the top n points in this ranking to be outliers. In addition to developing relatively straightforward solutions to finding such outliers based on the classical nested-loop join and index join algorithms, we develop a highly efficient partition-based algorithm for mining outliers. This algorithm first partitions the input data set into disjoint subsets, and then prunes entire partitions as soon as it is determined that they cannot contain outliers. This results in substantial savings in computation. We present the results of an extensive experimental study on real-life and synthetic data sets. The results from a real-life NBA database highlight and reveal several expected and unexpected aspects of the database. The results from a study on synthetic data sets demonstrate that the partition-based algorithm scales well with respect to both data set size and data set dimensionality.'],\n",
       "   '2000']],\n",
       " [[['Making B-Trees Cache Conscious in Main Memory.',\n",
       "    'Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees. However, CSS-Trees are designed for decision support workloads with relatively static data. Although B+-Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers. Nevertheless, for applications that require incremental updates, traditional B+-Trees perform well. Our goal is to make B+-Trees as cache conscious as CSS-Trees without increasing their update cost too much. We propose a new indexing technique called &ldquo;Cache Sensitive B+-Trees&rdquo; (CSB+-Trees). It is a variant of B+-Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node. The rest of the children can be found by adding an offset to that address. Since only one child pointer is stored explicitly, the utilization of a cache line is high. CSB+-Trees support incremental updates in a way similar to B+-Trees. We also introduce two variants of CSB+-Trees. Segmented CSB+-Trees divide the child nodes into segments. Nodes within the same segment are stored contiguously and only pointers to the beginning of each segment are stored explicitly in each node. Segmented CSB+-Trees can reduce the copying cost when there is a split since only one segment needs to be moved. Full CSB+-Trees preallocate space for the full node group and thus reduce the split cost. Our performance studies show that CSB+-Trees are useful for a wide range of applications.'],\n",
       "   '2000']],\n",
       " [[['Data Mining on an OLTP System (Nearly) for Free.',\n",
       "    'This paper proposes a scheme for scheduling disk requests that takes advantage of the ability of high-level functions to operate directly at individual disk drives. We show that such a scheme makes it possible to support a Data Mining workload on an OLTP system almost for free: there is only a small impact on the throughput and response time of the existing workload. Specifically, we show that an OLTP system has the disk resources to consistently provide one third of its sequential bandwidth to a background Data Mining task with close to zero impact on OLTP throughput and response time at high transaction loads. At low transaction loads, we show much lower impact than observed in previous work. This means that a production OLTP system can be used for Data Mining tasks without the expense of a second dedicated system. Our scheme takes advantage of close interaction with the on-disk scheduler by reading blocks for the Data Mining workload as the disk head &ldquo;passes over&rdquo; them while satisfying demand blocks from the OLTP request stream. We show that this scheme provides a consistent level of throughput for the background workload even at very high foreground loads. Such a scheme is of most benefit in combination with an Active Disk environment that allows the background Data Mining application to also take advantage of the processing power and memory available directly on the disk drives.'],\n",
       "   '2000']],\n",
       " [[['MOCHA: A Self-Extensible Database Middleware System for Distributed Data Sources.',\n",
       "    \"We present MOCHA, a new self-extensible database middleware system designed to interconnect distributed data sources. MOCHA is designed to scale to large environments and is based on the idea that some of the user-defined functionality in the system should be deployed by the middleware system itself. This is realized by shipping Java code implementing either advanced data types or tailored query operators to remote data sources and have it executed remotely. Optimized query plans push the evaluation of powerful data-reducing operators to the data source sites while executing data-inflating operators near the client's site. The Volume Reduction Factor is a new and more explicit metric introduced in this paper to select the best site to execute query operators and is shown to be more accurate than the standard selectivity factor alone. MOCHA has been implemented in Java and runs on top of Informix and Oracle. We present the architecture of MOCHA, the ideas behind it, and a performance study using scientific data and queries. The results of this study demonstrate that MOCHA provides a more flexible, scalable and efficient framework for distributed query processing compared to those in existing middleware solutions.\"],\n",
       "   '2000']],\n",
       " [[['Indexing the Positions of Continuously Moving Objects.',\n",
       "    'The coming years will witness dramatic advances in wireless communications as well as positioning technologies. As a result, tracking the changing positions of objects capable of continuous movement is becoming increasingly feasible and necessary. The present paper proposes a novel, R*-tree based indexing technique that supports the efficient querying of the current and projected future positions of such moving objects. The technique is capable of indexing objects moving in one-, two-, and three-dimensional space. Update algorithms enable the index to accommodate a dynamic data set, where objects may appear and disappear, and where changes occur in the anticipated positions of existing objects. A comprehensive performance study is reported.'],\n",
       "   '2000']],\n",
       " [[['Efficient and Extensible Algorithms for Multi Query Optimization.',\n",
       "    'Complex queries are becoming commonplace, with the growing use of decision support systems. These complex queries often have a lot of common sub-expressions, either within a single query, or across multiple such queries run as a batch. Multiquery optimization aims at exploiting common sub-expressions to reduce evaluation cost. Multi-query optimization has hither-to been viewed as impractical, since earlier algorithms were exhaustive, and explore a doubly exponential search space. In this paper we demonstrate that multi-query optimization using heuristics is practical, and provides significant benefits. We propose three cost-based heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple modifications to the Volcano search strategy, and a greedy heuristic. Our greedy heuristic incorporates novel optimizations that improve efficiency greatly. Our algorithms are designed to be easily added to existing optimizers. We present a performance study comparing the algorithms, using workloads consisting of queries from the TPC-D benchmark. The study shows that our algorithms provide significant benefits over traditional optimization, at a very acceptable overhead in optimization time.'],\n",
       "   '2000']],\n",
       " [[['Counting, Enumerating, and Sampling of Execution Plans in a Cost-Based Query Optimizer.',\n",
       "    \"Testing an SQL database system by running large sets of deterministic or stochastic SQL statements is common practice in commercial database development. However, code defects often remain undetected as the query optimizer''s choice of an execution plan is not only depending on the query but strongly influenced by a large number of parameters describing the database and the hardware environment. Modifying these parameters in order to steer the optimizer to select other plans is difficult since this means anticipating often complex search strategies implemented in the optimizer. In this paper we devise algorithms for counting, exhaustive generation, and uniform sampling of plans from the complete search space. Our techniques allow extensive validation of both generation of alternatives, and execution algorithms with plans other than the optimized one---if two candidate plans fail to produce the same results, then either the optimizer considered an invalid plan, or the execution code is faulty. When the space of alternatives becomes too large for exhaustive testing, which can occur even with a handful of joins, uniform random sampling provides a mechanism for unbiased testing. The technique is implemented in Microsoft''s SQL Server, where it is an integral part of the validation and testing process.\"],\n",
       "   '2000']],\n",
       " [[['Answering Complex SQL Queries Using Automatic Summary Tables.',\n",
       "    'We investigate the problem of using materialized views to answer SQL queries. We focus on modern decision-support queries, which involve joins, arithmetic operations and other (possibly user-defined) functions, aggregation (often along multiple dimensions), and nested subqueries. Given the complexity of such queries, the vast amounts of data upon which they operate, and the requirement for interactive response times, the use of materialized views (MVs) of similar complexity is often mandatory for acceptable performance. We present a novel algorithm that is able to rewrite a user query so that it will access one or more of the available MVs instead of the base tables. The algorithm extends prior work by addressing the new sources of complexity mentioned above, that is, complex expressions, multidimensional aggregation, and nested subqueries. It does so by relying on a graphical representation of queries and a bottom-up, pair-wise matching of nodes from the query and MV graphs. This approach offers great modularity and extensibility, allowing for the rewriting of a large class of queries.'],\n",
       "   '2000']],\n",
       " [[['A system architecture for pervasive computing.',\n",
       "    'Pervasive computing, with its focus on users and their tasks rather than on computing devices and technology, provides an attractive vision for the future of computing. But, while hardware and networking infrastructure to realize this vision are increasingly becoming a reality, precious few applications run in this infrastructure. We believe that this lack of applications can be attributed to three characteristics that are inadequately addressed by existing systems. First, devices are heterogeneous, ranging from wearable devices to conventional computers. Second, network connectivity often is limited and intermittent. And, third, interactions typically involve several autonomous administrative domains. In this paper, we introduce a system architecture that directly addresses these challenges. Our architecture is targeted at application developers and administrators, and it supports mobile computations, persistent storage, and resource discovery within a single, comprehensive framework.'],\n",
       "   '2000']],\n",
       " [[['The benefits of event: driven energy accounting in power-sensitive systems.',\n",
       "    'A prerequisite of energy-aware scheduling is precise knowledge of any activity inside the computer system. Embedded hardware monitors (e.g., processor performance counters) have proved to offer valuable information in the field of performance analysis. The same approach can be applied to investigate the energy usage patterns of individual threads. We use information about active hardware units (e.g., integer/floating-point unit, cache/memory interface) gathered by event counters to establish a thread-specific energy accounting. The evaluation shows that the correlation of events and energy values provides the necessary information for energy-aware scheduling policies.Our approach to OS-directed power management adds the energy usage pattern to the runtime context of a thread. Depending on the field of application we present two scenarios that benefit from applying energy usage patterns: Workstations with passive cooling on the one hand and battery-powered mobile systems on the other hand.Energy-aware scheduling evaluates the energy usage of each thread and throttles the system activity so that the scheduling goal is achieved. In workstations we throttle the system if the average energy use exceeds a predefined power-dissipation capacity. This makes a compact, noiseless and affordable system design possible that meets sporadic yet high demands in computing power. Nowadays, more and more mobile systems offer the features of reducible clock speed and dynamic voltage scaling. Energy-aware scheduling can employ these features to yield a longer battery life by slowing down low-priority threads while preserving a certain quality of service.'],\n",
       "   '2000']],\n",
       " [[['Increasing relevance of memory hardware errors: a case for recoverable programming models.',\n",
       "    \"It is a common belief that most of computer system failures nowadays stem from programming errors. Computer systems are becoming more complex and harder to maintain and administer, making software errors an even more common case, while contemporary computer architectures are optimized for price and performance and not for availability. In this paper, we raise a case for an increasing relevance of memory hardware soft-errors. In particular with the introduction of 64-bit processors, memory scaling is significantly increased, resulting in higher probability for memory errors. At the same time, due to the ubiquitous use of computers, such as at higher altitudes, environmental conditions impact errors (terrestrial cosmic rays). Finally, in shared memory systems, the failure of one node's memory can take the whole machine down. Current commodity systems do not tolerate memory errors, neither commodity hardware (processors, memories, interconnects) nor software (operating systems, applications, application environments). At the same time, users expect increased reliability. We present the problems of such errors and some solutions for memory error recovery at the processor, operating system and programming model level.\"],\n",
       "   '2000']],\n",
       " [[['CarNet: a scalable ad hoc wireless network system.',\n",
       "    'CarNet is an application for a large ad hoc mobile network system that scales well without requiring a fixed network infrastructure to route messages. CarNet places radio nodes in cars, which communicate using Grid, a novel scalable routing system. Grid uses geographic forwarding and a scalable distributed location service to route packets from car to car without flooding the network. CarNet will support IP connectivity as well as applications such as cooperative highway congestion monitoring, fleet tracking, and discovery of nearby points of interest.'],\n",
       "   '2000']],\n",
       " [[['Building appliances out of components using Pebble.',\n",
       "    'Appliances are special purpose systems that offer high processing speed, ease of configuration, safety, fault isolation, and minimal need for administration by human experts. Traditional approaches to building an appliance operating system have been either building it from scratch [CacheOS] or stripping down a monolithic kernel to its basic components [Jaeger99]. The former approach is costly and the resulting product is likely to be highly specialized and not easily extensible. The latter approach is not easy, as the OS code and data structures are often shared and closely intertwined. The resulting OS is also likely to be coarse-grained and not easily customizable. Most appliances are network-centric (e.g. HTTP caches, proxies, file servers, routers) in the sense that they require high-performance network connections. Such performance is often achieved with application-specific specialization of system I/O [Cao95] requiring a modification of a portion of the operating system, such as the protocol stack or the file system.Safety is a major concern for appliances, since new functions are constantly added, and there is never enough time to debug all possible interactions, especially when third party software is running on the appliance. The problem is more severe with legacy software written in C or other unsafe languages (in contrast with type-safe languages such as Java). Extensible routers are an example of network appliances where custom software processing has to be quickly added and safety is paramount. Diagnostic code and custom (e.g. multimedia) schedulers are also examples of extensions that appliances will need to support.Resource management is especially important for appliances. New operating system abstractions such as paths [Mosberger96], resource containers [Banga99], reservation domains [Bruno98] and activities [Jones95] have been proposed for resource management and control. We believe that an appliance operating system should be flexible enough to support such abstractions. It should also be able to overlay resource management in modular operating systems without such support.Pebble [Gabber99] is a component-based operating system that combines efficient IPC with strong modularity and safety features. Pebble provides the necessary infrastructure for building fine-grained, modular operating systems for appliances out of reusable components. We found that a typical network appliance application (e.g. a Web server) built on Pebble using components has comparable performance to a traditional monolithic kernel. In this way, there is no performance reason not to use a safer, more modular component structure for such appliances. Moreover, we claim that Pebble is a general purpose operating system framework that allows us to implement diverse OS structures and mechanisms, alleviating the need for writing specialized operating systems from scratch.'],\n",
       "   '2000']],\n",
       " [[['Towards robust OSes for appliances: a new approach based on domain-specific languages.',\n",
       "    'Appliances represent a quickly growing domain that raises new challenges in OS design and development. First, new products appear at a rapid pace to satisfy emerging needs. Second, the nature of these markets makes these needs unpredictable. Lastly, given the competitiveness of such markets, there exists tremendous pressure to deliver new products. In fact, innovation is a requirement in emerging markets to gain commercial success.The embedded nature of appliances makes upgrading and fixing bugs difficult (and sometimes impossible) to achieve. Consequently, there must be a high level of confidence in the software. Additionally, the pace of innovation requires rapid OS development so as to match ever changing needs of new appliances.To offer confidence, software must be highly robust. That is, for a given type of appliance, critical behavioral properties must be determined and guaranteed (e.g., power management must ensure that data are not lost). Robustness can be provided by mechanisms and/or tools. The ideal approach takes the form of certification tools aimed at statically verifying critical properties. Such tools avoid the need for a laborious and error-prone testing process.To be first in a market requires not only that the testing process be shortened, but the development time as well. To achieve this goal, three strategies are needed: re-use of code to rapidly produce a new product by assembling existing building blocks, factorization of expertise to capitalize on domain-specific experience, and open-endedness of software systems to match evolving functionalities and hardware features.In this paper, existing OS approaches are assessed with respect to the requirements raised by appliances. The limitations of these approaches are analyzed and used as a basis to propose a new approach to designing and structuring OSes for appliances. This approach is based on Domain-Specific Languages (DSLs), and offers rapid development of robust OSes. We illustrate and assess our approach by concrete examples.'],\n",
       "   '2000']],\n",
       " [[['Congestion prices as feedback signals: an approach to QoS management.',\n",
       "    'Recently there has been a renewed interest in the application of economic models to the management of computational resources. Most of this interest is focused on pricing models for the Internet; in particular, on congestion or shadow prices, that address the phenomenon of what economists call external costs --- users are exposed to the costs they impose on other users when causing congestion of a resource.This paper describes how congestion prices can be applied to resource management in operating systems. Shadow prices are interpreted as feedback signals to applications which can adjust their resource requirements according to an application-specific strategy. This leads to a decentralised approach of resource management where applications are enabled and encouraged to perform resource and quality tradeoffs themselves. We have implemented a simulation environment and a number of strategies to evaluate the usefulness of congestion prices as a feedback signal and demonstrate that this approach can offer different service levels to different tasks. We also discuss how the simulation results can be applied in a real operating system and how this work can be extended to form a generic resource management framework.'],\n",
       "   '2000']],\n",
       " [[['Deferring trust in fluid replication.',\n",
       "    'Mobile nodes rely on external services to provide safety, sharing, and additional resources. Unfortunately, as mobile nodes move through the networking infrastructure, the costs of accessing servers change. Fluid replication allows mobile clients to create replicas where and when they are needed. Unfortunately, one must trust the nodes holding these replicas, and establishing trust in autonomously administered nodes is a difficult task. Instead, we argue that trust should be deferred. In this position paper, we present the design of Stonewall, a system that defers trust decisions through the use of two mechanisms: packages and receipts. The former ensure confidentiality and detect breaches of integrity; the latter detect breaches of non-repudiation.'],\n",
       "   '2000']],\n",
       " [[['Every joule is precious: the case for revisiting operating system design for energy efficiency.',\n",
       "    'By some estimates, there will be close to one billion wireless devices capable of Internet connectivity within five years, surpassing the installed base of traditional wired compute devices. These devices will take the form of cellular phones, personal digital assistants (PDA\\'s), embedded processors, and \"Internet appliances\". This proliferation of networked computing devices will enable a number of compelling applications, centering around ubiquitous access to global information services, just in time delivery of personalized content, and tight synchronization among compute devices/appliances in our everyday environment. However, one of the principal challenges of realizing this vision in the post-PC environment is the need to reduce the energy consumed in using these next-generation mobile and wireless devices, thereby extending the lifetime of the batteries that power them. While the processing power, memory, and network bandwidth of post-PC devices are increasing exponentially, their battery capacity is improving at a more modest pace. Thus, to ensure the utility of post-PC applications, it is important to develop low-level mechanisms and higher-level policies to maximize energy efficiency. In this paper, we propose the systematic re-examination of all aspects of operating system design and implementation from the point of view of energy efficiency rather than the more traditional OS metric of maximizing performance. In [7], we made the case for energy as a first-class OS-managed resource. We emphasized the benefits of higher-level control over energy usage policy and the application/OS interactions required to achieve them. This paper explores the implications that this major shift in focus can have upon the services, policies, mechanisms, and internal structure of the OS itself based on our initial experiences with rethinking system design for energy efficiency. Our ultimate goal is to design an operating system where major components cooperate to explicitly optimize for energy efficiency. A number of research efforts have recently investigated aspects of energy-efficient operating systems (a good overview is available at [16, 20]) and we intend to leverage existing \"best practice\" in our own work where such results exist. However, we are not aware of any systems that systematically revisit system structure with energy in mind. Further, our examination of operating system functionality reveals a number of opportunities that have received little attention in the literature. To illustrate this point, Table 1 presents major operating system functionality, along with possible techniques for improving power consumption characteristics. Several of the techniques are well studied, such as disk spindown policies or adaptively trading content fidelity for power [8]. For example, to reduce power consumption for MPEG playback, the system could adapt to a smaller frame rate and window size, consuming less bandwidth and computation. One of the primary objectives of operating systems is allocating resources among competing tasks, typically for fairness and performance. Adding energy efficiency to the equation raises a number of interesting issues. For example, competing processes/users may be scheduled to receive a fair share of battery resources rather than CPU resources (e.g., an application that makes heavy use of DISK I/O may be given lower priority relative to a compute-bound application when energy resources are low). Similarly, for tasks such as ad hoc routing, local battery resources are often consumed on behalf of remote processes. Fair allocation dictates that one battery is not drained in preference to others. Finally, for the communication subsystem, a number of efforts already investigate adaptively setting the polling rate for wireless networks (trading latency for energy). Our efforts to date have focused on the last four areas highlighted in Table 1. For memory allocation, our work explores how to exploit the ability of memory chips to transition among multiple power states. We also investigate metrics for picking energy-efficient routes in ad hoc networks, energy-efficient placement of distributed computation, and flexible RPC/name binding that accounts for power consumption. These last two points of resource allocation and remote communication highlight an interesting property for energy-aware OS design in the post-PC environment. Many tasks are distributed across multiple machines, potentially running on machines with widely varying CPU, memory, and power source characteristics. Thus, energy-aware OS design must closely cooperate with and track the characteristics of remote computers to balance the often conflicting goals of optimizing for energy and speed. The rest of this paper illustrates our approach with selected examples extracted from our recent efforts toward building an integrated hardware/software infrastructure that incorporates cooperative power management to support mobile and wireless applications. The instances we present in subsequent sections cover the resource management policies and mechanisms necessary to exploit low power modes of various (existing or proposed) hardware components, as well as power-aware communications and the essential role of the wide-area environment. We begin our discussion with the resources of a single machine and then extend it to the distributed context.'],\n",
       "   '2000']],\n",
       " [[['Automated systematic testing for constraint-based interactive services.',\n",
       "    'Constraint-based languages can express in a concise way the complex logic of a new generation of interactive services for applications such as banking or stock trading, that must support multiple types of interfaces for accessing the same data. These include automatic speech-recognition interfaces where inputs may be provided in any order by users of the service. We study in this paper how to systematically test event-driven applications developed using such languages. We show how such applications can be tested automatically, without the need for any manually-written test cases, and efficiently, by taking advantage of their capability of taking unordered sets of events as inputs.'],\n",
       "   '2000']],\n",
       " [[['Compiler and tool support for debugging object protocols.',\n",
       "    \"We describe an extension to the Java programming language that supports static conformance checking and dynamic debugging of object &ldquo;protocols,&rdquo; i.e., sequencing constraints on the order in which methods may be called. Our Java protocols have a statically checkable subset embedded in richer descriptions that can be checked at run time. The statically checkable subtype conformance relation is based on Nierstrasz' proposal for regular (finite-state) process types, and is also very close to the conformance relation for architectural connectors in the Wright architectural description language by Allen and Garlan. Richer sequencing properties, which cannot be expressed by regular types alone, can be specified and checked at run time by associating predicates with object states. We describe the language extensions and their rationale, and the design of tool support for static and dynamic checking and debugging.\"],\n",
       "   '2000']],\n",
       " [[['Composing features and resolving interactions.',\n",
       "    \"One of the accepted techniques for developing and maintaining feature-rich applications is to treat each feature as a separate concern. However, most features are not separate concerns because they override and extend the same basic service. That is, &ldquo;independent&rdquo; features are coupled to one another through the system's basic service. As a result, seemingly unrelated features subtly interfere with each other when trying to override the system behaviour in different directions. The problem is how to coordinate features' access to the service's shared variables. This paper proposes coordinating features via feature composition. We model each feature as a separate labelled-transition system and define a 1conflict-free (CF) composition operator that prevents enabled transitions from synchronizing if they interact: if several features' transitions are simultaneously enabled but have conflicting actions, a non-conflicting subset of the enabled transitions are synchronized in the composition. We also define a conflict- and violation-free (CVF) composition operator that prevents enabled transitions from executing if they violate features' invariants. Both composition operators use priorities among features to decide whether to synchronize transitions.\"],\n",
       "   '2000']],\n",
       " [[['Coven: brewing better collaboration through software configuration management.',\n",
       "    'Our work focuses on building tools to support collaborative software development. We are building a new programming environment with integrated software configuration management which provides a variety of features to help programming teams coordinate their work. In this paper, we detail a hierachy-based software configuration management system called Coven, which acts as a collaborative medium for allowing teams of programmers to cooperate. By providing a family of inter-related mechanisms, our system provides powerful support for cooperation and coordination in a manner which matches the structure of development teams.'],\n",
       "   '2000']],\n",
       " [[['Automating first-order relational logic.',\n",
       "    'An automatic analysis method for first-order logic with sets and relations is described. A first-order formula is translated to a quantifier-free boolean formula, which has a model when the original formula has a model within a given scope (that is, involving no more than some finite number of atoms). Because the satisfiable formulas that occur in practice tend to have small models, a small scope usually suffices and the analysis is efficient. The paper presents a simple logic and gives a compositional translation scheme. It also reports briefly on experience using the Alloy Analyzer, a tool that implements the scheme.'],\n",
       "   '2000']],\n",
       " [[['COM revisited: tool-assisted modelling of an architectural framework.',\n",
       "    \"Designing architectural frameworks without the aid of formal modeling is error prone. But, unless supported by analysis, formal modeling is prone to its own class of errors, in which formal statements fail to match the designer's intent. A fully automatic analysis tool can rapidly expose such errors, and can make the process of constructing and refining a formal model more effective. This paper describes a case study in which we recast a model of Microsoft COM's query interface and aggregation mechanism into Alloy, a lightweight notation for describing structures. We used Alloy's analyzer to simulate the specification, to check properties and to evaluate changes. This allowed us to manipulate our model more quickly and with far greater confidence than would otherwise have been possible, resulting in a much simpler model and a better understanding of its key properties.\"],\n",
       "   '2000']],\n",
       " [[['A compositional approach to statecharts semantics.',\n",
       "    \"Statecharts is a visual language for specifying reactive system behavior. The formalism extends traditional finite-state machines with notions of hierarchy and concurrency, and it is used in many popular software design notations. A large part of the appeal of Statecharts derives from its basis in state machines, with their intuitive operational interpretation. The classical semantics of Statecharts, however, suffers from a serious defect; it is not compositional, meaning that the behavior of system descriptions cannot be inferred from the behavior of their subsystems. Compositionality is a prerequisite for exploiting the modular structure of Statecharts for simulation, verification, and code generation, and it also provides the necessary foundation for reusability. This paper suggests a new compositional approach to formalizing Statecharts semantics as flattened labeled transition systems in which transitions represent system steps. The approach builds on ideas developed for timed process calculi and employs structural operational rules to define the transitions of a Statecharts expression in terms of the transitions of its subexpressions. It is first presented for a simple dialect of Statecharts, with respect to a variant of Pnueli and Shalev's semantics, and is illustrated by means of a small example. To demonstrate its flexibility, the proposed approach is then extended to deal with practically useful features available in many Statecharts variants, namely state references, history states, and priority concepts along state hierarchies.\"],\n",
       "   '2000']],\n",
       " [[['Automated test oracles for GUIs.',\n",
       "    \"Graphical User Interfaces (GUIs) are critical components of today's software. Because GUIs have different characteristics than traditional software, conventional testing techniques do not apply to GUI software. In previous work, we presented an approach to generate GUI test cases, which take the form of sequences of actions. In this paper we develop a test oracle technique to determine if a GUI behaves as expected for a given test case. Our oracle uses a formal model of a GUI, expressed as sets of objects, object properties, and actions. Given the formal model and a test case, our oracle automatically derives the expected state for every action in the test case. We represent the actual state of an executing GUI in terms of objects and their properties derived from the GUI's execution. Using the actual state acquired from an execution monitor, our oracle automatically compares the expected and actual states after each action to verify the correctness of the GUI for the test case. We implemented the oracle as a component in our GUI testing system, called Planning Assisted Tester for grapHical user interface Systems (PATHS), which is based on AI planning. We experimentally evaluated the practicality and effectiveness of our oracle technique and report on the results of experiments to test and verify the behavior of our version of the Microsoft WordPad's GUI.\"],\n",
       "   '2000']],\n",
       " [[['Classifying properties: an alternative to the safety-liveness classification.',\n",
       "    'Traditionally, verification properties have been classified as safety or liveness properties. While this taxonomy has an attractive simplicity and is useful for identifying the appropriate analysis algorithm for checking a property, determining whether a property is safety, liveness, or neither can require significant mathematical insight on the part of the analyst. In this paper, we present an alternative property taxonomy. We argue that this taxonomy is a more natural classification of the kinds of questions that analysts want to ask. Moreover, most classes in our taxonomy have a known, direct mapping to the safety-liveness classification, and thus the appropriate analysis algorithm can be automatically determined.'],\n",
       "   '2000']],\n",
       " [[['Extracting library-based object-oriented applications.',\n",
       "    'In an increasingly popular model of software distribution, software is developed in one computing environment and deployed in other environments by transfer over the internet. Extraction tools perform a static whole-program analysis to determine unused functionality in applications in order to reduce the time required to download applications. We have identified a number of scenarios where extraction tools require information beyond what can be inferred through static analysis: software distributions other than complete applications, the use of reflection, and situations where an application uses separately developed class libraries. This paper explores these issues, and introduces a modular specification language for expressing the information required for extraction. We implemented this language in the context of Jax, an industrial-strength application extractor for Java, and present a small case study in which different extraction scenarios are applied to a commercially available library-based application.'],\n",
       "   '2000']],\n",
       " [[['Implicit context: easing software evolution and reuse.',\n",
       "    'Software systems should consist of simple, conceptually clean software components interacting along narrow, well-defined paths. All too often, this is not reality: complex components end up interacting for reasons unrelated to the functionality they provide. We refer to knowledge within a component that is not conceptually required for the individual behaviour of that component as extraneous embedded knowledge (EEK). EEK creeps into a system in many forms, including dependences upon particular names and the passing of extraneous parameters. This paper proposes the use of implicit context as a means for reducing EEK in systems by combining a mechanism to reflect upon what has happened in a system, through queries on the call history, with a mechanism for altering calls to and from a component. We demonstrate the benefits of implicit context by describing its use to reduce EEK in the Java&trade; Swing library.'],\n",
       "   '2000']],\n",
       " [[['Integrating active information delivery and reuse repository systems.',\n",
       "    \"Although software reuse can improve both the quality and productivity of software development, it will not do so until software developers stop believing that it is not worth their effort to find a component matching their current problem. In addition, if the developers do not anticipate the existence of a given component, they will not even make an effort to find it in the first place. Even the most sophisticated and powerful reuse repositories will not be effective if developers don't anticipate a certain component exists, or don't deem it worthwhile to seek for it. We argue that this crucial barrier to reuse is overcome by integrating active information delivery, which presents information without explicit queries from the user, and reuse repository systems. A prototype system, CodeBroker, illustrates this integration and raises several issues related to software reuse.\"],\n",
       "   '2000']],\n",
       " [[['Consensus in byzantine asynchronous systems.',\n",
       "    'This paper presents a consensus protocol resilient to Byzantine failures. It uses signed and certified messages and is based on two underlying failure detection modules. The first is a muteness failure detection module of the class ♦M. The second is a reliable Byzantine behaviour detection module. More precisely, the first module detects processes that stop sending messages, while processes experiencing other non-correct behaviours (i.e., Byzantine) are detected by the second module. The protocol is resilient to F faulty processes, F ≤ min(⌊(n - 1)/2⌋, C) (where C is the maximum number of faulty processes that can be tolerated by the underlying certification service).The approach used to design the protocol is new. While usual Byzantine consensus protocols are based on failure detectors to detect processes that stop communicating, none of them use a module to detect their Byzantine behaviour (this detection is not isolated from the protocol and makes it difficult to understand and prove correct). In addition to this modular approach and to a consensus protocol for Byzantine systems, the paper presents a finite state automaton-based implementation of the Byzantine behaviour detection module. Finally, the modular approach followed in this paper can be used to solve other problems in asynchronous systems experiencing Byzantine failures.'],\n",
       "   '2000']],\n",
       " [[['Self-stabilization with path algebra.',\n",
       "    'Self-stabilizing protocols can resist transient failures and guarantee system recovery in a finite time. We highlight the connexion between the formalism of self-stabilizing distributed systems and the formalism of generalized path algebra and asynchronous iterations with delay. We use the later to prove that a local condition on locally executed algorithm (being a strictly idempotent r-operator) ensures self-stabilization of the global system. As a result, a parametrized distributed algorithm applicable to any directed graph topology is proposed, and the function parameter of our algorithm is instantiated to produce distributed algorithms for both fundamental and high-level applications. Due to fault resilience properties of our algorithm, the resulting protocols are self-stabilizing at no additional cost.'],\n",
       "   '2000']],\n",
       " [[['Cooperative computing with fragmentable and mergeable groups.',\n",
       "    'This work considers the problem of performing a set of N tasks on a set of P cooperating message-passing processors (P ≤ N). The processors use a group communication service (GCS) to coordinate their activity in the setting where dynamic changes in the underlying network topology cause the processor groups to change over time. GCSs have been recognized as effective building blocks for fault-tolerant applications in such settings. Our results explore the efficiency of fault-tolerant cooperative computation using GCSs. The original investigation of this area by (Dolev et al., Dynamic load balancing with group communication, in: Proc. of the 6th International Colloquium on Structural Information and Communication Complexity, 1999) focused on competitive lower bounds, non-redundant task allocation schemes and work-efficient algorithms in the presence of fragmentation regroupings. In this work we investigate work-efficient and message-efficient algorithms for fragmentation and merge regroupings. We present an algorithm that uses GCSs and implements a coordinator-based strategy. For the analysis of our algorithm we introduce the notion of view-graphs that represent the partially-ordered view evolution history witnessed by the processors. For fragmentations and merges, the work of the algorithm (defined as the worst case total number of task executions counting multiplicities) is not more than min{N ċ f + N, N ċ P}, and the message complexity is no worse than 4(N ċ f + N + P ċ m), where f and m denote the number of new groups created by fragmentations and merges, respectively. Note that the constants are very small and that, interestingly, while the work efficiency depends on the number of groups f created as the result of fragmentations, work does not depend on the number of groups m created as the result of merges.'],\n",
       "   '2000']],\n",
       " [[['VideoPlus: A Method for Capturing the Structure and Appearance of Immersive Environments.',\n",
       "    'This paper presents a simple approach to capturing the appearance and structure of immersive scenes based on the imagery acquired with an omnidirectional video camera. The scheme proceeds by combining techniques from structure from motion with ideas from image based rendering. An interactive photogrammetric modeling scheme is used to recover the locations of a set of salient features in the scene (points and lines) from image measurements in a small set of keyframe images. The estimates obtained from this process are then used as a basis for estimating the position and orientation of the camera at every frame in the video clip. By augmenting the video sequence with pose information we provide the end user with the ability to index the video sequence spatially as opposed to temporally. This allows the user to explore the immersive scene by interactively selecting the desired viewpoint and viewing direction.'],\n",
       "   '2000']],\n",
       " [[['Approximation algorithms for projective clustering.',\n",
       "    'We consider the following two instances of the projective clustering problem: Given a set S of n points in Rd and an integer k > 0, cover S by k slabs (respectively d-cylinders) so that the maximum width of a slab (respectively the maximum diameter of a d-cylinder) is minimized. Let w* be the smallest value so that S can be covered by k slabs (respectively d-cylinders), each of width (respectively diameter) at most w*. This paper contains three main results: (i) For d = 2, we present a randomized algorithm that computes O(k log k) strips of width at most w* that cover S. Its expected running time is O(nk2log4n) if k2 log k ≤ n; for larger values of k, the expected running time is O(n2/3k8/3log14/3n). (ii) For d = 3, a cover of S by O(k log k) slabs of width at most w* can be computed in expected time O(n3/2k9/4polygon(n)).(iii) We compute a cover of S ⊂ Rd by O(dk log k) d-cylinders of diameter at most 8w* in expected time O(dnk3 log4 n). We also present a few extensions of this result.'],\n",
       "   '2000']],\n",
       " [[['Faster algorithms for string matching with mismatches.',\n",
       "    'The string matching with mismatches problem is that of finding the number of mismatches between a pattern P of length m and every length m substring of the text T. Currently, the fastest algorithms for this problem are the following. The Galil-Giancarlo algorithm finds all locations where the pattern has at most k errors (where k is part of the input) in time O(nk). The Abrahamson algorithm finds the number of mismatches at every location in time O(n√ m log m). We present an algorithm that is faster than both. Our algorithm finds all locations where the pattern has at most k errors in time O(n√k log k). We also show an algorithm that solves the above problem in time O((n + (nk3)/m) log k).'],\n",
       "   '2000']],\n",
       " [[['Inplace run-length 2d compressed search.',\n",
       "    'The recent explosion in the amount of stored data has necessitated the storage and transmission of data in compressed form. The need to quickly access this data has given rise to a new paradigm in searching, that of compressed matching (Proc. Data Compression Conf, Snow Bird, UT, 1992, pp. 279-288; Proc. 8th Annu. Symp. on Combinatorial Pattern Matching (CPM 97), Lecture Notes in Computer Science, Vol. 1264, Springer, Berlin, 1997, pp. 40-51; Proc. 7th Annu. Symp. on Combinatorial Pattern Matching (CPM 96), Lecture Notes in Computer Science, Vol. 1075, Springer, Berlin, 1996, pp. 39-49). The goal of the compressed pattern matching problem is to find a pattern in a text without decompressing the text.The criterion of extra space is very relevant to compressed searching. An algorithm is called inplace if the amount of extra space used is proportional to the input size of the pattern. In this paper we present a 2d compressed matching algorithm that is inplace. Let compressed(T) and compressed(P) denote the compressed text and pattern, respectively. The algorithm presented in this paper runs in time O(|compressed(T)| + |P|log σ) where σ is min(|P|,|Σ), and Σ is the alphabet, for all patterns that have no trivial rows (rows consisting of a single repeating symbol). The amount of space used is O(|compressed(P)|). The compression used is the 2d run-length compression, used in FAX transmission.'],\n",
       "   '2000']],\n",
       " [[['Instability of FIFO in session-oriented networks.',\n",
       "    'We show that the First-In-First-Out (FIFO) scheduling discipline can be unstable in the (σ,ρ) regulated session model for packet-switched networks. In this model packets are injected into the network in fixed sessions. The total size of the session-i packets injected during the time interval [x, y) is at most σi + ρi(y - x) for some burst parameter σi and rate ρi. The sum of the rates of sessions passing through a server is at most the server speed.Previous work on FIFO stability either allowed for dynamically changing session paths or else assumed that session-i packets are injected at a constant rate. Our result shows that FIFO can be unstable for static paths as long as the injections into a session can be temporarily suspended.'],\n",
       "   '2000']],\n",
       " [[['Improved approximation algorithms for MAX SAT.',\n",
       "    \"MAX SAT (the maximum satisfiability problem) is stated as follows: given a set of clauses with weights, find a truth assignment that maximizes the sum of the weights of the satisfied clauses. In this paper, we consider approximation algorithms for MAX SAT proposed by Goemans and Williamson and present a sharpened analysis of their performance guarantees. We also show that these algorithms, combined with recent approximation algorithms for MAX 2SAT, MAX 3SAT, and MAX SAT due to Feige and Goemans, Karloff and Zwick, and Zwick, respectively, lead to an improved approximation algorithm for MAX SAT. By using the MAX 2SAT and 3SAT algorithms, we obtain a performance guarantee of 0.7846, and by using Zwick's algorithm, we obtain a performance guarantee of 0.8331, which improves upon the performance guarantee of 0.7977 based on Zwick's conjecture. The best previous result for MAX SAT without assuming Zwick's conjecture is a 0.770-approximation algorithm of Asano. Our best algorithm requires a new family of 3/4-approximation algorithms that generalize a previous algorithm of Goemans and Williamson.\"],\n",
       "   '2000']],\n",
       " [[['Computing contour trees in all dimensions.',\n",
       "    'We show that contour trees can be computed in all dimensions by a simple algorithm that merges two trees. Our algorithm extends, simplifies, and improves work of Tarasov and Vyalyi and of van Kreveld et al.'],\n",
       "   '2000']],\n",
       " [[['Approximation algorithms for data placement on parallel disks.',\n",
       "    'We study an optimization problem that arises in the context of data placement in a multimedia storage system. We are given a collection of M multimedia objects (data objects) that need to be assigned to a storage system consisting of N disks d1,d2&hellip;,dN. We are also given sets U1,U2,&hellip;,UM such that Ui is the set of clients seeking the ith data object. Each disk dj is characterized by two parameters, namely, its storage capacity Cj which indicates the maximum number of data objects that may be assigned to it, and a load capacity Lj which indicates the maximum number of clients that it can serve. The goal is to find a placement of data objects to disks and an assignment of clients to disks so as to maximize the total number of clients served, subject to the capacity constraints of the storage system. We study this data placement problem for two natural classes of storage systems, namely, homogeneous and uniform ratio. We show that an algorithm developed by Shachnai and Tamir [2000a] for data placement achieves the best possible absolute bound regarding the number of clients that can always be satisfied. We also show how to implement the algorithm so that it has a running time of O((N + M) log(N + M)). In addition, we design a polynomial-time approximation scheme, solving an open problem posed in the same paper.'],\n",
       "   '2000']],\n",
       " [[['The data locality of work stealing.',\n",
       "    'This paper studies the data locality of the work-stealing scheduling algorithm on hardware-controlled shared-memory machines. We present lower and upper bounds on the number of cache misses using work stealing, and introduce a locality-guided work-stealing algorithm along with experimental validation. As a lower bound, we show that there is a family of multi-threaded computations Gn each member of which requires &THgr;(n) total instructions (work) for which when using work-stealing the number of cache misses on one processor is constant, while even on two processors the total number of cache misses is &OHgr;(n). This implies that for general computations there is no useful bound relating multiprocessor to uninprocessor cache misses. For nested-parallel computations, however, we show that on P processors the expected additional number of cache misses beyond those on a single processor is bounded by O(C&lceil;m/s PT&infin;), where m is the execution time of an instruction incurring a cache miss, s is the steal time, C is the size of cache, and T&infin; is the number of nodes on the longest chain of dependences. Based on this we give strong bounds on the total running time of nested-parallel computations using work stealing. For the second part of our results, we present a locality-guided work stealing algorithm that improves the data locality of multi-threaded computations by allowing a thread to have an affinity for a processor. Our initial experiments on iterative data-parallel applications show that the algorithm matches the performance of static-partitioning under traditional work loads but improves the performance up to 50% over static partitioning under multiprogrammed work loads. Furthermore, the locality-guided work stealing improves the performance of work-stealing up to 80%.'],\n",
       "   '2000']],\n",
       " [[['DCAS-based concurrent deques.',\n",
       "    \"The computer industry is currently examining the use of strong synchronization operations such as double compare-and-swap (DCAS) as a means of supporting non-blocking synchronization on tomorrow's multiprocessor machines. However, before such a strong primitive will be incorporated into hardware design, its utility needs to be proven by developing a body of effective non-blocking data structures using DCAS. As part of this effort, we present two new linearizable non-blocking implementations of concurrent deques using the DCAS operation. The first uses an array representation, and improves on former algorithms by allowing uninterrupted concurrent access to both ends of the deque while correctly handling the difficult boundary cases when the deque is empty or full. The second uses a linked-list representation, and is the first non-blocking unbounded-memory deque implementation. It too allows uninterrupted concurrent access to both ends of the deque.\"],\n",
       "   '2000']],\n",
       " [[['Scheduling Cilk multithreaded parallel programs on processors of different speeds.',\n",
       "    'We study the problem of executing parallel programs, in particular Cilk programs, on a collection of processors of different speeds. We consider a model in which each processor maintains an estimate of its own speed, where communication between processors has a cost, and where all scheduling must be online. This problem has been considered previously in the fields of asynchronous parallel computing and scheduling theory. Our model is a bridge between the assumptions in these fields. We provide a new more accurate analysis of of an old scheduling algorithm called the maximum utilization scheduler. Based on this analysis, we generalize this scheduling policy and define the high utilization scheduler. We next focus on the Cilck platform and introduce a new algorithm for scheduling Cilk multithreaded parallel programs on heterogeneous processors. This scheduler is inspired by the high utilization scheduler and is modified to fit in a Cilk context. A crucial aspect of our algorithm is that it keeps the original spirit of the Cilk scheduler. In fact, when our new algorithm runs on homogeneous processors, it exactly mimics the dynamics of the original Cilk scheduler.'],\n",
       "   '2000']],\n",
       " [[['Infinite parallel job allocation (extended abstract).',\n",
       "    'In recent years, the task of allocating jobs to servers has been studied with the &ldquo;balls and bins&rdquo; abstraction. Results in this area exploit the large decrease in maximum load that can be achieved by allowing each job (ball) a little freedom in choosing its destination server (bin).In this paper we examine an infinite and parallel allocation process (see [ABS98]) which is related to the &ldquo;balls and bins&rdquo; abstraction. The simple process can be used to model many problems arising in applications like load balancing, data accesses for parallel data servers, hashing, and PRAM simulations.Unfortunately, the parallel allocation process behaves in a highly non-uniform manner which makes its analysis challenging. Even the typically simple question of for which arrival rates the process is stable, is highly non-trivial. In order to cope with this non-uniform behavior we introduce a new sequential process and show (via simulations) that the sequential process models the behavior of the parallel one very accurately. We develop a system of ordinary differential equations in order to describe the behavior of our sequential process and present a thorough analysis of the performance this process. For example, we show that the queue length distribution decreases double-exponentially. Finally, we present simulation results indicating that the solutions to the differential equations very well predict the queue length distribution of our sequential process and the largest injection rate for which it is stable. Summarizing, we can conclude that in all the performance characteristics we have measured experimentally, the parallel and the sequential process are closely related. This indicates that the obtained solution of the differential equations and the results presented above are applicable to the parallel process, too.'],\n",
       "   '2000']],\n",
       " [[['Efficient, distributed data placement strategies for storage area networks (extended abstract).',\n",
       "    'In the last couple of years a dramatic growth of enterprise data storage capacity can be observed. As a result, new strategies have been sought that allow servers and storage being centralized to better manage the explosion of data and the overall cost of ownership. Nowadays, a common approach is to combine storage devices into a dedicated network that is connected to LANs and/or servers. Such networks are usually called storage area networks (SAN). A very important aspect for these networks is scalability. If a SAN undergoes changes (for instance, due to insertions or removals of disks), it may be necessary to replace data in order to allow an efficient use of the system. To keep the influence of data replacements on the performance of the SAN small, this should be done as efficiently as possible. In this paper, we investigate the problem of evenly distributing and efficiently locating data in dynamically changing SANs. We consider two scenarios: (1) all disks have the same capacity, and (2) the capacities of the disks are allowed to be arbitrary. For both scenarios, we present placement strategies capable of locating blocks efficiently and that are able to quickly adjust the data placement to insertions or removals of disks or data blocks. Furthermore, we study how the performance of our placement strategies changes if we allow to waste a certain amount of capacity of the disks.'],\n",
       "   '2000']],\n",
       " [[['Diffusive load balancing schemes on heterogeneous networks.',\n",
       "    'Up to now, diffusive load balancing schemes have only been developed for homogeneous networks. We generalize existing diffusion schemes, in order to deal with heterogeneous networks. In these networks, every processor can have arbitrary computing power, and the load has to be balanced proportionally to these weights. The balancing flow that is calculated by the schemes for homogeneous networks is minimal with regard to the l2-norm and we prove this to hold true for the generalized schemes, too. By means of a number of experiments we demonstrate the usability of the generalized schemes on heterogeneous networks.'],\n",
       "   '2000']],\n",
       " [[['Broadcast scheduling optimization for heterogeneous cluster systems.',\n",
       "    'Network of workstation (NOW) is a cost-effective alternative to massively parallel supercomputers. As commercially available off-the-shelf processors become cheaper and faster, it is now possible to build a PC or workstation cluster that provides high computing power within a limited budget. However, a cluster may consist of different types of processors and this heterogeneity within a cluster complicates the design of efficient collective communication protocols. This paper shows that a simple heuristic called fastest-node-first (FNF) [3] is very effective in reducing broadcast time for heterogeneous cluster systems. Despite the fact that FNF heuristic fails to give the optimal broadcast time for a general heterogeneous network of workstation, we prove that FNF always gives the optimal broadcast time in several special cases of clusters. Based on these special case results, we show that FNF is an approximation algorithm that guarantees a competitive ratio of 2. From these theoretical results we also derive techniques to speed up the branch-and-bound search for the optimal broadcast schedule in HNOW.'],\n",
       "   '2000']],\n",
       " [[['An experimental study of a simple, distributed edge coloring algorithm.',\n",
       "    'We conduct an experimental analysis of a distributed randomized algorithm for edge coloring simple undirected graphs. The algorithm is extremely simple yet, according to the probabilistic analysis, it computes nearly optimal colorings very quickly [Grable and Panconesi 1997]. We test the algorithm on a number of random as well as nonrandom graph families.The test cases were chosen based on two objectives: (i) to provide insights into the worst-case behavior (in terms of time and quality) of the algorithm and (ii) to test the performance of the algorithm with instances that are likely to arise in practice. Our main results include the following:(1) The empirical results obtained compare very well with the recent empirical results reported by other researchers [Durand et al. 1994, 1998; Jain and Werth 1995].(2) The empirical results confirm the bounds on the running time and the solution quality as claimed in the theoretical paper. Our results show that for certain classes of graphs the algorithm is likely to perform much better than the analysis suggests.(3) The results demonstrate that the algorithm might be well suited (from a theoretical as well as practical standpoint) for edge coloring graphs quickly and efficiently in a distributed setting.Based on our empirical study, we propose a simple modification of the original algorithm with substantially improved performance in practice.'],\n",
       "   '2000']],\n",
       " [[['Algorithmic foundations for a parallel vector access memory system.',\n",
       "    'This paper presents mathematical foundations for the design of a memory controller subcomponent that helps to bridge the processor/memory performance gap for applications with strided access patterns. The Parallel Vector Access (PVA) unit exploits the regularity of vectors or streams to access them efficiently in parallel on a multi-bank SDRAM memory system. The PVA unit performs scatter/gather operations so that only the elements accessed by the application are transmitted across the system bus. Vector operations are broadcast in parallel to all memory banks, each of which implements an efficient algorithm to determine which vector elements it holds. Earlier performance evaluations have demonstrated that our PVA implementation loads elements up to 32.8 times faster than a conventional memory system and 3.3 times faster than a pipelined vector unit, without hurting the performance of normal cache-line fills. Here we present the underlying PVA algorithms for both word interleaved and cache-line inter-leaved memory systems.'],\n",
       "   '2000']],\n",
       " [[['Asynchronous scheduling of redundant disk arrays.',\n",
       "    'Allocation of data to parallel disk using redundant storage and random placement of blocks can be exploited to achieve low access delays. New algorithms are proposed which improve the previously known shortest queue algorithm by systematically exploiting that scheduling decisions can be deferred until a block access is actually started on a disk. These algorithms are also generalized for coding schemes with low redundancy. Using extensive simulations, practically important quantities are measured which have so far eluded an analytical treatment: The delay distribution when a stream of requests approaches the limit of the sytem capacity, the system efficiency for parallel disk applications with bounded prefetching buffers, and the combination of both for mixed traffic. A further step toward practice is taken by outlining the system design for \\\\alpha: automatically load-balanced parallel hard-disk array. Additional algorithmic measures are proposed for \\\\alpha that allow variable sized blocks, seek time reduction, fault tolerance, inhomogeneous systems, and flexible priorization schemes.'],\n",
       "   '2000']],\n",
       " [[['The Complexity of Planarity Testing.',\n",
       "    'We clarify the computational complexity of planarity testing, by showing that planarity testing is hard for L, and lies in SL. This nearly settles the question, since it is widely conjectured that L = SL. The Upper bound of SL matches the lower bound of L in the context of (non-uniform) circuit complexity, since L/poly is equal to SL/poly. Similarly, we show that a planar embedding, when one exists, can be found in FLSL. Previously, these problems were known to reside in the complexity class AC1, via the O(log n) time CRCW-PRAM algorithm of Ramachandran and Reif, although planarity checking for degree-three graphs had been shown to be in SL [Chicago J. Theoret. Comput. Sci. (1995); J. ACM 31(2) (1984) 401].'],\n",
       "   '2000']],\n",
       " [[['Almost Complete Sets.',\n",
       "    \"We show that there is a set that is almost complete but not complete under polynomial-time many-one (p-m) reductions for the class E of sets computable in deterministic time 2lin. Here a set in a complexity class C is almost complete for C under some given reducibility if the class of the problems in C that do not reduce to this set has measure 0 in C in the sense of Lutz's resource-bounded measure theory. We also show that the almost complete sets for E under polynomial time-bounded length-increasing one-one reductions and truth-table reductions of norm 1 coincide with the almost p-m-complete sets for E. Moreover, we obtain similar results for the class EXP of sets computable in deterministic time 2poly.\"],\n",
       "   '2000']],\n",
       " [[['Listing All Potential Maximal Cliques of a Graph.',\n",
       "    'A potential maximal clique of a graph is a vertex set that induces a maximal clique in some minimal triangulation of that graph. It is known that if these objects can be listed in polynomial time for a class of graphs, the treewidth and the minimum fill-in are polynomially tractable for these graphs. We show here that the potential maximal cliques of a graph can be generated in polynomial time in the number of minimal separators of the graph. Thus, the treewidth and the minimum fill-in are polynomially tractable for all classes of graphs with a polynomial number of minimal separators.'],\n",
       "   '2000']],\n",
       " [[['The Weighted 2-Server Problem.',\n",
       "    'We consider a generalization of the 2-server problem in which servers have different costs. We prove that, in uniform spaces, a version of the work function algorithm is 5-competitive, and that no better ratio is possible. We also give a 5-competitive randomized, memoryless algorithm for uniform spaces, and a matching lower bound.For arbitrary metric spaces, in contrast with the non-weighted case, we prove that there is no memoryless randomized algorithm with finite competitive ratio. We also propose a version of the problem in which a request specifies two points to be covered by the servers, and the algorithm must decide which server to move to which point. For this version, we show a 9-competitive algorithm and we prove that no better ratio is possible.'],\n",
       "   '2000']],\n",
       " [[['The Boolean Hierarchy of NP-Partitions.',\n",
       "    'We introduce the boolean hierarchy of k-partitions over NP for k>=3 as a generalization of the boolean hierarchy of sets (i.e., 2-partitions) over NP. Whereas the structure of the latter hierarchy is rather simple the structure of the boolean hierarchy of k-partitions over NP for k>=3 turns out to be much more complicated. We formulate the Embedding Conjecture which enables us to get a complete idea of this structure. This conjecture is supported by several partial results.'],\n",
       "   '2000']],\n",
       " [[['The CNN Problem and Other k-Server Variants.',\n",
       "    'We study several interesting variants of the k-server problem. In the CNN problem, one server services requests in the Euclidean plane. The difference from the k-server problem is that the server does not have to move to a request, but it has only to move to a point that lies in the same horizontal or vertical line with the request. This, for example, models the problem faced by a crew of a certain News Network trying to shoot scenes on the streets of Manhattan from a distance; for any event at an intersection, the crew has only to be on a matching street or avenue. The CNN problem contains as special cases two important problems: the BRIDGE problem, also known as the cow-path problem, and the weighted 2-server problem in which the 2 servers may have different speeds. We show that any deterministic online algorithm has competitive ratio at least 6 + √17. We also show that some successful algorithms for the k-server problem fail to be competitive. In particular, no memoryless randomized algorithm can be competitive.We also consider another variant of the k-server problem, in which servers can move simultaneously, and we wish to minimize the time spent waiting for service. This is equivalent to the regular k-server problem under the L∞ norm for movement costs. We give a ½ k(k + 1) upper bound for the competitive ratio on trees.'],\n",
       "   '2000']],\n",
       " [[['Logics Capturing Local Properties.',\n",
       "    \"Well-known theorems of Hanf and Gaifman establishing locality of first-order definable properties have been used in many applications. These theorems were recently generalized to other logics, which led to new applications in descriptive complexity and database theory. However, a logical characterization of local properties that correspond to Hanf's and Gaifman's theorems is still lacking. Such a characterization only exists for structures of bounded valence. In this paper, we give logical characterizations of local properties behind Hanf's and Gaifman's theorems. We first deal with an infinitary logic with counting terms and quantifiers that is known to capture Hanf-locality on structures of bounded valence. We show that testing isomorphism of neighborhoods can be added to it without violating Hanf-locality, while increasing its expressive power. We then show that adding local second-order quantification to it caputures precisely all Hanf-local properties. To capture Gaifman-locality, one must also add a (potentially infinite) case statement. We further show that the hierarchy based on the number of variants in the case statement is strict.\"],\n",
       "   '2000']],\n",
       " [[['Quantum lower bounds by quantum arguments.',\n",
       "    'We propose a new method for proving lower bounds on quantum query algorithms. Instead of a classical adversary that runs the algorithm with on input and then modifies the input, we use a quantum adversary that runs the algorithm with a superposition of inputs. If the algorithm works correctly, its state becomes entangled with the superposition over inputs. We bound the number of queries needed to achieve a sufficient entanglement and this implies a lower bound on the number of queries for the computation. Using this method, we prove two new Ω(√N) lower bounds on computing AND of ORs and inverting a permutation and also provide more uniform proofs for several known lower bounds which have been previously proven via a variety of different techniques.'],\n",
       "   '2000']],\n",
       " [[['A unified approach to approximating resource allocation and scheduling.',\n",
       "    'We present a general framework for solving resource allocation and scheduling problems. Given a resource of fixed size, we present algorithms that approximate the maximum throughput or the minimum loss by a constant factor. Our approximation factors apply to many problems, among which are: (i) real-time scheduling of jobs on parallel machines, (ii) bandwidth allocation for sessions between two endpoints, (iii) general caching, (iv) dynamic storage allocation, and (v) bandwidth allocation on optical line and ring topologies. For some of these problems we provide the first constant factor approximation algorithm. Our algorithms are simple and efficient and are based on the local-ratio technique. We note that they can equivalently be interpreted within the primal-dual schema.'],\n",
       "   '2000']],\n",
       " [[['Noise-tolerant learning, the parity problem, and the statistical query model.',\n",
       "    'We describe a slightly subexponential time algorithm for learning parity functions in the presence of random classification noise, a problem closely related to several cryptographic and coding problems. Our algorithm runs in polynomial time for the case of parity functions that depend on only the first O(log n log log n) bits of input, which provides the first known instance of an efficient noise-tolerant algorithm for a concept class that is not learnable in the Statistical Query model of Kearns [1998]. Thus, we demonstrate that the set of problems learnable in the statistical query model is a strict subset of those problems learnable in the presence of noise in the PAC model.In coding-theory terms, what we give is a poly(n)-time algorithm for decoding linear k &times; n codes in the presence of random noise for the case of k = c log n log log n for some c > 0. (The case of k = O(log n) is trivial since one can just individually check each of the 2k possible messages and choose the one that yields the closest codeword.)A natural extension of the statistical query model is to allow queries about statistical properties that involve t-tuples of examples, as opposed to just single examples. The second result of this article is to show that any class of functions learnable (strongly or weakly) with t-wise queries for t = O(log n) is also weakly learnable with standard unary queries. Hence, this natural extension to the statistical query model does not increase the set of weakly learnable functions.'],\n",
       "   '2000']],\n",
       " [[['Finding smooth integers in short intervals using CRT decoding.',\n",
       "    'We present a new algorithm for CRT list decoding. An instance of the, CRT list decoding problem consists of integers B, 〈p1, ..., pn〉 and 〈r1, ..., rn〉, where p1 < p2 < ... < pn is a sequence of relatively prime integers. The CRT list decoding problem is to find all positive integers x < B such that x = ri mod pi for all but e values of i ∈ {1, ..., n}. Suppose B = Πi=1r pi for some integer k. Goldreich, Ron, and Sudan (in \"Proc. of STOC\\'99\", pp. 225-234, 1999) recently gave several applications for this problem and presented the first efficient algorithm that works whenever e (approximately) satisfies e < n - √2kn log pn/log p1. Our new algorithm achieves the stronger bound e < n - √kn log pn/log p1 (approximately). The improvement is significant when k is relatively close to n, e.g. k > n/3. The bounds we obtain are similar to the bounds obtained by Guruswami and Sudan for Reed-Solomon list decoding. Hence, our algorithm reduces the gap between CRT list decoding and list decoding of Reed-Solomon codes. In addition, we give a new application for CRT list decoding: finding smooth integers in short intervals. Problems of this type come up in several algorithms for factoring large integers. We define and solve a generalized CRT list decoding problem and discuss how it might be used within the quadratic sieve factoring method.'],\n",
       "   '2000']],\n",
       " [[['Are bitvectors optimal?',\n",
       "    'We study the it static membership problem: Given a set S of at most n keys drawn from a universe U of size m, store it so that queries of the form \"Is u in S?\" can be answered by making few accesses to the memory. We study schemes for this problem that use space close to the information theoretic lower bound of $\\\\Omega(n\\\\log(\\\\frac{m}{n}))$ bits and yet answer queries by reading a small number of bits of the memory.We show that, for $\\\\epsilon > 0$, there is a scheme that stores $O(\\\\frac{n}{\\\\epsilon^2}\\\\log m)$ bits and answers membership queries using a randomized algorithm that reads just one bit of memory and errs with probability at most $\\\\epsilon$. We consider schemes that make no error for queries in S but are allowed to err with probability at most $\\\\epsilon$ for queries not in S. We show that there exist such schemes that store $O((\\\\frac{n}{\\\\epsilon})^2 \\\\log m)$ bits and answer queries using just one bitprobe. If multiple probes are allowed, then the number of bits stored can be reduced to $O(n^{1+\\\\delta}\\\\log m)$ for any $\\\\delta > 0$. The schemes mentioned above are based on probabilistic constructions of set systems with small intersections.We show lower bounds that come close to our upper bounds (for a large range of n and $\\\\epsilon$): Schemes that answer queries with just one bitprobe and error probability $\\\\epsilon$ must use $\\\\Omega(\\\\frac{n}{\\\\epsilon\\\\log(1/\\\\epsilon)} \\\\log m)$ bits of storage; if the error is restricted to queries not in S, then the scheme must use $\\\\Omega(\\\\frac{n^2}{\\\\epsilon^2 \\\\log (n/\\\\epsilon)}\\\\log m)$ bits of storage. We also consider deterministic schemes for the static membership problem and show tradeoffs between space and the number of probes.'],\n",
       "   '2000']],\n",
       " [[['Faster suffix tree construction with missing suffix links.',\n",
       "    \"We consider suffix tree construction for situations with missing suffix links. Two examples of such situations are suffix trees for parameterized strings and suffix trees for two-dimensional arrays. These trees also have the property that the node degrees may be large. We add a new back-propagation component to McCreight's algorithm and also give a high probability hashing scheme for large degrees. We show that these two features enable construction of suffix trees for general situations with missing suffix links in O(n) time, with high probability. This gives the first randomized linear time algorithm for constructing suffix trees for parameterized strings.\"],\n",
       "   '2000']],\n",
       " [[['On the sum-of-squares algorithm for bin packing.',\n",
       "    'In this article we present a theoretical analysis of the online Sum-of-Squares algorithm (SS) for bin packing along with several new variants. SS is applicable to any instance of bin packing in which the bin capacity B and item sizes s(a) are integral (or can be scaled to be so), and runs in time O(nB). It performs remarkably well from an average case point of view: For any discrete distribution in which the optimal expected waste is sublinear, SS also has sublinear expected waste. For any discrete distribution where the optimal expected waste is bounded, SS has expected waste at most O(log n). We also discuss several interesting variants on SS, including a randomized O(nB log B)-time online algorithm SS&ast; whose expected behavior is essentially optimal for all discrete distributions. Algorithm SS&ast; depends on a new linear-programming-based pseudopolynomial-time algorithm for solving the NP-hard problem of determining, given a discrete distribution F, just what is the growth rate for the optimal expected waste.'],\n",
       "   '2000']],\n",
       " [[['Self-testing of universal and fault-tolerant sets of quantum gates.',\n",
       "    'We consider the design of self-testers for quantum gates. A self-tester for the gates $\\\\boldsymbol{F}_1,\\\\ldots, \\\\boldsymbol{F}_m$ is a procedure that, given any gates $\\\\boldsymbol{G}_1, \\\\ldots, \\\\boldsymbol{G}_m$, decides with high probability if each $\\\\boldsymbol{G}_i$ is close to $\\\\boldsymbol{F}_i$. This decision has to rely only on measuring in the computational basis the effect of iterating the gates on the classical states. It turns out that, instead of individual gates, we can design only procedures for families of gates. To achieve our goal we borrow some elegant ideas of the theory of program testing: We characterize the gate families by specific properties, develop a theory of robustness for them, and show that they lead to self-testers. In particular we prove that the universal and fault-tolerant set of gates consisting of a Hadamard gate, a $\\\\mathrm{c\\\\text{-}NOT}$ gate, and a phase rotation gate of angle $\\\\pi/4$ is self-testable.'],\n",
       "   '2000']],\n",
       " [[['Better algorithms for unfair metrical task systems and applications.',\n",
       "    'Unfair metrical task systems are a generalization of online metrical task systems. In this paper we introduce new techniques to combine algorithms for unfair metrical task systems and apply these techniques to obtain improved randomized online algorithms for metrical task systems on arbitrary metric spaces.'],\n",
       "   '2000']],\n",
       " [[['A matter of degree: improved approximation algorithms for degree-bounded minimum spanning trees.',\n",
       "    'In this paper, we present a new bicriteria approximation algorithm for the degree-bounded minimum spanning tree problem. In this problem, we are given an undirected graph, a nonnegative cost function on the edges, and a positive integer B*, and the goal is to find a minimum-cost spanning tree T with maximum degree at most B*. In an n-node graph, our algorithm finds a spanning tree with maximum degree O(B*+logn) and cost O(optB*), where optB* is the minimum cost of any spanning tree whose maximum degree is at most B*. Our algorithm uses ideas from Lagrangean duality. We show how a set of optimum Lagrangean multipliers yields bounds on both the degree and the cost of the computed solution.'],\n",
       "   '2000']],\n",
       " [[['A new proof of the weak pigeonhole principle.',\n",
       "    'The exact complexity of the weak pigeonhole principle is an old and fundamental problem in proof complexity. Using a diagonalization argument, J. B. Paris et al. (J. Symbolic Logic 53 (1988), 1235-1244) showed how to prove the weak pigeonhole principle with bounded-depth, quasipolynomialsize proofs. Their argument was further refined by J. Krajícek (J. Symbolic Logic 59 (1994), 73-86). In this paper, we present a new proof: we show that the weak pigeonhole principle has quasipolynomial-size LK proofs where every formula consists of a single AND/OR of polylog fan-in. Our proof is conceptually simpler than previous arguments, and is optimal with respect to depth.'],\n",
       "   '2000']],\n",
       " [[['On External-Memory MST, SSSP, and Multi-way Planar Graph Separation.',\n",
       "    'Recently external memory graph problems have received considerable attention because massive graphs arise naturally in many applications involving massive data sets. Even though a large number of I/O-efficient graph algorithms have been developed, a number of fundamental problems still remain open.The results in this paper fall in two main classes. First we develop an improved algorithm for the problem of computing a minimum spanning tree (MST) of a general undirected graph. Second we show that on planar undirected graphs the problems of computing a multi-way graph separation and single source shortest paths (SSSP) can be reduced I/O-efficiently to planar breadth-first search (BFS). Since BFS can be trivially reduced to SSSP by assigning all edges weight one, it follows that in external memory planar BFS, SSSP, and multi-way separation are equivalent. That is, if any of these problems can be solved I/O-efficiently, then all of them can be solved I/O-efficiently in the same bound. Our planar graph results have subsequently been used to obtain I/O-efficient algorithms for all fundamental problems on planar undirected graphs.'],\n",
       "   '2000']],\n",
       " [[['Data Structures for Maintaining Set Partitions.',\n",
       "    'Efficiently maintaining the partition induced by a set of features is an important problem in building decision-tree classifiers. In order to identify a small set of discriminating features, we need the capability of efficiently adding and removing specific features and determining the effect of these changes on the induced classification or partition. In this paper we introduce a variety of randomized and deterministic data structures to support these operations on both general and geometrically induced set partitions. We give both Monte Carlo and Las Vegas data structures that realize near-optimal time bounds and are practical to implement. We then provide a faster solution to this problem in the geometric setting. Finally, we present a data structure that efficiently estimates the number of partitions separating elements.'],\n",
       "   '2000']],\n",
       " [[['Approximation Algorithms for Clustering to Minimize the Sum of Diameters.',\n",
       "    'We consider the problem of partitioning the n nodes of a complete edge weighted graph into k clusters so as to minimize the sum of the diameters of the clusters. Since the problem is NP-complete, our focus is on the development of good approximation algorithms. When edge weights satisfy the triangle inequality, we present the first approximation algorithm for the problem. The approximation algorithm yields a solution which has no more than O(k) clusters such that the sum of cluster diameters is within a factor O(ln (n/k)) of the optimal value using exactly k clusters. Our approach also permits a tradeoff among the constant terms hidden by the two big-O terms and the running time. For any fixed k, we present an approximation algorithm that produces k clusters whose total diameter is at most twice the optimal value. When the distances are not required to satisfy the triangle inequality, we show that, unless P = NP, for any ρ ≥ 1, there is no polynomial time approximation algorithm that can provide a performance guarantee of ρ even when the number of clusters is fixed at 3. We also present some results for the problem of minimizing the sum of cluster radii.'],\n",
       "   '2000']],\n",
       " [[['Embeddings of -Connected Graphs of Pathwidth .',\n",
       "    'We present O(n3) embedding algorithms (subgraph isomorphism and its generalizations) for classes of graphs of bounded pathwidth, where n is the number of vertices in the graph. These include the first polynomial-time algorithm for minor containment and the first O(nc) algorithm (c a constant independent of k) for topological embedding of graphs from subclasses of partial k-trees, as well as an O(n2) algorithm for subgraph isomorphism. Of independent interest are structural properties of k-connected graphs of bounded pathwidth on which our algorithms are based. We also describe special cases which reduce to various generalizations of string matching, permitting more efficient solutions. Finally, we describe nk+O(1) algorithms for solving these problems on arbitrary graphs of pathwidth at most k.'],\n",
       "   '2000']],\n",
       " [[['A Blocked All-Pairs Shortest-Path Algorithm.',\n",
       "    \"We propose a blocked version of Floyd's all-pairs shortest-paths algorithm. The blocked algorithm makes better utilization of cache than does Floyd's original algorithm. Experiments indicate that the blocked algorithm delivers a speedup (relative to the unblocked Floyd's algorithm) between 1.6 and 1.9 on a Sun Ultra Enterprise 4000/5000 for graphs that have between 480 and 3200 vertices. The measured speedup on an SGI O2 for graphs with between 240 and 1200 vertices is between 1.6 and 2.\"],\n",
       "   '2000']],\n",
       " [[['XML-enabled Workflow Management for E-Services across Heterogeneous Platforms.',\n",
       "    \"Advanced e-services require efficient, flexible, and easy-to-use workflow technology that integrates well with mainstream Internet technologies such as XML and Web servers. This paper discusses an XML-enabled architecture for distributed workflow management that is implemented in the latest version of our Mentor-lite prototype system. The key asset of this architecture is an XML mediator that handles the exchange of business and flow control data between workflow and business-object servers on the one hand and client activities on the other via XML messages over http. Our implementation of the mediator has made use of Oracle's XSQL servlet. The major benefit of the advocated architecture is that it provides seamless integration of client applications into e-service workflows with scalable efficiency and very little explicit coding, in contrast to an earlier, Java-based, version of our Mentor-lite prototype that required much more code and exhibited potential performance problems.\"],\n",
       "   '2000']],\n",
       " [[['Active Rules for XML: A New Paradigm for E-Services.',\n",
       "    'XML is rapidly becoming one of the most widely adopted technologies for information exchange and representation. As the use of XML becomes more widespread, we foresee the development of active XML rules, i.e., rules explicitly designed for the management of XML information. In particular, we argue that active rules for XML offer a natural paradigm for the rapid development of innovative e-services. In the paper, we show how active rules can be specified in the context of XSLT, a pattern-based language for publishing XML documents (promoted by the W3C) which is receiving strong commercial support, and Lorel, a query language for XML documents that is quite popular in the research world. We demonstrate, through simple examples of active rules for XSLT and Lorel, that active rules can be effective for the implementation of e-commerce services. We also discuss the various issues that need to be considered in adapting the notion of relational triggers to the XML context.'],\n",
       "   '2000']],\n",
       " [[['Integrating and customizing heterogeneous e-commerce applications.',\n",
       "    'A broad spectrum of electronic commerce applications is currently available on the Web, providing services in almost any area one can think of. As the number and variety of such applications grow, more business opportunities emerge for providing new services based on the integration and customization of existing applications. (Web shopping malls and support for comparative shopping are just a couple of examples.) Unfortunately, the diversity of applications in each specific domain and the disparity of interfaces, application flows, actor roles in the business transaction, and data formats, renders the integration and manipulation of applications a rather difficult task. In this paper we present the Application Manifold system, aimed at simplifying the intricate task of integration and customization of e-commerce applications. The scope of the work in this paper is limited to web-enabled e-commerce applications. We do not support the integration/customization of proprietary/legacy applications. The wrapping of such applications as web services is complementary to our work. Based on the emerging Web data standard, XML, and application modeling standard, UML, the system offers a novel declarative specification language for describing the integration/customization task, supporting a modular approach where new applications can be added and integrated at will with minimal effort. Then, acting as an application generator, the system generates a full integrated/customized e-commerce application, with the declarativity of the specification allowing for the optimization and verification of the generated application. The integration here deals with the full profile of the given e-commerce applications: the various services offered by the applications, the activities and roles of the different actors participating in the application (e.g., customers, vendors), the application flow, as well as with the data involved in the process. This is in contrast to previous works on Web data integration that focused primarily on querying the data available in the applications, mostly ignoring the additional aspects mentioned above.'],\n",
       "   '2000']],\n",
       " [[['Sound and Complete Elimination of Singleton Kinds.',\n",
       "    \"Singleton kinds provide an elegant device for expressing type equality information resulting from modern module languages, but they can complicate the metatheory of languages in which they appear. I present a translation from a language with singleton kinds to one without, and prove this translation to be sound and complete. This translation is useful for type-preserving compilers generating typed target languages. The proof of soundness and completeness is done by normalizing type equivalence derivations using Stone and Harper's type equivalence decision procedure.\"],\n",
       "   '2000']],\n",
       " [[['SATIN: a toolkit for informal ink-based applications.',\n",
       "    \"Software support for making effective pen-based applications is currently rudimentary. To facilitate the creation of such applications, we have developed SATIN, a Java-based toolkit designed to support the creation of applications that leverage the informal nature of pens. This support includes a scenegraph for manipulating and rendering objects; support for zooming and rotating objects, switching between multiple views of an object, integration of pen input with interpreters, libraries for manipulating ink strokes, widgets optimized for pens, and compatibility with Java's Swing toolkit. SATIN includes a generalized architecture for handling pen input, consisting of recognizers, interpreters, and multi-interpreters. In this paper, we describe the functionality and architecture of SATIN, using two applications built with SATIN as examples.\"],\n",
       "   '2000']],\n",
       " [[['Interaction techniques for ambiguity resolution in recognition-based interfaces.',\n",
       "    'Because of its promise of natural interaction, recognition is coming into its own as a mainstream technology for use with computers. Both commercial and research applications are beginning to use it extensively. However the errors made by recognizers can be quite costly, and this is increasingly becoming a focus for researchers. We present a survey of existing error correction techniques in the user interface. These mediation techniques most commonly fall into one of two strategies, repetition and choice. Based on the needs uncovered by this survey, we have developed OOPS, a toolkit that supports resolution of input ambiguity through mediation. This paper describes four new interaction techniques built using OOPS, and the toolkit mechanisms required to build them. These interaction techniques each address problems not directly handled by standard approaches to mediation, and can all be re-used in a variety of settings.'],\n",
       "   '2000']],\n",
       " [[['Reconciling the Needs of Architectural Description with Object-Modeling Notations.',\n",
       "    'Complex software systems require expressive notations for representing their software architectures. Two competing paths have emerged. One is to use a specialized notation for architecture - or architecture description language (ADL). The other is to adapt a general-purpose modeling notation, such as UML. The latter has a number of benefits, including familiarity to developers, close mapping to implementations, and commercial tool support. However, it remains an open question as to how best to use object-oriented notations for architectural description, and, indeed, whether they are sufficiently expressive, as currently defined. In this paper we take a systematic look at these questions, examining the space of possible mappings from ADLs into object notations. Specifically, we describe (a) the principle strategies for representing architectural structure in UML; (b) the benefits and limitations of each strategy; and (c) aspects of architectural description that are intrinsically difficult to model in UML using the strategies.'],\n",
       "   '2000']],\n",
       " [[['UML Based Performance Modeling of Distributed Systems.',\n",
       "    'The development of distributed software systems satisfying performance requirements is achievable only spending careful attention to performance goals throughout the lifecycle, and especially from its very beginning. The aim of our approach is to encompass the performance validation task as an integrated activity within the development process of distributed systems. To this end we consider object oriented distributed systems based on UML, the Unified Modeling Language. We show how a system modeled by UML diagrams can be translated into a queueing network based performance model. The main contribution of this work consists of an extensive application to a case study of our methodological approach for the automatic generation of performance models. The considered case study falls in the domain of distributed software systems, where the proposed methodology suitably exploits and combines information derived from different UML diagrams to generate a quite accurate performance model.'],\n",
       "   '2000']],\n",
       " [[['Towards Availability Benchmarks: A Case Study of Software RAID Systems.',\n",
       "    'Benchmarks have historically played a key role in guiding the progress of computer science systems research and development, but have traditionally neglected the areas of availability, maintainability, and evolutionary growth, areas that have recently become critically important in high-end system design. As a first step in addressing this deficiency, we introduce a general methodology for benchmarking the availability of computer systems. Our methodology uses fault injection to provoke situations where availability may be compromised, leverages existing performance benchmarks for workload generation and data collection, and can produce results in both detail-rich graphical presentations or in distilled numerical summaries. We apply the methodology to measure the availability of the software RAID systems shipped with Linux, Solaris 7 Server, and Windows 2000 Server, and find that the methodology is powerful enough not only to quantify the impact of various failure conditions on the availability of these systems, but also to unearth their design philosophies with respect to transient errors and recovery policy.'],\n",
       "   '2000']],\n",
       " [[['Dynamic Function Placement for Data-Intensive Cluster Computing.',\n",
       "    'Optimally partitioning application and filesystem functionality within a cluster of clients and servers is a difficult problem due to dynamic variations in application behavior, resource availability, and workload mixes. This paper presents ABACUS, a run-time system that monitors and dynamically changes function placement for applications that manipulate large data sets. Several examples of data-intensive workloads are used to show the importance of proper function placement and its dependence on dynamic run-time characteristics, with performance differences frequently reaching 2-10X. We evaluate how well the ABACUS prototype adapts to run-time system behavior, including both long-term variation (e.g., filter selectivity) and short-term variation (e.g., multi-phase applications and interapplication resource contention). Our experiments with ABACUS indicate that it is possible to adapt in all of these situations and that the adaptation converges most quickly in those cases where the performance impact is most significant.'],\n",
       "   '2000']],\n",
       " [[['Scalable Content-aware Request Distribution in Cluster-based Network Servers.',\n",
       "    'We present a scalable architecture for content-aware request distribution in web server clusters. In this architecture, a level-4 switch acts as the point of contact for the server on the Inernet and distributes the incoming requests to a number of back-end nodes. The switch does not perform any contect-based distribution. This function is performed by each of the back-end nodes, which may forward the incoming request to another back-end based on the requested contect. In terms of scalability, this architecture compares favorably to existing approaches where a front-end node performs contect-based distribution. In our architecture, the expensive operations of TCP connection estabilishment and handoff are distributes among the back-ends, rather than being centralized in the front-end node. Only a minimal additional latency penatly is paid for much improved scalability. We have implemented this new architecture, and we demonstrate its superior scalability by comparing it to a system that performs contect-aware distribution in the front-end, both under synthetic and trace-drive workloads.'],\n",
       "   '2000']],\n",
       " [[['Integrating a Command Shell into a Web Browser.',\n",
       "    \"The transition from command-line interfaces to graphical interfaces has resulted in programs that are easier to learn and use, but harder to automate and reuse. Another transition is now underway, to HTML interfaces hosted by a web browser. To help users automate HTML interfaces, we propose the browser-shell, a web browser that integrates a command interpreter into the browser's Location box. The browser-shell's command language is designed for extracting and manipulating HTML and text, and commands can also invoke local programs. Command input is drawn from the current browser page, and command output is displayed as a new page. The browser-shell brings to web browsing many advantages of the Unix shell, including scripting web services and creating pipelines of web services and local programs. A browser-shell also allows legacy command-line programs to be wrapped with an HTML/CGI interface that is graphical but still scriptable, and offers a new shell interaction model, different from the conventional typescript model, which may improve usability in some respects.\"],\n",
       "   '2000']],\n",
       " [[['A Comparison of File System Workloads.',\n",
       "    'In this paper, we describe the collection and analysis of file system traces from a variety of different environments, including both UNIX and NT systems, clients and servers, and instructional and production systems. Our goal is to understand how modern workloads affect the ability of file systems to provide high performance to users. Because of the increasing gap between processor speed and disk latency, file system performance is largely determined by its disk behavior. Therefore we primarily focus on the disk I/O aspects of the traces. We find that more processes access files via the memory-map interface than through the read interface. However, because many processes memory-map a small set of files, these files are likely to be cached. We also find that file access has a bimodal distribution pattern: some files are written repeatedly without being read; other files are almost exclusively read. We develop a new metric for measuring file lifetime that accounts for files that are never deleted. Using this metric, we find that the average block lifetime for some workloads is significantly longer than the 30-second write delay used by many file systems. However, all workloads show lifetime locality: the same files tend to be overwritten multiple times.'],\n",
       "   '2000']],\n",
       " [[['Journaling Versus Soft Updates: Asynchronous Meta-data Protection in File Systems.',\n",
       "    'The UNIX Fast File System (FFS) is probably the most widely-used file system for performance comparisons. However, such comparisons frequently overlook many of the performance enhancements that have been added over the past decade. In this paper, we explore the two most commonly used approaches for improving the performance of meta-data operations and recovery: journaling and Soft Updates. Journaling systems use an auxiliary log to record meta-data operations and Soft Updates uses ordered writes to ensure metadata consistency. The commercial sector has moved en masse to journaling file systems, as evidenced by their presence on nearly every server platform available today: Solaris, AIX, Digital UNIX, HP-UX, Irix, and Windows NT. On all but Solaris, the default file system uses journaling. In the meantime, Soft Updates holds the promise of providing stronger reliability guarantees than journaling, with faster recovery and superior performance in certain boundary cases. In this paper, we explore the benefits of Soft Updates and journaling, comparing their behavior on both microbenchmarks and workload-based macrobenchmarks. We find that journaling alone is not sufficient to \"solve\" the meta-data update problem. If synchronous semantics are required (i.e., meta-data operations are durable once the system call returns), then the journaling systems cannot realize their full potential. Only when this synchronicity requirement is relaxed can journaling systems approach the performance of systems like Soft Updates (which also relaxes this requirement). Our asynchronous journaling and Soft Updates systems perform comparably in most cases. While Soft Updates excels in some meta-data intensive microbenchmarks, the macrobenchmark results are more ambiguous. In three cases Soft Updates and journaling are comparable. In a file intensive news workload, journaling prevails, and in a small ISP workload, Soft Updates prevails.'],\n",
       "   '2000']],\n",
       " [[['Isolation with Flexibility: A Resource Management Framework for Central Servers.',\n",
       "    \"Proportional-share resource management is becoming increasingly important in today's computing environments. In particular, the growing use of the computational resources of central service providers argues for a proportional-share approach that allows resource principals to obtain allocations that reflect their relative importance. In such environments, resource principals must be isolated from one another to prevent the activities of one principal from impinging on the resource rights of others. However, such isolation limits the flexibility with which resource allocations can be modified to reflect the actual needs of applications. We present extensions to the lottery-scheduling resource management framework that increase its flexibility while preserving its ability to provide secure isolation. To demonstrate how this extended framework safely overcomes the limits imposed by existing proportional-share schemes, we have implemented a prototype system that uses the framework to manage CPU time, physical memory, and disk bandwidth. We present the results of experiments that evaluate the prototype, and we show that our framework has the potential to enable server applications to achieve significant gains in performance.\"],\n",
       "   '2000']],\n",
       " [[['Operating System Support for Multi-User, Remote, Graphical Interaction.',\n",
       "    \"The emergence of thin client computing and multi-user, remote, graphical interaction revives a range of operating system research issues long dormant, and introduces new directions as well. This paper investigates the effect of operating system design and implementation on the performance of thin client service and interactive applications. We contend that the key performance metric for this type of system and its applications is user-perceived latency and we give a structured approach for investigating operating system design with this criterion in mind. In particular, we apply our approach to a quantitative comparison and analysis of Windows NT, Terminal Server Edition (TSE), and Linux with the X Windows System, two popular implementations of thin client service. We find that the processor and memory scheduling algorithms in both operating systems are not tuned for thin client service. Under heavy CPU and memory load, we observed user-perceived latencies up to 100 times beyond the threshold of human perception. Even in the idle state, these systems induce unnecessary latency. TSE performs particularly poorly despite scheduler modifications to improve interactive responsiveness. We also show that TSE's network protocol outperforms X by up to six times, and also makes use of a bitmap cache which is essential for handling dynamic elements of modern user interfaces and can reduce network load in these cases by up to 2000%.\"],\n",
       "   '2000']],\n",
       " [[['SIMPLIcity: Semantics-sensitive Integrated Matching for Picture Libraries.',\n",
       "    'The need for efficient content-based image retrieval has increased tremendously in many application areas such as biomedicine, military, commerce, education, and Web image classification and searching. We present here SIMPLIcity (Semantics-sensitive Integrated Matching for Picture LIbraries), an image retrieval system, which uses semantics classification methods, a wavelet-based approach for feature extraction, and integrated region matching based upon image segmentation. As in other region-based retrieval systems, an image is represented by a set of regions, roughly corresponding to objects, which are characterized by color, texture, shape, and location. The system classifies images into semantic categories, such as textured-nontextured, graph-photograph. Potentially, the categorization enhances retrieval by permitting semantically-adaptive searching methods and narrowing down the searching range in a database. A measure for the overall similarity between images is developed using a region-matching scheme that integrates properties of all the regions in the images. Compared with retrieval based on individual regions, the overall similarity approach 1) reduces the adverse effect of inaccurate segmentation, 2) helps to clarify the semantics of a particular region, and 3) enables a simple querying interface for region-based image retrieval systems. The application of SIMPLIcity to several databases, including a database of about 200,000 general-purpose images, has demonstrated that our system performs significantly better and faster than existing ones. The system is fairly robust to image alterations.'],\n",
       "   '2000']],\n",
       " [[['New techniques for topologically correct surface reconstruction.',\n",
       "    'We present a new approach to surface reconstruction based on theDelaunay complex. First we give a simple and fast algorithm thatpicks locally a surface at each vertex. For that, we introduce theconcept of ¿-intervals. It turns out that for smooth regions of thesurface this method works very well and at difficult parts of the surfaceyields an output well-suited for postprocessing. As a postprocessingstep we propose a topological clean up and a new techniquebased on linear programming in order to establish a topologicallycorrect surface. These techniques should be useful also for manyother reconstruction schemes.'],\n",
       "   '2000']],\n",
       " [[['Bicubic subdivision-surface wavelets for large-scale isosurface representation and visualization.',\n",
       "    'We introduce a new subdivision-surface wavelet transform for arbitrarytwo-manifolds with boundary that is the first to use simplelifting-style filtering operations with bicubic precision. We alsodescribe a conversion process for re-mapping large-scale isosurfacesto have subdivision connectivity and fair parameterizationsso that the new wavelet transform can be used for compressionand visualization. The main idea enabling our wavelet transformis the circular symmetrization of the filters in irregular neighborhoods,which replaces the traditional separation of filters into two1-D passes. Our wavelet transform uses polygonal base meshes torepresent surface topology, from which a Catmull-Clark-style subdivisionhierarchy is generated. The details between these levels ofresolution are quickly computed and compactly stored as waveletcoefficients. The isosurface conversion process begins with a contourtriangulation computed using conventional techniques, whichwe subsequently simplify with a variant edge-collapse procedure,followed by an edge-removal process. This provides a coarse initialbase mesh, which is subsequently refined, relaxed and attractedin phases to converge to the contour. The conversion is designed toproduce smooth, untangled and minimally- skewed parameterizations,which improves the subsequent compression after applyingthe transform. We have demonstrated our conversion and transformfor an isosurface obtained from a high-resolution turbulent-mixinghydrodynamics simulation, showing the potential for compressionand level-of-detail visualization.'],\n",
       "   '2000']],\n",
       " [[['Isosurfacing in higher dimensions.',\n",
       "    'Visualization algorithms have seen substantial improvements inthe past several years. However, very few algorithms have beendeveloped for directly studying data in dimensions higher thanthree. Most algorithms require a sampling in three-dimensionsbefore applying any visualization algorithms. This samplingtypically ignores vital features that may be present whenexamined in oblique cross-sections, and places an undo burden onsystem resources when animation through additional dimensionsis desired. For time-varying data of large data sets, smoothanimation is desired at interactive rates. This paper provides afast Marching Cubes like algorithm for hypercubes of anydimension. To support this, we have developed a new algorithm toautomatically generate the isosurface and triangulation tables forany dimension. This allows the efficient calculation of 4Disosurfaces, which can be interactively sliced to provide smoothanimation or slicing through oblique hyperplanes. The formerallows for smooth animation in a very compressed format. Thelatter provide better tools to study time-evolving features as theymove downstream. We also provide examples in using thistechnique to show interval volumes or the sensitivity of aparticular isovalue threshold.'],\n",
       "   '2000']],\n",
       " [[['Toward a compelling sensation of telepresence: demonstrating a portal to a distant (static) office.',\n",
       "    'In 1998 we introduced the idea for a project we call the Office ofthe Future. Our long-term vision is to provide a better everydayworking environment, with high-fidelity scene reconstruction forlife-sized 3D tele-collaboration. In particular, we want a true senseof presence with our remote collaborator and their real surroundings.The challenges related to this vision are enormous and involvemany technical tradeoffs. This is true in particular for scenereconstruction. Researchers have been striving to achieve real-timeapproaches, and while they have made respectable progress, thelimitations of conventional technologies relegate them to relativelylow resolution in a restricted volume.In this paper we present a significant step toward our ultimategoal, via a slightly different path. In lieu of low-fidelity dynamicscene modeling we present an exceedingly high fidelity reconstructionof a real but static office. By assembling the best of availablehardware and software technologies in static scene acquisition,modeling algorithms, rendering, tracking and stereo projective display,we are able to demonstrate a portal to a real office, occupiedtoday by a mannequin, and in the future by a real remote collaborator.We now have both a compelling sense of just how good it couldbe, and a framework into which we will later incorporate dynamicscene modeling, as we continue to head toward our ultimate goal of3D collaborative, telepresence.'],\n",
       "   '2000']],\n",
       " [[['Simplification of Tetrahedral meshes with accurate error evaluation.',\n",
       "    'The techniques for reducing the size of a volume dataset by preserving both the geometrical / topological shape and the information encoded in an attached scalar field areattracting growing interest. Given the framework of incremental 3Dmesh simplification based on edge collapse, the paper proposes an approach for the integrated evaluation of the error introduced by both the modification of the domain and the approximation of the field of the original volume dataset. We present and compare various techniques to evaluate the approximation error or to produce a sound prediction. A flexible simplification tool has beenimplemented, which provides different degree of accuracy and computational efficiency for the selection of the edge to be collapsed. Techniques for preventing a geometric or topological degeneration of the mesh are also presented.'],\n",
       "   '2000']],\n",
       " [[['Geometric compression for interactive transmission.',\n",
       "    'The compression of geometric structures is a relatively new fieldof data compression. Since about 1995, several articles have dealtwith the coding of meshes, using for most of them the followingapproach: the vertices of the mesh are coded in an order that partiallycontains the topology of the mesh. In the same time, somesimple rules attempt to predict the position of each vertex from thepositions of its neighbors that have been previously coded.In this article, we describe a compression algorithm whose principleis completely different: the coding order of the vertices is usedto compress their coordinates, and then the topology of the mesh isreconstructed from the vertices. This algorithm achieves compressionratios that are slightly better than those of the currently availablealgorithms, and moreover, it allows progressive and interactivetransmission of the meshes.'],\n",
       "   '2000']],\n",
       " [[['Texturing techniques for terrain visualization.',\n",
       "    'We present a new rendering technique for processing multiplemultiresolution textures of LOD terrain models and describe itsapplication to interactive, animated terrain content design. Theapproach is based on a multiresolution model for terrain texturewhich cooperates with a multiresolution model for terrain geometry.For each texture layer, an image pyramid and a texture treeare constructed. Multiple texture layers can be associated with oneterrain model and can be combined in different ways, e.g., byblending and masking. The rendering algorithm traverses simultaneouslythe geometry multiresolution model and the texture multiresolutionmodel, and takes into account geometric and textureapproximation errors. It uses multi-pass rendering and exploitsmultitexturing to achieve real-time performance. Applicationsinclude interactive texture lenses, texture animation, and topographictextures. These techniques offer an enormous potential fordeveloping new visualization applications for presenting, exploringand manipulating spatio-temporal data.'],\n",
       "   '2000']],\n",
       " [[['Volume illustration: non-photorealistic rendering of volume models.',\n",
       "    'Accurately and automatically conveying the structure of a volumemodel is a problem not fully solved by existing volume renderingapproaches. Physics-based volume rendering approaches createimages which may match the appearance of translucent materialsin nature, but may not embody important structural details.Transfer function approaches allow flexible design of the volumeappearance, but generally require substantial hand tuning for eachnew data set in order to be effective. We introduce the volumeillustration approach, combining the familiarity of a physics-basedillumination model with the ability to enhance importantfeatures using non-photorealistic rendering techniques. Sincefeatures to be enhanced are defined on the basis of local volumecharacteristics rather than volume sample value, the applicationof volume illustration techniques requires less manual tuning thanthe design of a good transfer function. Volume illustrationprovides a flexible unified framework for enhancing structuralperception of volume models through the amplification offeatures and the addition of illumination effects.'],\n",
       "   '2000']],\n",
       " [[['A continuous clustering method for vector fields.',\n",
       "    'A new method for the simplification of flow fields is presented. It isbased on continuous clustering. A well-known physical clusteringmodel, the Cahn Hillard model which describes phase separation, ismodified to reflect the properties of the data to be visualized. Clustersare defined implicitly as connected components of the positivityset of a density function. An evolution equation for this function isobtained as a suitable gradient flow of an underlying anisotropicenergy functional. Here, time serves as the scale parameter. Theevolution is characterized by a successive coarsening of patterns -the actual clustering - and meanwhile the underlying simulationdata specifies preferable pattern boundaries. Here we discuss theapplicability of this new type of approach mainly for flow fields,where the cluster energy penalizes cross streamline boundaries, butthe method also carries provisions in other fields as well. The clustersare visualized via iconic representations. A skeletonization algorithmis used to find suitable positions for the icons.'],\n",
       "   '2000']],\n",
       " [[['Topology preserving and controlled topology simplifying multiresolution isosurface extraction.',\n",
       "    'Multiresolution methods are becoming increasingly important toolsfor the interactive visualization of very large data sets. Multiresolutionisosurface visualization allows the user to explore volumedata using simplified and coarse representations of the isosurfacefor overview images, and finer resolution in areas of high interestor when zooming into the data. Ideally, a coarse isosurface shouldhave the same topological structure as the original. The topologicalgenus of the isosurface is one important property which is often neglectedin multiresolution algorithms. This results in uncontrolledtopological changes which can occur whenever the level-of-detailis changed. The scope of this paper is to propose an efficient techniquewhich allows preservation of topology as well as controlledtopology simplification in multiresolution isosurface extraction.'],\n",
       "   '2000']],\n",
       " [[['Fairing of non-manifolds for visualization.',\n",
       "    'The concept of fairing applied to irregular triangular meshes hasbecome more and more important. Previous contributions constructedbetter fairing operators, and applied them both to multiresolutionediting tools and to multiresolution representations ofmeshes. In this paper, we generalize these powerful techniques tohandle non-manifold models. Our framework computes a multi-levelfairing of models by fairing both the two-manifold surfacesthat define the model, the so-called two-features, and all theboundary and intersection curves of the model, the so-called one-features.In addition we introduce two extensions that can be usedin our framework as well as in manifold fairing concepts: an exactlocal volume preservation strategy and a method for feature preservation.Our framework works with any of the manifold fairingoperators for meshes.'],\n",
       "   '2000']],\n",
       " [[['Uniform frequency images: adding geometry to images to produce space-efficient textures.',\n",
       "    'We discuss the concept of uniform frequency images, whichexhibit uniform local frequency properties. Such images makeoptimal use of space when sampled close to their Nyquist limit.A warping function may be applied to an arbitrary image to redistributeits local frequency content, reducing its highest frequenciesand increasing its lowest frequencies in order to approachthis uniform frequency ideal. The warped image maythen be downsampled according to its new, reduced Nyquistlimit, thereby reducing its storage requirements. To reconstructthe original image, the inverse warp is applied.We present a general, top-down algorithm to automaticallygenerate a piecewise-linear warping function with this frequencybalancing property for a given input image. The image sizeis reduced by applying the warp and then downsampling. Westore this warped, downsampled image plus a small number ofpolygons with texture coordinates to describe the inverse warp.The original image is later reconstructed by rendering the associatedpolygons with the warped image applied as a texture map,a process which is easily accelerated by current graphics hardware.As compared to previous image compression techniques,we generate a similar graceful space-quality tradeoff with theadvantage of being able to \"uncompress\" images during rendering.We report results for several images with sizes ranging from15,000 to 300,000 pixels, achieving reduction rates of 70-90%with improved quality over downsampling alone.'],\n",
       "   '2000']],\n",
       " [[['Hardware-accelerated texture advection for unsteady flow visualization.',\n",
       "    'We present a novel hardware-accelerated texture advectionalgorithm to visualize the motion of two-dimensional unsteadyflows. Making use of several proposed extensions to theOpenGL-1.2 specification, we demonstrate animations of over65,000 particles at 2 frames/sec on an SGI Octane with EMXIgraphics. High image quality is achieved by careful attention toedge effects, noise frequency, and image enhancement. Weprovide a detailed description of the hardware implementation,including temporal and spatial coherence techniques, dyeadvection techniques, and feature extraction.'],\n",
       "   '2000']],\n",
       " [[['Image based rendering with stable frame rates.',\n",
       "    'This paper presents an efficient keyframeless image-based renderingtechnique. An intermediate image is used to exploit the coherencesamong neighboring frames. The pixels in the intermediateimage are first rendered by a ray-casting method and then warpedto the intermediate image at the current viewpoint and view direction.We use an offset buffer to record the precise positions of thesepixels in the intermediate image. Every frame is generated in threesteps: warping the intermediate image onto the frame, filling inholes, and selectively rendering a group of old pixels. By dynamicallyadjusting the number of those old pixels in the last step, theworkload at every frame can be balanced. The pixels generated bythe last two steps make contributions to the new intermediate image.Unlike occasional keyframes in conventional image-based renderingwhich need to be totally rerendered, intermediate images onlyneed to be partially updated at every frame. In this way, we guaranteemore stable frame rates and more uniform image qualities. Theintermediate image can be warped efficiently by a modified incremental3D warp algorithm. As a specific application, we demonstrateour technique with a voxel-based terrain rendering system.'],\n",
       "   '2000']],\n",
       " [[['Hardware-accelerated volume and isosurface rendering based on cell-projection.',\n",
       "    'We present two beneficial rendering extensions to the ProjectedTetrahedra (PT) algorithm by Shirley and Tuchman. These extensionsare compatible with any cell sorting technique, for examplethe BSP-XMPVO sorting algorithm for unstructured meshes.Using 3D texture mapping our first extension solves the long-standingproblem of hardware-accelerated but accurate renderingof tetrahedral volume cells with arbitrary transfer functions.By employing 2D texture mapping our second extension realizesthe hardware-accelerated rendering of multiple shaded isosurfaceswithin the PT algorithm without reconstructing the isosurfaces.Additionally, two methods are presented to combine projectedtetrahedral volumes with isosurfaces. The time complexity of allour algorithms is linear in the number of tetrahedra and does neitherdepend on the number of isosurfaces nor on the employed transferfunctions.'],\n",
       "   '2000']],\n",
       " [[['Pen-and-Ink rendering in volume visualisation.',\n",
       "    'This paper is concerned with the development of non-photorealisticrendering techniques for volume visualisation.In particular, we present two pen-and-ink renderingmethods, a 3D method based on non-photorealistic solidtextures, and a 2D method that involves two renderingphases in the object space and image space respectively.As both techniques utilize volume- and image-based datarepresentations, they can be built upon a traditional volumerendering pipeline, and be integrated with photorealisticmethods available in such a pipeline. We demonstratethat such an integration facilitates an effective mechanismfor enhancing visualisation and its interpretation.'],\n",
       "   '2000']],\n",
       " [[['A topology simplification method for 2D vector fields.',\n",
       "    'Topology analysis of plane, turbulent vector fields results in visualclutter caused by critical points indicating vortices of finer and finerscales. A simplification can be achieved by merging critical pointswithin a prescribed radius into higher order critical points. Afterbuilding clusters containing the singularities to merge, the methodgenerates a piecewise linear representation of the vector field ineach cluster containing only one (higher order) singularity. Anyvisualization method can be applied to the result after this process.Using different maximal distances for the critical points tobe merged results in a hierarchy of simplified vector fields that canbe used for analysis on different scales.'],\n",
       "   '2000']],\n",
       " [[['A flow-guided streamline seeding strategy.',\n",
       "    'This paper presents a seed placement strategy for streamlines basedon flow features in the dataset. The primary goal of our seedingstrategy is to capture flow patterns in the vicinity of critical pointsin the flow field, even as the density of streamlines is reduced. Secondarygoals are to place streamlines such that there is sufficientcoverage in non-critical regions, and to vary the streamline placementsand lengths so that the overall presentation is aestheticallypleasing (avoid clustering of streamlines, avoid sharp discontinuitiesacross several streamlines, etc.). The procedure is straight forwardand non-iterative. First, critical points are identified. Next,the flow field is segmented into regions, each containing a singlecritical point. The critical point in each region is then seeded with atemplate depending on the type of critical point. Finally, additionalseed points are randomly distributed around the field using a Poissondisk distribution to minimize closely spaced seed points. Themain advantage of this approach is that it does not miss the featuresaround critical points. Since the strategy is not image-guided, andhence not view dependent, significant savings are possible whenexamining flow fields from different viewpoints, especially for 3Dflow fields.'],\n",
       "   '2000']],\n",
       " [[['Scanline surfacing: building separating surfaces from planar contours.',\n",
       "    'A standard way to segment medical imaging datasets is by tracingcontours around regions of interest in parallel planar slices. Unfortunately,the standard methods for reconstructing three dimensionalsurfaces from those planar contours tend to be either complicatedor not very robust. Furthermore, they fail to consistently mesh abuttingstructures which share portions of contours. In this paper wepresent a novel, straight-forward algorithm for accurately and automaticallyreconstructing surfaces from planar contours. Our algorithmis based on scanline rendering and separating surface extraction.By rendering the contours as distinctly colored polygonsand reading back each rendered slice into a segmented volume, wereduce the complex problem of building a surface from planar contoursto the much simpler problem of extracting separating surfacesfrom a classified volume. Our scanline surfacing algorithmrobustly handles complex surface topologies such as bifurcations,embedded features, and abutting surfaces.'],\n",
       "   '2000']],\n",
       " [[['A level-set method for flow visualization.',\n",
       "    'In this paper we propose a technique for visualizing steady flow.Using this technique, we first convert the vector field data into ascalar level-set representation. We then analyze the dynamic behaviorand subsequent distortion of level-sets and interactively monitorthe evolving structures by means of texture-based surface rendering.Next, we combine geometrical and topological considerationsto derive a multiscale representation and to implement a methodfor the automatic placement of a sparse set of graphical primitivesdepicting homogeneous streams in the fields. Using the resulting algorithms,we have built a visualization system that enables us to effectivelydisplay the flow direction and its dynamics even for dense3D fields.'],\n",
       "   '2000']],\n",
       " [[['Approximate Query Processing Using Wavelets.',\n",
       "    \"Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today's decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing algorithms that operate directly on the wavelet-coefficient synopses of relational tables, allowing us to process arbitrarily complex queries entirely in the wavelet-coefficient domain. This guarantees extremely fast response times since our approximate query execution engine can do the bulk of its processing over compact sets of wavelet coefficients, essentially postponing the expansion into relational tuples until the end-result of the query. We also propose a novel wavelet decomposition algorithm that can build these synopses in an I/O-efficient manner. Finally, we conduct an extensive experimental study with synthetic as well as real-life data sets to determine the effectiveness of our wavelet-based approach compared to sampling and histograms. Our results demonstrate that our techniques: (1) provide approximate answers of better quality than either sampling or histograms; (2) offer query execution-time speedups of more than two orders of magnitude; and (3) guarantee extremely fast synopsis construction times that scale linearly with the size of the data.\"],\n",
       "   '2000']],\n",
       " [[['Efficiently Publishing Relational Data as XML Documents.',\n",
       "    'XML is rapidly emerging as a standard for exchanging business data on the World Wide Web. For the foreseeable future, however, most business data will continue to be stored in relational database systems. Consequently, if XML is to fulfill its potential, some mechanism is needed to publish relational data as XML documents. Towards that goal, one of the major challenges is finding a way to efficiently structure and tag data from one or more tables as a hierarchical XML document. Different alternatives are possible depending on when this processing takes place and how much of it is done inside the relational engine. In this paper, we characterize and study the performance of these alternatives. Among other things, we explore the use of new scalar and aggregate functions in SQL for constructing complex XML documents directly in the relational engine. We also explore different execution plans for generating the content of an XML document. The results of an experimental study show that constructing XML documents inside the relational engine can have a significant performance benefit. Our results also show the superiority of having the relational engine use what we call an &ldquo;outer union plan&rdquo; to generate the content of an XML document.'],\n",
       "   '2000']],\n",
       " [[['Mu3D: a causal consistency protocol for a collaborative VRML editor.',\n",
       "    'This paper describes the implementation of the Mu3D application protocol and consistency control mechanisms to allow the collaborative editing of CAD design. The collaborative editor (M3D editor) developed by us is VRML compliant. The editor has been used as a base for the European Esprit project No. 26287 - M3D and the Spanish project TEL 96-0544/CODI for Cooperative CAD applications.In our system, only the changes to local databases are transmitted to other collaborative session members. To assure database consistency, the system provides consistency control over the shared data space. A great effort has been paid also to provide a high capability of cooperation and user interactivity while narrowing networks bandwidth requirements.'],\n",
       "   '2000']],\n",
       " [[['Interactive reconstruction of virtual environments from photographs, with application to scene-of-crime analysis.',\n",
       "    'There are many real-world applications of Virtual Reality that require the construction of complex and accurate three-dimensional models, suitably structured for interactive manipulation. In this paper, we present semi-automatic methods that allow such environments to be quickly and easily built from photographs taken with uncalibrated cameras, and illustrate the techniques by application to the real-world problem of scene-of-crime reconstruction.'],\n",
       "   '2000']],\n",
       " [[['VR user interface: closed world interaction.',\n",
       "    'In this paper, we describe a user interface technique that uses a bounding box as a m taphor to facilitate interaction in a Virtual Reality (VR) environment. Because this technique is based on the observation that some of the VR application fields are contained in a closed world, we call it Closed World Interaction (CWI). After the user defines a closed world, the necessar virtual buttons are shown around the closed world which is presented by a frame. These virtual buttons are then used to interact with models. We also integrate some of the 2D Windows, Icons, Mouse and Pointer (WIMP) metaphors into CWI technique, reflecting our belief that users will be able to adapt to this environment quickly, A series of user studies were conducted to investigate the effectiveness of this technique. The results indicate that users can define a closed world quickly. Experience appears to be an important factor, and users can be trained to become familiar with CWI in the VR environment. The constrained interactions can also enhance the accuracy of selection. Two-handed manipulation somewhat improves the speed.'],\n",
       "   '2000']],\n",
       " [[['Animated deformations with radial basis functions.',\n",
       "    'We present a novel approach to creating deformations of polygonal models using Radial Basis Functions (RBFs) to produce localized real-time deformations. Radial Basis Functions assume surface smoothness as a minimal constraint and animations produce smooth displacements of affected vertices in a model. Animations are produced by controlling an arbitrary sparse set of control points defined on or near the surface of the model. The ability to directly manipulate a facial surface with a small number of point motions facilitates an intuitive method for creating facial expressions for virtual environment applications such as an immersive teleconferencing system or entertainment. Smooth deformations of the human face or other models are possible and illustrated with examples of a variety of expressions and mouth shapes.'],\n",
       "   '2000']],\n",
       " [[['Conceptual free-form styling on the responsive workbench.',\n",
       "    'A two-handed 3D styling system for free-form surfaces in a table-like Virtual Environment, the Responsive Workbench (RWB)TM, is described. Intuitive curve and surface deformation tools based on variational modeling and interaction techniques adapted to 3D VR modeling applications are proposed. The user draws curves (cubic B-splines) directly in the Virtual Environment using a stylus as an input device. The curves are connected automatically, such that a curve network develops. A combination of automatic and user-controlled topology extraction modules creates the connectivity information. The underlying surface model is based on B-spline surfaces, or, alternatively, uses multisided patches [20] bounded by closed loops of curve pieces.'],\n",
       "   '2000']],\n",
       " [[['Portable List Ranking: An Experimental Study.',\n",
       "    'We present and analyze two portable algorithms for the List Ranking Problem in the Coarse Grained Multicomputer model (CGM). We report on implementations of these algorithms and experiments that were done with these on a variety of parallel and distributed architectures, ranging from PC clusters to a mainframe parallel machine. With these experiments, we validate the chosen CGM model, and also show the possible gains and limits of such algorithms.'],\n",
       "   '2000']],\n",
       " [[['Planar Point Location for Large Data Sets: To Seek or Not to Seek.',\n",
       "    \"We present an algorithm for external memory planar point location that is both effective and easy to implement. The base algorithm is an external memory variant of the bucket method by Edahiro, Kokubo and Asano that is combined with Lee and Yang's batched internal memory algorithm for planar point location. Although our algorithm is not optimal in terms of its worst-case behavior, we show its efficiency for both batched and single-shot queries by experiments with real-world data. The experiments show that the algorithm benefits from the mainly sequential disk access pattern and significantly outperforms the fastest algorithm for internal memory. Due to its simple concept, the algorithm can take advantage of multiple disks and processors in a rather straightforward yet efficient way.\"],\n",
       "   '2000']],\n",
       " [[['On the Importance of Having an Identity or is Consensus Really Universal?',\n",
       "    \"We show that Naming - the existence of distinct IDs known to all - is a hidden, but necessary, assumption of Herlihy's universality result for Consensus. We then show in a very precise sense that Naming is harder than Consensus and bring to the surface some relevant differences existing between popular shared memory models.\"],\n",
       "   '2000']],\n",
       " [[['Disk Paxos.',\n",
       "    'We present an algorithm, called Disk Paxos, for implementing a reliable distributed system with a network of processors and disks. Like the original Paxos algorithm, Disk Paxos maintains consistency in the presence of arbitrary non-Byzantine faults. Progress can be guaranteed as long as a majority of the disks are available, even if all processors but one have failed.'],\n",
       "   '2000']],\n",
       " [[['Objects Shared by Byzantine Processes.',\n",
       "    'Work to date on algorithms for message-passing systems has explored a wide variety of types of faults, but corresponding work on shared memory systems has usually assumed that only crash faults are possible. In this work, we explore situations in which processes accessing shared objects can fail arbitrarily (Byzantine faults).'],\n",
       "   '2000']],\n",
       " [[['On the Domination Search Number.',\n",
       "    'We introduce the domination search game which can be seen as a natural modification of the well-known node search game. Various results concerning the domination search number of a graph are presented. In particular, we establish a very interesting connection between domination graph searching and a relatively new graph parameter called dominating target number.'],\n",
       "   '2000']],\n",
       " [[['Coloring Mixed Hypertrees.',\n",
       "    \"A mixed hypergraph is a triple (V, C, D) where V is its vertex set and C and D are families of subsets of V, called C-edges and D-edges, respectively. For a proper coloring, we require that each C-edge contains two vertices with the same color and each D-edge contains two vertices with different colors. The feasible set of a mixed hypergraph is the set of all k's for which there exists a proper coloring using exactly k colors. A hypergraph is a hypertree if there exists a tree such that the edges of the hypergraph induce connected subgraphs of the tree.We prove that feasible sets of mixed hypertrees are gap-free, i.e., intervals of integers, and we show that this is not true for precolored mixed hypertrees. The problem to decide whether a mixed hypertree can be colored by k colors is NP-complete in general; we investigate complexity of various restrictions of this problem and we characterize their complexity in most of the cases.\"],\n",
       "   '2000']],\n",
       " [[['People, places, things: Web presence for the real world.',\n",
       "    \"The convergence of Web technology, wireless networks, and portable client devices provides new design opportunities for computer/communications systems. In the HP Labs' Cooltown project we have been exploring these opportunities through an infrastructure to support Web presence for people, places and things. We put Web servers into things like printers and put information into Web servers about things like artwork; we group physically related things into places embodied in Web servers. Using URLs for addressing, physical URL beaconing and sensing of URLs for discovery, and localized Web servers for directories, we can create a location-aware but ubiquitous system to support nomadic users. On top of this infrastructure we can leverage Internet connectivity to support communications services. Web presence bridges the World Wide Web and the physical world we inhabit, providing a model for supporting nomadic users without a central control point.\"],\n",
       "   '2000']],\n",
       " [[['Networked surfaces: a new concept in mobile networking.',\n",
       "    'Networked surfaces are surfaces which provide network connectivity to specially augmented objects, when these objects are physically placed on top of the surface. When an object (e.g. a notebook computer) connects, a handshaking protocol assigns functions such as data or power transmission to the various conducting paths that are established. This paper describes the position occupied by this concept in the world of networking, presents an overview of the technology used in its realisation, describes the current prototype implementation, and outlines the potential implications of its introduction.'],\n",
       "   '2000']],\n",
       " [[['Managing the storage and battery resources in an image capture device (digital camera) using dynamic transcoding.',\n",
       "    'Advances in hardware imaging technology and user demand for convenient mobile electronic image capture are fueling the development of inexpensive image capture devices that can acquire images rivaling the image quality of photographic film. Improvements in the hardware imaging technology have to be matched with intelligent image storage mechanisms that are aware of local storage and battery constraints. In this paper, we explore using a dynamic, informed image transcoding technique to manage the consumed battery and storage resources in digital cameras. Such application aware technologies are fundamental for the mass consumer acceptance of these newer digital technologies. We show that this technique can allow the camera to store an order of magnitude more images. For a moderate number of images (e.g. 40), transcoding techniques can also maintain high quality images. The availability of fast wireless networks can allow the camera to capture 58 high quality images (51 uploaded) before running out of battery power. Storage technologies with expensive read and write operations (such as micro disks) can have a minor negative impact on battery life because of the extra read and write operations associated with transcoding operations. We show that the ability to effectively communicate the power vs. size vs. quality tradeoff to the end user is important for applications to adapt to the prevailing operating conditions.'],\n",
       "   '2000']],\n",
       " [[['A wireless fair scheduling algorithm for error-prone wireless channels.',\n",
       "    'In order to sustain relatively differentiated QoS over time-varying shared wireless medium with location-dependent errors, we propose in this paper a wireless fair scheduling algorithm which tries to both provide short-term fairness in the rate proportional guarantee sense and maintain a reasonable system throughput. Different implementation issues are discussed and performance is compared to alternative approaches found in the literature in which short term fairness is sacrificed for system throughput maximization.'],\n",
       "   '2000']],\n",
       " [[['Arena: the Arena product family: enterprise modeling solutions.',\n",
       "    'This paper introduces the Arena suite of products for modeling, simulation, and optimization highlighting product architecture and technology features that are targeted toward successful deployment of simulation and Arena throughout an enterprise.'],\n",
       "   '2000']],\n",
       " [[['Bayesian methods: bayesian methods for simulation.',\n",
       "    'This tutorial describes some ways that Bayesian methods address problems that arise during simulation studies. This includes quantifying uncertainty about input distributions and parameters, sensitivity analysis, and the selection of the best of several simulated alternatives. Focus is on illustrating the main ideas and their relevance to practical problems. Numerous citations for both introductory and more advanced material provide a launching pad into the Bayesian literature.'],\n",
       "   '2000']],\n",
       " [[['On simulation model complexity.',\n",
       "    'Nowadays the size and complexity of models is growing more and more, forcing modelers to face some problems that they were not accustomed to. Before trying to study ways to deal with complex models, a more important and primary question to explore is, is there any means to avoid the generation of complex models? The primary purpose of this paper is to discuss several issues regarding the complexity of simulation models, summarizing the findings in this area so far, and calling attention to this area that, despite its importance, appears to remain at the bottom of simulation research agendas.'],\n",
       "   '2000']],\n",
       " [[['SLX: the X is for extensibility.',\n",
       "    'SLX, Simulation Language with Extensibility, is the newest member in Wolverine Software\\'s family of simulation and animation software. SLX features unique extensibility mechanisms that allow users to tailor and extend SLX\\'s modeling capabilities. There are two advantages to extensibility. First, it ensures virtually unlimited adaptability. You\\'ll never get \"stuck\" with a problem you can\\'t solve with SLX. Second, extensibility allows packaging the use of highly efficient, low-level primitives in such a way that cumbersome details are hidden. Problems are described using nouns and verbs appropriate to the application. The tools provided for extending SLX include many of the tools used to develop SLX itself; however, these tools are by no means intended to be used exclusively by language developers. They are \"user-level\" tools that can be mastered by anyone. This paper presents an overview of SLX. Earlier papers (Henriksen 1997, 1998) presented the development of a conveyor modeling package in SLX, and example of how SLX has been coupled with other software, respectively.'],\n",
       "   '2000']],\n",
       " [[['Parallel execution of a sequential network simulator.',\n",
       "    'Parallel discrete event simulation (PDES) techniques have not yet made a substantial impact on the network simulation community because of the need to recast the simulation models using a new set of tools. To address this problem, we present a case study in transparently parallelizing a widely used network simulator, called ns. The use of this parallel ns does not require the modeler to learn any new tools or complex PDES techniques. The paper describes our approach and design choices to build the parallel ns and presents preliminary performance results, which are very encouraging.'],\n",
       "   '2000']],\n",
       " [[['A review of web based simulation: whither we wander?',\n",
       "    'This paper considers a variety of new technologies for discrete-event simulation software development. Environments and languages for web based simulation are reviewed. Web based applications are discussed. After proposing a summary of the review, ways of working that will have an unpredictable effect on the future of simulation modeling are proposed.'],\n",
       "   '2000']],\n",
       " [[['Dynamic component substitution in web-based simulation.',\n",
       "    'Recent breakthroughs in communication and software engineering has resulted in significant growth of web-based computing. Web-based techniques have been employed for modeling, simulation, and analysis of systems. The models for simulation are usually developed using component based techniques. In a component based model, a system is represented as a set of interconnected components. A component is a well defined software module that is viewed as a \"black box\" i.e., only its interface is of concern and not its implementation. However, the behavior of a component, which is necessary for simulation, could be implemented by different modelers including third party manufacturers. Web-based simulation environments enable effective sharing and reuse of components thereby minimizing model development overheads. In component based simulations, one or more components can be substituted during simulation with a functionally equivalent set of components. Such Dynamic Component Substitutions (DCS) provide an effective technique for selectively changing the level of abstraction of a model during simulation. It provides a tradeoff between simulation overheads and model details. It can be used to effectively study large systems and accelerate rare event simulations to desired scenarios of interest. DCS may also be used to achieve fault-tolerance in Web-based simulations. This paper presents the ongoing research to design and implement support for DCS in A Web-based Environment for Systems Engineering (WESE).'],\n",
       "   '2000']],\n",
       " [[['Design of experiments: robust design: seeking the best of all possible worlds.',\n",
       "    'We describe a framework for analyzing simulation output in order to find solutions that will work well after implementation. We show how the use of a loss function that incorporates both system mean and system variability can be used to efficiently and effectively carry out system optimization and improvement efforts. For models whose behavior depends on quantitative factors, we illustrate how robust design can be accomplished by using simple experimental designs in conjunction with response-surface metamodels. The results can yield new insights into system behavior, and may lead to recommended system configurations that differ substantially from those selected by analysis solely on the basis of mean response. We assume a knowledge base at the level of Chapter 12 of Simulation Modeling and Analysis (Law and Kelton 2000) but will review essential elements and distribute illustrative examples at the session.'],\n",
       "   '2000']],\n",
       " [[['Verification, validation, and accreditation: verification, validation, and accreditation of simulation models.',\n",
       "    'This paper discusses verification, validation, and accreditation of simulation models. The different approaches to deciding model validity are presented; how model verification and validation relate to the model development process are discussed; various validation techniques are defined; conceptual model validity, model verification, operational validity, and data validity are described; ways to document results are given; a recommended procedure is presented; and accreditation is briefly discussed.'],\n",
       "   '2000']],\n",
       " [[['Simulation optimization: a survey of simulation optimization techniques and procedures.',\n",
       "    'Discrete-event simulation optimization is a problem of significant interest to practitioners interested in extracting useful information about an actual (or yet to be designed) system that can be modeled using discrete-event simulation. This paper presents a brief survey of the literature on discrete-event simulation optimization over the past decade (1988 to the present). Swisher et al. (2000) provides a more comprehensive review of this topic while Jacobson and Schruben (1989) covers the literature preceding 1988. Optimization of both discrete and continuous input parameters are examined herein. The continuous input parameter case is separated into gradient and non-gradient based optimization procedures. The discrete input parameter case differentiates techniques appropriate for small and for large numbers of feasible input parameter values.'],\n",
       "   '2000']],\n",
       " [[['Web page classification based on k-nearest neighbor approach.',\n",
       "    'Automatic categorization is the only viable method to deal with the scaling problem of the World Wide Web. In this paper, we propose a Web page classifier based on an adaptation of k-Nearest Neighbor (k-NN) approach. To improve the performance of k-NN approach, we supplement k-NN approach with a feature selection method and a term-weighting scheme using markup tags, and reform document-document similarity measure used in vector space model. In our experiments on a Korean commercial Web directory, our proposed methods in k-NN approach for Web page classification improved the performance of classification.'],\n",
       "   '2000']],\n",
       " [[['Concurrency, objects and visualisation.',\n",
       "    'Object-oriented programming and concurrency are increasingly popular in computing education. Both are difficult topics in themselves, and the combination of both introduces subtle interactions that are not easily understood. We propose the development of a visualisation tool to illustrate both object-orientation as well as concurrency issues. Designing such a tool is a challenging task. It has been shown that visualisation tools are not always as effective as their authors had hoped, and the issues to be illustrated by our potential tools are not yet well defined. In this paper, we investigate both the visualisation aspect and the functionality that such a tool may have and we develop some guidelines for the design of a concurrent object visualisation tool.'],\n",
       "   '2000']],\n",
       " [[['Rational Coordination in Multi-Agent Environments.',\n",
       "    \"We address the issue of rational communicative behavior among autonomous self-interested agents that have to make decisions as to what to communicate, to whom, and how. Following decision theory, we postulate that a rational speaker should design a speech act so as to optimize the benefit it obtains as the result of the interaction. We quantify the gain in the quality of interaction in terms of the expected utility, and we present a framework that allows an agent to compute the expected utilities of various communicative actions. Our framework uses the Recursive Modeling Method as the specialized representation used for decision-making in a multi-agent environment. This representation includes information about the agent's state of knowledge, including the agent's preferences, abilities and beliefs about the world, as well as the beliefs the agent has about the other agents, the beliefs it has about the other agents' beliefs, and so on. Decision-theoretic pragmatics of a communicative act can be then defined as the transformation the act induces on the agent's state of knowledge about its decision-making situation. This transformation leads to a change in the quality of interaction, expressed in terms of the expected utilities of the agent's best actions before and after the communicative act. We analyze decision-theoretic pragmatics of a number of important kinds of communicative acts and investigate their expected utilities using examples. Finally, we report on the agreement between our method of message selection and messages that human subjects choose in various circumstances, and show an implementation and experimental validation of our framework in a simulated multi-agent environment.\"],\n",
       "   '2000']],\n",
       " [[['A Logic for Characterizing Multiple Bounded Agents.',\n",
       "    \"We describe a meta-logic for characterizing the evolving internal reasoning of various families of agents. We view the reasoning of agents as ongoing processes rather than as fixed sets of conclusions. Our approach utilizes a strongly sorted calculus, distinguishing the application language, time, and various syntactic sorts. We have established soundness and completeness results corresponding to various families of agents. This allows for useful and intuitively natural characterizations of such agents' reasoning abilities. We discuss and contrast consistency issues as in the work of Montague and Thomason. We also show how to represent the concept of focus of attention in this framework.\"],\n",
       "   '2000']],\n",
       " [[['On the Relationship Between BDI Logics and Standard Logics of Concurrency.',\n",
       "    \"The behavior of an agent is mainly governed by the specific way in which it handles the rational balance between information and deliberation. Rao and Georgeff's BDI theory is most popular among the formalisms capturing this very balance. This formalism has been proposed as a language for specifying agents in an abstract manner or, alternatively, for verifying various properties of agents implemented in some other programming language. In mainstream computer science, there are formalisms designed for a purpose similar to the BDI theory; not specifically aiming at agents, but at concurrency in general. These formalisms are known as logics of concurrent programs. In this paper these two frameworks are compared with each other for the first time. The result shows that the basic BDI theory, BDI_CTL&ast;, can be captured within a standard logic of concurrency. The logic which is relevant here is Kozen's propositional &mu;-calculus. &mu;-calculus turns out to be even strictly stronger in expressive power than BDI_CTL&ast; while enjoying a computational complexity which is not higher than that of BD_CTL&ast;'s small fragment CTL. This correspondence puts us in a position to provide the first axiomatization of Rao and Georgeff's full theory. Immediate consequences for the computational complexity of BDI theory are also explored, both for theorem proving and model checking.\"],\n",
       "   '2000']],\n",
       " [[['Synthesizing Coordination Requirements for Heterogeneous Autonomous Agents.',\n",
       "    'As agents move into ever more important applications, there is a natural growth in interest in techniques for synthesizing multiagent systems. We describe an approach for engineering the coordination requirements of a multiagent system based on an analysis of conversation instances extracted from usage scenarios. This approach exploits the notion of Dooley graphs that were recently introduced to the multiagent systems community from the linguistics and discourse analysis literature. We show how, with a few key modifications, Dooley graphs can be used to generate coordination requirements and constraints on the behavior models of the agents participating in a multiagent system.Our present approach is embodied in the context of our recent work on a distributed coordination service for heterogeneous, autonomous agents. This approach takes as input (a) agent skeletons, giving compact descriptions of the given agents in terms of their events that are significant for coordination, as well as (b) relationships among the events occurring in these skeletons. A natural question is how may the skeletons and relationships be produced in the first place. It turns out that a methodology that begins with Dooley graphs can readily yield the skeletons and relationships needed to achieve the desired coordination.Consequently, our approach combines the benefits of an intuitive methodology with a formal and distributed framework for developing multiagent systems from autonomous agents.'],\n",
       "   '2000']],\n",
       " [[['Towards Flexible Teamwork in Persistent Teams: Extended Report.',\n",
       "    \"Teamwork is a critical capability in multi-agent environments. Many such environments mandate that the agents and agent-teams must be persistent i.e., exist over long periods of time. Agents in such persistent teams are bound together by their long-term common interests and goals. This paper focuses on flexible teamwork in such persistent teams. Unfortunately, while previous work has investigated flexible teamwork, persistent teams remain unexplored. For flexible teamwork, one promising approach that has emerged is model-based, i.e., providing agents with general models of teamwork that explicitly specify their commitments in teamwork. Such models enable agents to autonomously reason about coordination. Unfortunately, for persistent teams, such models may lead to coordination and communication actions that while locally optimal, are highly problematic for the team's long-term goals. We present a decision-theoretic technique based on Markov decision processes to enable persistent teams to overcome such limitations of the model-based approach. In particular, agents reason about expected team utilities of future team states that are projected to result from actions recommended by the teamwork model, as well as lower-cost (or higher-cost) variations on these actions. To accommodate real-time constraints, this reasoning is done in an any-time fashion. Implemented examples from an analytic search tree and some real-world domains are presented.\"],\n",
       "   '2000']],\n",
       " [[['Semantic Issues in the Verification of Agent Communication Languages.',\n",
       "    'This article examines the issue of developing semantics for agent communication languages. In particular, it considers the problem of giving a verifiable semantics for such languages&mdash;a semantics where conformance (or otherwise) to the semantics could be determined by an independent observer. These problems are precisely defined in an abstract formal framework. Using this framework, a number of example agent communication frameworks are defined. A discussion is then presented, of the various options open to designers of agent communication languages, with respect the problem of verifying conformance.'],\n",
       "   '2000']],\n",
       " [[['The Gaia Methodology for Agent-Oriented Analysis and Design.',\n",
       "    'This article presents Gaia: a methodology for agent-oriented analysis and design. The Gaia methodology is both general, in that it is applicable to a wide range of multi-agent systems, and comprehensive, in that it deals with both the macro-level (societal) and the micro-level (agent) aspects of systems. Gaia is founded on the view of a multi-agent system as a computational organisation consisting of various interacting roles. We illustrate Gaia through a case study (an agent-based business process management system).'],\n",
       "   '2000']],\n",
       " [[['Algorithms for Distributed Constraint Satisfaction: A Review.',\n",
       "    'When multiple agents are in a shared environment, there usually exist constraints among the possible actions of these agents. A distributed constraint satisfaction problem (distributed CSP) is a problem to find a consistent combination of actions that satisfies these inter-agent constraints. Various application problems in multi-agent systems can be formalized as distributed CSPs. This paper gives an overview of the existing research on distributed CSPs. First, we briefly describe the problem formalization and algorithms of normal, centralized CSPs. Then, we show the problem formalization and several MAS application problems of distributed CSPs. Furthermore, we describe a series of algorithms for solving distributed CSPs, i.e., the asynchronous backtracking, the asynchronous weak-commitment search, the distributed breakout, and distributed consistency algorithms. Finally, we show two extensions of the basic problem formalization of distributed CSPs, i.e., handling multiple local variables, and dealing with over-constrained problems.'],\n",
       "   '2000']],\n",
       " [[['A Note on the Utility of Incremental Learning.',\n",
       "    'Historically, inductive machine learning has focused on non&dash;incremental learning tasks, i.e., where the training set can be constructed a priori and learning stops once this set has been duly processed. There are, however, a number of areas, such as agents, where learning tasks are incremental. This paper defines the notion of incrementality for learning tasks and algorithms. It then provides some motivation for incremental learning and argues in favour of the design of incremental learning algorithms for solving incremental learning tasks. A number of issues raised by such systems are outlined and the incremental learner ILA is used for illustration.'],\n",
       "   '2000']],\n",
       " [[['Quantitative Disjunctive Logic Programming: Semantics and Computation.',\n",
       "    'A new knowledge representation language, called QDLP, which extends DLP to deal with uncertain values is introduced. Each (quantitative) rule is assigned a certainty degree interval (a subinterval of [0,1]). The propagation of uncertainty information from the premises to the conclusion of a quantitative rule is achieved by means of triangular norms (T&dash;norms). Different T&dash;norms induce different semantics for one given quantitative program. In this sense, QDLP is parameterized and each choice of a T&dash;norm induces a different QDLP language. Each T&dash;norm is eligible for events with determinate relationships (e.g., independence, exclusiveness) between them. Since there are infinitely many T&dash;norms, it turns out that there is a family of infinitely many QDLP languages. This family is carefully studied and the set of QDLP languages which generalize traditional DLP is precisely singled out. Algorithms for computing the minimal models of quantitative programs are proposed.'],\n",
       "   '2000']],\n",
       " [[['Adaptive Intrusion Detection: A Data Mining Approach.',\n",
       "    'In this paper we describe a data mining framework for constructing intrusion detection models. The first key idea is to mine system audit data for consistent and useful patterns of program and user behavior. The other is to use the set of relevant system features presented in the patterns to compute inductively learned classifiers that can recognize anomalies and known intrusions. In order for the classifiers to be effective intrusion detection models, we need to have sufficient audit data for training and also select a set of predictive system features. We propose to use the association rules and frequent episodes computed from audit data as the basis for guiding the audit data gathering and feature selection processes. We modify these two basic algorithms to use axis attribute(s) and reference attribute(s) as forms of item constraints to compute only the relevant patterns. In addition, we use an iterative level-wise approximate mining procedure to uncover the low frequency but important patterns. We use meta-learning as a mechanism to make intrusion detection models more effective and adaptive. We report our extensive experiments in using our framework on real-world audit data.'],\n",
       "   '2000']],\n",
       " [[['PlanMine: Predicting Plan Failures Using Sequence Mining.',\n",
       "    'This paper presents the PlanMine sequence mining algorithm to extract patterns of events that predict failures in databases of plan executions. New techniques were needed because previous data mining algorithms were overwhelmed by the staggering number of very frequent, but entirely unpredictive patterns that exist in the plan database. This paper combines several techniques for pruning out unpredictive and redundant patterns which reduce the size of the returned rule set by more than three orders of magnitude. PlanMine has also been fully integrated into two real-world planning systems. We experimentally evaluate the rules discovered by PlanMine, and show that they are extremely useful for understanding and improving plans, as well as for building monitors that raise alarms before failures happen.'],\n",
       "   '2000']],\n",
       " [[['A taxonomy of parallel strategies for deduction.',\n",
       "    'This paper presents a taxonomy of parallel theorem-proving methods based on the control of search (e.g., master&ndash;slaves versus peer processes), the granularity of parallelism (e.g., fine, medium and coarse grain) and the nature of the method (e.g., ordering-based versus subgoal-reduction). We analyze how the different approaches to parallelization affect the control of search: while fine and medium&ndash;grain methods, as well as master-slaves methods, generally do not modify the sequential search plan, parallel-search methods may combine sequential search plans (i>multi-search) or extend the search plan with the capability of subdividing the search space (i>distributed search). Precisely because the search plan is modified, the latter methods may produce radically different searches than their sequential base, as exemplified by the first distributed proof of the i>Robbins theorem generated by the i>Modified Clause-Diffusion prover i>Peers-mcd. An overview of the state of the field and directions for future research conclude the paper.'],\n",
       "   '2000']],\n",
       " [[['TAME: Using PVS strategies for special-purpose theorem proving.',\n",
       "    \"TAME (Timed Automata Modeling Environment), an interface to the theorem proving system PVS, is designed for proving properties of three classes of automata: I/O automata, Lynch&ndash;Vaandrager timed automata, and SCR automata. TAME provides templates for specifying these automata, a set of auxiliary theories, and a set of specialized PVS strategies that rely on these theories and on the structure of automata defined using the templates. Use of the TAME strategies simplifies the process of proving automaton properties, particularly state and transition invariants. TAME provides two types of strategies: strategies for &ldquo;automatic&rdquo; proof and strategies designed to implement &ldquo;natural&rdquo; proof steps, i.e., proof steps that mimic the high-level steps in typical natural language proofs. TAME's &ldquo;natural&rdquo; proof steps can be used both to mechanically check hand proofs in a straightforward way and to create proof scripts that can be understood without executing them in the PVS proof checker. Several new PVS features can be used to obtain better control and efficiency in user-defined strategies such as those used in TAME. This paper describes the TAME strategies, their use, and how their implementation exploits the structure of specifications and various PVS features. It also describes several features, currently unsupported in PVS, that would either allow additional &ldquo;natural&rdquo; proof steps in TAME or allow existing TAME proof steps to be improved. Lessons learned from TAME relevant to the development of similar specialized interfaces to PVS or other theorem provers are discussed.\"],\n",
       "   '2000']],\n",
       " [[['A survey of temporal extensions of description logics.',\n",
       "    'This paper surveys the temporal extensions of description logics appearearing in the literature. The analysis considers a large spectrum of approaches appearearing in the temporal description logics area: from the loosely coupled approaches &ndash; which comprise, for example, the enhancement of simple description logics with a constraint based mechanism &ndash; to the most principled ones &ndash; which consider a combined semantics for the abstract and the temporal domains. It will be shown how these latter approaches have a strict connection with temporal logics.Advantages of using temporal description logics are their high expressivity combined with desirable computational properties &ndash; such as decidability, soundness and completeness of deduction procedures. In this survey the computational properties of various families of temporal description logics will be pointed out.'],\n",
       "   '2000']],\n",
       " [[['Temporal representation and reasoning in artificial intelligence: Issues and approaches.',\n",
       "    'Time is one of the most relevant topics in AI. It plays a major role in several areas, ranging from logical foundations to applications of knowledge&dash;based systems. In this paper, we survey a wide range of research in temporal representation and reasoning, without committing ourselves to the point of view of any specific application. The organization of the paper follows the commonly recognized division of the field in two main subfields: reasoning about actions and change, and reasoning about temporal constraints. We give an overview of the basic issues, approaches, and results in these two areas, and outline relevant recent developments. Furthermore, we briefly analyze the major emerging trends in temporal representation and reasoning as well as the relationships with other well&dash;established areas, such as temporal databases and logic programming.'],\n",
       "   '2000']],\n",
       " [[['A graph-theoretic approach to efficiently reason about partially ordered events in (Modal) Event Calculus.',\n",
       "    \"In this paper, we show how well-known graph-theoretic techniques can be successfully exploited to efficiently reason about partially ordered events in Kowalski and Sergot's Event Calculus and in its skeptical and credulous modal variants. To overcome the computational weakness of the traditional generate-and-test algorithm of (Modal) Event Calculus, we propose two alternative graph-traversal algorithms that operate on the underlying directed acyclic graph of events representing ordering information. The first algorithm pairs breadth-first and depth-first visits of such an event graph in a suitable way, while the second one operates on its transitive closure and reduction. We prove the soundness and completeness of both algorithms, and thoroughly analyze and compare their computational complexity.\"],\n",
       "   '2000']],\n",
       " [[['Proof planning for strategy development.',\n",
       "    'Proof planning extends the tactic-based theorem proving paradigm through the explicit representation of proof strategies. We see three key benefits to the proof planning approach to the development of proof strategies: flexibility, re-usability and synergy. Here we demonstrate these benefits in terms of reasoning about imperative programs where we reuse strategies developed previously for proof by mathematical induction. In particular, we focus upon strategies for automating the discovery of loop invariants. Our approach tightly couples the discovery of invariants with the process of patching proof strategy failures.'],\n",
       "   '2000']],\n",
       " [[['Warm fusion in Stratego: A case study in generation of program transformation systems.',\n",
       "    'Stratego is a domain-specific language for the specification of program transformation systems. The design of Stratego is based on the paradigm of rewriting strategies: user-definable programs in a little language of strategy operators determine where and in what order transformation rules are (automatically) applied to a program. The separation of rules and strategies supports modularity of specifications. Stratego also provides generic features for specification of program traversals.In this paper we present a case study of Stratego as applied to a non-trivial problem in program transformation. We demonstrate the use of Stratego in eliminating intermediate data structures from (also known as i>deforesting) functional programs via the i>warm fusion algorithm of Launchbury and Sheard. This algorithm has been specified in Stratego and embedded in a fully automatic transformation system for kernel Haskell. The entire system consists of about 2600 lines of specification code, which breaks down into 1850 lines for a general framework for Haskell transformation and 750 lines devoted to a highly modular, easily extensible specification of the warm fusion transformer itself. Its successful design and construction provides further evidence that programs generated from Stratego specifications are suitable for integration into real systems, and that rewriting strategies are a good paradigm for the implementation of such systems.'],\n",
       "   '2000']],\n",
       " [[['TALplanner: A temporal logic based forward chaining planner.',\n",
       "    'We present TALplanner, a forward-chaining planner based on the use of domain-dependent search control knowledge represented as formulas in the Temporal Action Logic (TAL). TAL is a narrative based linear metric time logic used for reasoning about action and change in incompletely specified dynamic environments. TAL is used as the formal semantic basis for TALplanner, where a TAL goal narrative with control formulas is input to TALplanner which then generates a TAL narrative that entails the goal and control formulas. The sequential version of TALplanner is presented. The expressivity of plan operators is then extended to deal with an interesting class of resource types. An algorithm for generating concurrent plans, where operators have varying durations and internal state, is also presented. All versions of TALplanner have been implemented. The potential of these techniques is demonstrated by applying TALplanner to a number of standard planning benchmarks in the literature.'],\n",
       "   '2000']],\n",
       " [[['Design-code traceability for object-oriented systems.',\n",
       "    'Traceability is a key issue to ensure consistency among software artifacts of subsequent phases of the development cycle. However, few works have so far addressed the theme of tracing object oriented (OO) design into its implementation and evolving it. This paper presents an approach to checking the compliance of OO design with respect to source code and support its evolution. The process works on design artifacts expressed in the OMT (Object Modeling Technique) notation and accepts C&plus;&plus; source code. It recovers an &ldquo;as is&rdquo; design from the code, compares the recovered design with the actual design and helps the user to deal with inconsistencies. The recovery process exploits the edit distance computation and the maximum match algorithm to determine traceability links between design and code. The output is a similarity measure associated to design&dash;code class pairs, which can be classified as matched and unmatched by means of a maximum likelihood threshold. A graphic display of the design with different green levels associated to different levels of match and red for the unmatched classes is provided as a support to update the design and improve its traceability to the code.'],\n",
       "   '2000']],\n",
       " [[['Object modelling languages: An evaluation and some key expectations for the future.',\n",
       "    'Object modelling languages are graphical semi&dash;formal specification languages. They are tools to capture and formalise requirements in the earlier phases of software development, as well as providing support for describing designs, software architecture and even detailed implementations later in the process. One can consider these languages to have reached some level of maturity, especially because their precursors, the Object&dash;Oriented Analysis and Design methods, have now been used and tested intensively in industry for many years. In addition, these modelling languages have been the subject of many improvements by the scientific community. Nevertheless, some dissatisfaction persists. In this paper, we aim to re&dash;analyse several parts of the deep structure of two leading object modelling languages: OML and UML, in order to show how they can really increase software quality. Their structure is based on metamodelling, which is the way the semantics of these two languages is expressed. This structure is also the source of a proliferation of modelling constructs (for example, different forms of inheritance associated with distinct notational elements) whose use must clearly influence, in particular, reusability &ndash; a key expectation in a software engineering process. More generally, we identify some deficiencies in these languages, which allows us to highlight some appropriate evolutionary paths. In discussing dynamic metamodelling and scalability, we specifically outline that a main current drawback is the difficulty of implementing these languages in Computer&dash;Aided Software Engineering tools.'],\n",
       "   '2000']],\n",
       " [[['Toward a discipline of scenario-based architectural engineering.',\n",
       "    'Software architecture analysis is a cost&dash;effective means of controlling risk and maintaining system quality throughout the processes of software design, development and maintenance. This paper presents a sequence of steps that maps architectural quality goals into scenarios that measure the goals, mechanisms that realize the scenarios and analytic models that measure the results. This mapping ensures that design decisions and their rationale are documented in such a fashion that they can be systematically explored, varied, and potentially traded off against each other. As systems evolve, the analytic models can be used to assess the impact of architectural changes, relative to the system&rsquo;s changing quality goals. Although scenarios have been extensively used in software design to understand the ways in which a system meets its operational requirements, there has been little systematic use of scenarios to support analysis, particularly analysis of a software architecture&rsquo;s quality attributes: modifiability, portability, extensibility, security, availability, and so forth. In this paper we present a unified approach to using scenarios to support both the design, analysis and maintenance of software architectures, and examples from large&dash;scale software development projects where we have applied the approach. We also present a tool, called Brie, that aids in: scenario capture, mapping scenarios to software architectures, and the association of analytic models with particular portions of architectures. The approach that we have devised, and that Brie supports, is a foundation for a discipline of architectural engineering. Architectural engineering is an iterative method of design, analysis and maintenance where design decisions are motivated by scenarios, and are supported by documented analyses.'],\n",
       "   '2000']],\n",
       " [[['Accuracy of software quality models over multiple releases.',\n",
       "    'Many evolving mission&dash;critical systems must have high software reliability. However, it is often difficult to identify fault&dash;prone modules early enough in a development cycle to guide software enhancement efforts effectively and efficiently. Software quality models can yield timely predictions of membership in the fault&dash;prone class on a module&dash;by&dash;module basis, enabling one to target enhancement techniques. However, it is an open empirical question, &ldquo;Can a software quality model remain useful over several releases&quest;&rdquo; Most prior software quality studies have examined only one release of a system, evaluating the model with modules from the same release. We conducted a case study of a large legacy telecommunications system where measurements on one software release were used to build models, and three subsequent releases of the same system were used to evaluate model accuracy. This is a realistic assessment of model accuracy, closely simulating actual use of a software quality model. A module was considered fault&dash;prone if any of its faults were discovered by customers. These faults are extremely expensive due to consequent loss of service and emergency repair efforts. We found that the model maintained useful accuracy over several releases. These findings are initial empirical evidence that software quality models can remain useful as a system is maintained by a stable software development process.'],\n",
       "   '2000']],\n",
       " [[['Modeling software evolution by evolving interoperation graphs.',\n",
       "    'Software evolution is the process of software change, most often change in software requirements. This paper presents a theoretical model for the evolution of component&dash;based software, based on evolving interoperation graphs. The model assumes that each change consists of smaller granularity steps of change propagation, each of them being a visit to one specific component. If the component is modified, it may no longer fit with the other components because it may no longer properly interact with them. In that case secondary changes must be made in neighboring components, which may trigger additional changes, etc. The paper contains an example of evolution of a calendar program, represented in UML.'],\n",
       "   '2000']],\n",
       " [[['The use of domain knowledge in program understanding.',\n",
       "    'Program understanding is an essential part of all software maintenance and enhancement activities. As currently practiced, program understanding consists mainly of code reading. The few automated understanding tools that are actually used in industry provide helpful but relatively shallow information, such as the line numbers on which variable names occur or the calling structure possible among system components. These tools rely on analyses driven by the nature of the programming language used. As such, they are adequate to answer questions concerning implementation details, so called what questions. They are severely limited, however, when trying to relate a system to its purpose or requirements, the why questions. Application programs solve real&dash;world problems. The part of the world with which a particular application is concerned is that application&rsquo;s domain. A model of an application&rsquo;s domain can serve as a supplement to programming&dash;language&dash;based analysis methods and tools. A domain model carries knowledge of domain boundaries, terminology, and possible architectures. This knowledge can help an analyst set expectations for program content. Moreover, a domain model can provide information on how domain concepts are related. This article discusses the role of domain knowledge in program understanding. It presents a method by which domain models, together with the results of programming&dash;language&dash;based analyses, can be used to answers both what and why questions. Representing the results of domain&dash;based program understanding is also important, and a variety of representation techniques are discussed. Although domain&dash;based understanding can be performed manually, automated tool support can guide discovery, reduce effort, improve consistency, and provide a repository of knowledge useful for downstream activities such as documentation, reengineering, and reuse. A tools framework for domain&dash;based program understanding, a dowser, is presented in which a variety of tools work together to make use of domain information to facilitate understanding. Experience with domain&dash;based program understanding methods and tools is presented in the form of a collection of case studies. After the case studies are described, our work on domain&dash;based program understanding is compared with that of other researchers working in this area. The paper concludes with a discussion of the issues raised by domain&dash;based understanding and directions for future work.'],\n",
       "   '2000']],\n",
       " [[['The canonical activities of reverse engineering.',\n",
       "    'This paper describes three categories of canonical activities that are characteristic of reverse engineering for program understanding. The activities are data gathering, knowledge management, and information exploration. All tasks carried out by a software engineer during a program understanding exercise can be mapped to a composition of one or more of these canonical activities. The design space formed by the canonical activities can be used to classify the capabilities provided by individual support mechanisms using a common vocabulary. A descriptive model that categorizes important support mechanism features based on a hierarchy of attributes is used to structure the canonical activities.'],\n",
       "   '2000']],\n",
       " [[['A comparative study of formal verification techniques for software architecture specifications.',\n",
       "    'With the rapid growth of network computing, the demand for large&dash;scale and complex software systems has increased dramatically. However, the development of large&dash;scale and complex software systems is much more difficult and error prone. This is due to the fact that techniques and tools for assuring the correctness and reliability of software systems lag far behind the increasing growth in size and complexity of software systems. The concept of software architecture has recently emerged as a new way to improve our ability to effectively construct and maintain large&dash;scale complex software systems. The architecture based development of software systems focuses on the architectural elements and their overall interconnection structure. Several Architectural Definition Languages (ADLs) have been proposed for specifying domain specific or general purpose architectures. On the other hand, formal verification is rapidly becoming a promising and automated method to ensure the accuracy and correctness of software systems. In this paper, we survey several architecture description languages and formal verification methods. We present an environment to conduct experiments to study the performance of five different verification tools on software architecture specifications. Based on these experiments, we are able to compare the efficiency of these verification tools in verifying certain software property.'],\n",
       "   '2000']],\n",
       " [[['Towards automated modification of legacy assets.',\n",
       "    'In this paper we argue that there is a necessity for automating modifications to legacy assets. We propose a five layered process for the introduction and employment of tool support that enables automated modification to entire legacy systems. Furthermore, we elaborately discuss each layer on a conceptual level, and we make appropriate references to sources where technical contributions supporting that particular layer can be found. We sketch the perspective that more and more people working in the software engineering area will be contributing to working on existing systems and/or tools to support such work.'],\n",
       "   '2000']],\n",
       " [[['Interactive Case-Based Planning for Forest Fire Management.',\n",
       "    'This paper describes an AI system for planning the first attack on a forest fire. This planning system is based on two major techniques, case-based reasoning, and constraint reasoning, and is part of a decision support system called CHARADE. CHARADE is aimed at supporting the user in the whole process of forest fire management. The novelty of the proposed approach is mainly due to the use of a local similarity metric for case-based reasoning and the integration with a constraint solver in charge of temporal reasoning.'],\n",
       "   '2000']],\n",
       " [[['Two-Loop Real-Coded Genetic Algorithms with Adaptive Control of Mutation Step Sizes.',\n",
       "    'Genetic algorithms are adaptive methods based on natural evolution that may be used for search and optimization problems. They process a population of search space solutions with three operations: selection, crossover, and mutation. Under their initial formulation, the search space solutions are coded using the binary alphabet, however other coding types have been taken into account for the representation issue, such as real coding. The real-coding approach seems particularly natural when tackling optimization problems of parameters with variables in continuous domains.A problem in the use of genetic algorithms is premature convergence, a premature stagnation of the search caused by the lack of population diversity. The mutation operator is the one responsible for the generation of diversity and therefore may be considered to be an important element in solving this problem. For the case of working under real coding, a solution involves the control, throughout the run, of the strength in which real genes are mutated, i.e., the step size.This paper presents TRAMSS, a Two-loop Real-coded genetic algorithm with Adaptive control of Mutation Step Sizes. It adjusts the step size of a mutation operator applied during the inner loop, for producing efficient local tuning. It also controls the step size of a mutation operator used by a restart operator performed in the outer loop, for reinitializing the population in order to ensure that different promising search zones are focused by the inner loop throughout the run. Experimental results show that the proposal consistently outperforms other mechanisms presented for controlling mutation step sizes, offering two main advantages simultaneously, better reliability and accuracy.'],\n",
       "   '2000']],\n",
       " [[['Exception Handling in Workflow Systems.',\n",
       "    'In this paper, defeasible workflow is proposed as a framework to support exception handling for workflow management. By using the &ldquo;justified&rdquo; ECA rules to capture more contexts in workflow modeling, defeasible workflow uses context dependent reasoning to enhance the exception handling capability of workflow management systems. In particular, this limits possible alternative exception handler candidates in dealing with exceptional situations. Furthermore, a case-based reasoning (CBR) mechanism with integrated human involvement is used to improve the exception handling capabilities. This involves collecting cases to capture experiences in handling exceptions, retrieving similar prior exception handling cases, and reusing the exception handling experiences captured in those cases in new situations.'],\n",
       "   '2000']],\n",
       " [[['Conceptual Models and Architectures for Advanced Information Systems.',\n",
       "    'This paper addresses several issues related to the use of conceptual modeling to support service-oriented, advanced information systems. It shows how conceptual modeling of information resources can be used to integrate information obtained from multiple data sources, including both internal and external data. The notion of an i>intelligent thesaurus is presented and a meta-model of the thesaurus is developed. It is then used to create a three-layer architecture consisting of the actual data source schemas, a &ldquo;wrapped&rdquo; object-oriented abstraction of the schemas expressed in terms of the thesaurus primitives, and an integrated version which serves as the federation schema. The sharing of information among constituents is also addressed, and a special export schema&mdash;the export data/knowledge/task schema&mdash;is proposed that ties together the objects, their constraints, and their usage rules. Knowledge sharing among constituents during cooperative query processing is accomplished using i>data/knowledge packets.'],\n",
       "   '2000']],\n",
       " [[['Dynamic Flexible Constraint Satisfaction.',\n",
       "    'Existing techniques for solving constraint satisfaction problems (CSPs) are largely concerned with a static set of imperative, inflexible constraints. Recently, work has addressed these shortcomings of classical constraint satisfaction in the form of two separate extensions known as flexible and dynamic CSP. Little, however, has been done to combine these two approaches in order to bring to bear the benefits of both in solving more complex problems. This paper presents a new integrated algorithm, Flexible Local Changes, for dynamic flexible problems. It is further shown how the use of flexible consistency-enforcing techniques can improve solution re-use and hence the efficiency of the core algorithm. Empirical evidence is provided to support the success of the present approach.'],\n",
       "   '2000']],\n",
       " [[['A Probabilistic Approach to Collaborative Multi-Robot Localization.',\n",
       "    \"This paper presents a statistical algorithm for collaborative mobile robot localization. Our approach uses a sample-based version of Markov localization, capable of localizing mobile robots in an any-time fashion. When teams of robots localize themselves in the same environment, probabilistic methods are employed to synchronize each robot's belief whenever one robot detects another. As a result, the robots localize themselves faster, maintain higher accuracy, and high-cost sensors are amortized across multiple robot platforms. The technique has been implemented and tested using two mobile robots equipped with cameras and laser range-finders for detecting other robots. The results, obtained with the real robots and in series of simulation runs, illustrate drastic improvements in localization speed and accuracy when compared to conventional single-robot localization. A further experiment demonstrates that under certain conditions, successful localization is only possible if teams of heterogeneous robots collaborate during localization.\"],\n",
       "   '2000']],\n",
       " [[['Multiagent Systems: A Survey from a Machine Learning Perspective.',\n",
       "    'Distributed Artificial Intelligence (DAI) has existed as a subfield of AI for less than two decades. DAI is concerned with systems that consist of multiple independent entities that interact in a domain. Traditionally, DAI has been divided into two sub-disciplines: Distributed Problem Solving (DPS) focuses on the information management aspects of systems with several components working together towards a common goal&semi; Multiagent Systems (MAS) deals with behavior management in collections of several independent entities, or agents. This survey of MAS is intended to serve as an introduction to the field and as an organizational framework. A series of general multiagent scenarios are presented. For each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. The presented techniques are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. When options exist, the techniques presented are biased towards machine learning approaches. Additional opportunities for applying machine learning to MAS are highlighted and robotic soccer is presented as an appropriate test bed for MAS. This survey does not focus exclusively on robotic systems. However, we believe that much of the prior research in non-robotic MAS is relevant to robotic MAS, and we explicitly discuss several robotic MAS, including all of those presented in this issue.'],\n",
       "   '2000']],\n",
       " [[['A Gesture Based Interface for Human-Robot Interaction.',\n",
       "    'Service robotics is currently a highly active research area in robotics, with enormous societal potential. Since service robots directly interact with people, finding &ldquo;natural&rdquo; and easy-to-use user interfaces is of fundamental importance. While past work has predominately focussed on issues such as navigation and manipulation, relatively few robotic systems are equipped with flexible user interfaces that permit controlling the robot by &ldquo;natural&rdquo; means. This paper describes a gesture interface for the control of a mobile robot equipped with a manipulator. The interface uses a camera to track a person and recognize gestures involving arm motion. A fast, adaptive tracking algorithm enables the robot to track and follow a person reliably through office environments with changing lighting conditions. Two alternative methods for gesture recognition are compared: a template based approach and a neural network approach. Both are combined with the Viterbi algorithm for the recognition of gestures defined through arm motion (in addition to static arm poses). Results are reported in the context of an interactive clean-up task, where a person guides the robot to specific locations that need to be cleaned and instructs the robot to pick up trash.'],\n",
       "   '2000']],\n",
       " [[['Identifying Objects in Procedural Programs Using Clustering Neural Networks.',\n",
       "    'This paper presents a general approach for the identification of objects in procedural programs. The approach is based on neural architectures that perform an unsupervised learning of clusters. We describe two such neural architectures, explain how to use them in identifying objects in software systems and briefly describe a prototype tool, which implements the clustering algorithms. With the aid of several examples, we explain how our approach can identify abstract data types as well as groups of routines which reference a common set of data. The clustering results are compared to the results of many other object identification techniques. Finally, several case studies were performed on existing programs to evaluate the object identification approach. Results concerning two representative programs and their generated clusters are discussed.'],\n",
       "   '2000']],\n",
       " [[['Specifying and Automatically Generating a Specialization Tool for Fortran 90.',\n",
       "    'Partial evaluation is an optimization technique traditionally used in compilation. We have adapted this technique to the understanding of scientific application programs during their maintenance. We have implemented a tool that analyzes Fortran 90 application programs and performs an interprocedural pointer analysis. This paper presents a dynamic semantics of Fortran 90 and manually derives a partial evaluator from this semantics. The tool implementing the specifications is also detailed. The partial evaluator has been implemented in a generic programming environment and a graphical interface has been developed to visualize the information computed during the partial evaluation (values of variables, already analyzed procedures, scope of variables, removed statements, etc.).'],\n",
       "   '2000']],\n",
       " [[['Specification-Based Browsing of Software Component Libraries.',\n",
       "    'Specification-based retrieval provides exact content-oriented access to component libraries but requires too much deductive power. Specification-based browsing evades this bottleneck by moving any deduction into an off-line indexing phase. In this paper, we show how match relations are used to build an appropriate index and how formal concept analysis is used to build a suitable navigation structure. This structure has the single-focus property (i.e., any sensible subset of a library is represented by a single node) and supports attribute-based (via explicit component properties) and object-based (via implicit component similarities) navigation styles. It thus combines the exact semantics of formal methods with the interactive navigation possibilities of informal methods. Experiments show that current theorem provers can solve enough of the emerging proof problems to make browsing feasible. The navigation structure also indicates situations where additional abstractions are required to build a better index and thus helps to understand and to re-engineer component libraries.'],\n",
       "   '2000']],\n",
       " [[['Automating Support for Software Evolution in UML.',\n",
       "    'Disciplined support for evolution of software artifacts is important in all phases of the software life-cycle. In order to achieve this support, a uniform underlying foundation for software evolution is necessary. While, in the past, reuse contracts have been proposed as such a formalism in a number of different domains, this paper generalises the formalism, and integrates it into the UML metamodel. As such, support for evolution becomes readily available for many kinds of UML models, ranging from requirements to the implementation phase.'],\n",
       "   '2000']],\n",
       " [[['Planning Proofs of Equations in CCS.',\n",
       "    'Most efforts to automate formal verification of communicating systems have centred around finite-state systems (FSSs). However, FSSs are incapable of modelling many practical communicating systems, including a novel class of problems, which we call VIPS. VIPSs are value-passing, infinite-state, parameterised systems. Existing approaches using model checking over FSSs are insufficient for VIPSs. This is due to their inability both to reason with and about domain-specific theories, and to cope with systems having an unbounded or arbitrary state space.We use the Calculus of Communicating Systems (CCS) (i>Communication and Concurrency. London: Prentice Hall, 1989) to express and specify VIPSs. We take i>program verification to be proving the program and its intended specification equivalent. We use the laws of CCS to conduct the verification task. This approach allows us to study communicating systems and the data such systems communicate. Automating theorem proving in this context is an extremely difficult task.We provide automated methods for CCS analysis&semi; they are applicable to both FSSs and VIPSs. Adding these methods to the i>CLi>Ai>M proof planner (Lecture Notes in Artificial Intelligence, Vol. 449, Springer, 1990, pp. 647, 648), we have implemented an automated verification planner capable of dealing with problems that previously required human interaction. This paper describes these methods, gives an account as to why they work, and provides a short summary of experimental results.'],\n",
       "   '2000']],\n",
       " [[['A Case Study on Applying a Tool for Automated System Analysis Based on Modular Specifications Written in TRIO.',\n",
       "    'An effective means for analyzing and reasoning on software systems is to use formal specifications to simulate their execution. The simulation traces can be used for specification testing and reused for functional testing of the system later in the development process. It is widely acknowledged that, to deal with the complexity of industrial-size systems, specifications must be structured into modules providing abstraction mechanisms and clear interfaces. In our past work, we defined and implemented a method for simulating specifications written in the TRIO temporal logic language, and applied it to functional testing of time-critical industrial systems. In the present paper, we report on a case study with a tool that analyzes TRIO specifications by taking advantage of their modular structure, so as to overcome the well-known state-explosion problem and make the proposed method really scalable. We discuss the fundamental operations and the algorithms on which the tool is based. Then, we illustrate its use in a realistic case study, inspired from an industrial application. Finally, we comment on the overall results in terms of usability of the tool and effectiveness of the approach, and we outline future improvements.'],\n",
       "   '2000']],\n",
       " [[['Executing Formal Specifications with Concurrent Constraint Programming.',\n",
       "    'We have implemented a technique for execution of formal, model-based specifications. The specifications we can execute are written at a level of abstraction that is close to that used in nonexecutable specifications. The specification abstractions supported by our execution technique include using quantified assertions to directly construct post-state values, and indirect definitions of post-state values (definitions that do not use equality). Our approach is based on translating specifications to the concurrent constraint programming language AKL. While there are, of course, expressible assertions that are not executable, our technique is amenable to any formal specification language based on a finite number of intrinsic types and pre- and postcondition assertions.'],\n",
       "   '2000']],\n",
       " [[['Immersive VR for Scientific Visualization: A Progress Report.',\n",
       "    'Immersive virtual reality can provide powerful techniques for scientific visualization. The research agenda for the technology sketched here offers a progress report, a hope, and a call to action.'],\n",
       "   '2000']],\n",
       " [[['Metamorphosis of Arbitrary Triangular Meshes.',\n",
       "    'In this article we present an efficient framework for morphing between two topologically equivalent, arbitrary meshes with the user controlling the surface correspondences. Each of the partitioned meshes is embedded into a polygonal region on the plane with harmonic mapping. Those embedded meshes have the same graph structure as their original meshes. By overlapping those two embedded meshes, we can establish correspondence between them. Metamorphosis results from interpolating the corresponding vertices from one mesh to the other. We demonstrate that minimal control of surface correspondences by the user generates sophisticated interpolation between two meshes.'],\n",
       "   '2000']],\n",
       " [[['Toward Spontaneous Interaction with the Perceptive Workbench.',\n",
       "    'The Perceptive Workbench works toward a spontaneous and unimpeded interface between the physical and virtual worlds. Its vision-based methods for interaction eliminate the need for wired input devices and wired tracking.'],\n",
       "   '2000']],\n",
       " [[['Combining Active and Passive Simulations for Secondary Motion.',\n",
       "    'Objects that move in response to the actions of a main character often make an important contribution to the visual richness of an animated scene. We use the term \"secondary motion\" to refer to passive motions generated in response to the movements of characters and other objects or environmental forces. Secondary motions aren\\'t normally the mail focus of an animated scene, yet their absence can distract or disturb the viewer, destroying the illusion of reality created by the scene. We describe how to generate secondary motion by coupling physically based simulations of passive objects to actively controlled characters.'],\n",
       "   '2000']],\n",
       " [[['A comparative study of online scheduling algorithms for networks of workstations.',\n",
       "    'Networks of workstations offer large amounts of unused processing time. Resource management systems are able to exploit this computing capacity by assigning compute&hyphen;intensive tasks to idle workstations. To avoid interferences between multiple, concurrently running applications, such resource management systems have to schedule application jobs carefully. Continuously arriving jobs and dynamically changing amounts of available CPU capacity make traditional scheduling algorithms difficult to apply in workstation networks. Online scheduling algorithms promise better results by adapting schedules to changing situations. This paper compares six online scheduling algorithms by simulating several workload scenarios. Based on the insights gained by simulation, the three online scheduling algorithms performing best were implemented in the Winner resource management system. Experiments conducted with Winner in a real workstation network confirm the simulation results obtained.'],\n",
       "   '2000']],\n",
       " [[['Host load prediction using linear models.',\n",
       "    'This paper evaluates linear models for predicting the Digital Unix five-second host load average from 1 to 30 seconds into the future. A detailed statistical study of a large number of long, fine grain load traces from a variety of real machines leads to consideration of the Box&ndash;Jenkins models (AR, MA, ARMA, ARIMA), and the ARFIMA models (due to self-similarity.) We also consider a simple windowed-mean model. The computational requirements of these models span a wide range, making some more practical than others for incorporation into an online prediction system. We rigorously evaluate the predictive power of the models by running a large number of randomized testcases on the load traces and then data-mining their results. The main conclusions are that load is consistently predictable to a very useful degree, and that the simple, practical models such as AR are sufficient for host load prediction. We recommend AR(16) models or better for host load prediction. We implement an online host load prediction system around the AR(16) model and evaluate its overhead, finding that it uses miniscule amounts of CPU time and network bandwidth.'],\n",
       "   '2000']],\n",
       " [[['Heterogeneous process state capture and recovery through Process Introspection.',\n",
       "    'The ability to capture the state of a process and later recover that state in the form of an equivalent running process is the basis for a number of important features in parallel and distributed systems. Adaptive load sharing and fault tolerance are well&hyphen;known examples. Traditional state capture mechanisms have employed an external agent (such as the operating system kernel) to examine and capture process state. However, the increasing prevalence of heterogeneous cluster and &ldquo;metacomputing&rdquo; systems as high&hyphen;performance computing platforms has prompted investigation of process&hyphen;internal state capture mechanisms. Perhaps the greatest advantage of the process&hyphen;internal approach is the ability to support cross&hyphen;platform state capture and recovery, an important feature in heterogeneous environments. Among the perceived disadvantages of existing process&hyphen;internal mechanisms are poor performance in multiple respects, and difficulty of use in terms of programmer effort. In this paper we describe a new process&hyphen;internal state capture and recovery mechanism: Process Introspection. Experiences with this system indicate that the perceived disadvantages associated with process&hyphen;internal mechanisms can be largely overcome, making this approach to state capture an appropriate one for cluster and metacomputing environments.'],\n",
       "   '2000']],\n",
       " [[['Supporting high-performance I/O in QoS-enabled ORB middleware.',\n",
       "    'To be an effective platform for high-performance distributed applications, off-the-shelf Object Request Broker (ORB) middleware, such as CORBA, must preserve communication-layer quality of service (QoS) properties both vertically (i.e., network interface${}\\\\leftrightarrow{}$application layer) and horizontally (i.e., end-to-end). However, conventional network interfaces, I/O subsystems, and middleware interoperability protocols are not well-suited for applications that possess stringent throughput, latency, and jitter requirements. It is essential, therefore, to develop vertically and horizontally integrated ORB endsystems that can be (1) configured flexibly to support high-performance network interfaces and I/O subsystems and (2) used transparently by performance-sensitive applications. This paper provides three contributions to research on high-performance I/O support for QoS-enabled ORB middleware. First, we outline the key research challenges faced by high-performance ORB endsystem developers. Second, we describe how our real-time I/O (RIO) subsystem and pluggable protocol framework enables ORB endsystems to preserve high-performance network interface QoS up to applications running on off-the-shelf hardware and software. Third, we illustrate empirically how highly optimized ORB middleware can be integrated with real-time I/O subsystem to reduce latency bounds on communication between high-priority clients without unduly penalizing low-priority and best-effort clients. Our results demonstrate how it is possible to develop ORB endsystems that are both highly flexible and highly efficient.'],\n",
       "   '2000']],\n",
       " [[['Direct queries for discovering network resource properties in a distributed environment.',\n",
       "    'The development and performance of network-aware applications depends on the availability of accurate predictions of network resource properties. Obtaining this information directly from the network is a scalable solution that provides the accurate performance predictions and topology information needed for planning and adapting application behavior across a variety of networks. The performance predictions obtained directly from the network are as accurate as application-level benchmarks, but the network-based technique provides the added advantages of scalability and topology discovery. We describe how to determine network properties directly from the network using SNMP. We provide an overview of SNMP and describe the features it provides that make it possible to extract both available bandwidth and network topology information from network devices. The available bandwidth predictions based on network queries using SNMP are compared with traditional predictions based on application history to demonstrate that they are equally useful. To demonstrate the feasibility of topology discovery, we present results for a large Ethernet LAN.'],\n",
       "   '2000']],\n",
       " [[['The Rhetorical Parsing of Unrestricted Texts: A Surface-Based Approach.',\n",
       "    'Coherent texts are not just simple sequences of clauses and sentences, but rather complex artifacts that have highly elaborate rhetorical structure. This paper explores the extent to which well-formed rhetorical structures can be automatically derived by means of surface-form-based algorithms. These algorithms identify discourse usages of cue phrases and break sentences into clauses, hypothesize rhetorical relations that hold among textual units, and produce valid rhetorical structure trees for unrestricted natural language texts. The algorithms are empirically grounded in a corpus analysis of cue phrases and rely on a first-order formalization of rhetorical structure trees.The algorithms are evaluated both intrinsically and extrinsically. The intrinsic evaluation assesses the resemblance between automatically and manually constructed rhetorical structure trees. The extrinsic evaluation shows that automatically derived rhetorical structures can be successfully exploited in the context of text summarization.'],\n",
       "   '2000']],\n",
       " [[['Models of Translational Equivalence among Words.',\n",
       "    'Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data. First, most words translate to only one other word. Second, bitext correspondence is typically only partial---many words in each text have no clear equivalent in the other text. This article presents methods for biasing statistical translation models to reflect these properties. Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model. This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs. Even the simplest kinds of language-specific knowledge, such as the distinction between content words and function words, are shown to reliably boost translation model performance on some tasks. Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms.'],\n",
       "   '2000']],\n",
       " [[['Practical Experiments with Regular Approximation of Context-free Languages.',\n",
       "    'Several methods are discussed that construct a finite automaton given a context-free grammar, including both methods that lead to subsets and those that lead to supersets of the original context-free language. Some of these methods of regular approximation are new, and some others are presented here in a more refined form with respect to existing literature. Practical experiments with the different methods of regular approximation are performed for spoken-language input: hypotheses from a speech recognizer are filtered through a finite automaton.'],\n",
       "   '2000']],\n",
       " [[['Learning Methods to Combine Linguistic Indicators: Improving Aspectual Classification and Revealing Linguistic Insights.',\n",
       "    \"Aspectual classification maps verbs to a small set of primitive categories in order to reason about time. This classification is necessary for interpreting temporal modifiers and assessing temporal relationships, and is therefore a required component for many natural language applications.A verb's aspectual category can be predicted by co-occurrence frequencies between the verb and certain linguistic modifiers. These frequency measures, called linguistic indicators, are chosen by linguistic insights. However, linguistic indicators used in isolation are predictively incomplete, and are therefore insufficient when used individually.In this article, we compare three supervised machine learning methods for combining multiple linguistic indicators for aspectual classification: decision trees, genetic programming, and logistic regression. A set of 14 indicators are combined for classification according to two aspectual distinctions. This approach improves the classification performance for both distinctions, as evaluated over unrestricted sets of verbs occurring across two corpora. This demonstrates the effectiveness of the linguistic indicators and provides a much-needed full-scale method for automatic aspectual classification. Moreover, the models resulting from learning reveal several linguistic insights that are relevant to aspectual classification. We also compare supervised learning methods with an unsupervised method for this task.\"],\n",
       "   '2000']],\n",
       " [[['A Model for Multimodal Reference Resolution.',\n",
       "    'An important aspect of the interpretation of multimodal messages is the ability to identify when the same object in the world is the referent of symbols in different modalities. To understand the caption of a picture, for instance, one needs to identify the graphical symbols that are referred to by names and pronouns in the natural language text. One way to think of this problem is in terms of the notion of anaphora; however, unlike linguistic anaphoric inference, in which antecedents for pronouns are selected from a linguistic context, in the interpretation of the textual part of multimodal messages the antecedents are selected from a graphical context. Under this view, resolving multimodal references is like resolving anaphora across modalities. Another way to see the same problem is to look at pronouns in texts about drawings as deictic. In this second view, the context of interpretation of a natural language term is defined as a set of expressions of a graphical language with well-defined syntax and semantics. Natural language and graphical terms are thought of as standing in a relation of translation similar to the translation relation that holds between natural languages. In this paper a theory based on this second view is presented. In this theory, the relations between multimodal representation and spatial deixis, on the one hand, and multimodal reasoning and deictic inference, on the other, are discussed. An integrated model of anaphoric and deictic resolution in the context of the interpretation of multimodal discourse is also advanced.'],\n",
       "   '2000']],\n",
       " [[['An Empirically-based System for Processing Definite Descriptions.',\n",
       "    'We present an implemented system for processing definite descriptions in arbitrary domains. The design of the system is based on the results of a corpus analysis previously reported, which highlighted the prevalence of discourse-new descriptions in newspaper corpora. The annotated corpus was used to extensively evaluate the proposed techniques for matching definite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchors for bridging descriptions.'],\n",
       "   '2000']],\n",
       " [[['Using Paths to Measure, Explain, and Enhance Program Behavior.',\n",
       "    \"What happens when a computer program runs? The answer can be frustratingly elusive, as anyone who has debugged or tuned a program knows. As it runs, a program overwrites its previous state, which might have provided a clue as to how the program got to the point at which it computed the wrong answer or otherwise failed. This all-too-common experience is symptomatic of a more general problem: the difficulty of accurately and efficiently capturing and analyzing the sequence of events that occur when a program executes. Program paths offer an insight into a program's dynamic behavior that is difficult to achieve any other way. Unlike simpler measures such as program profiles, which aggregate information to reduce the cost of collecting or storing data, paths capture some of the usually invisible dynamic sequencing of statements. This work exploits the insight that program statements do not execute in isolation, but are typically correlated with the behavior of previously executed code.\"],\n",
       "   '2000']],\n",
       " [[['The Real-Time Specification for Java.',\n",
       "    \"New languages, programming disciplines, operating systems, and software engineering techniques sometimes hold considerable potential for real-time software developers. A promising area of interest-but one fairly new to the real-time community-is object-oriented programming. Java, for example, draws heavily from object orientation and is highly suitable for extension to real-time and embedded systems.Recognizing this fit between Java and real-time software development, the Real-Time for Java Experts Group (RTJEG) began developing the real-time specification for Java (RTSJ) in March 1999 under the Java Community Process. This article explains RTSJ's features and the thinking behind the specification's design. The goal of the RTJEG, of which the authors are both members, was to provide a platform-a Java execution environment and application program interface (API)- that lets programmers correctly reason about the temporal behavior of executing software.\"],\n",
       "   '2000']],\n",
       " [[[\"What's Ahead for Embedded Software?\",\n",
       "    \"Most of today's gadgets and cars use embedded software, which in many cases has taken over what mechanical and dedicated electronic systems used to do. Indeed, embedded software appears in everything from telephones and pagers to systems for medical diagnostics, climate control, and manufacturing. The author believes that research computer scientists have largely ignored embedded software because it has not been sufficiently complex or general to warrant the effort. There are many re-search questions but most center around one issue: how to reconcile a set of domainspecific requirements with the demands of interaction in the physical world. How do you adapt software abstractions designed merely to transform data to meet requirements like realtime constraints, concurrency, and stringent safety considerations? The answer to this question has given rise to some promising research angles discussed in this article, including novel ways to deal with concurrency and real time, and methods for augmenting component interfaces to promote safety and adaptability.\"],\n",
       "   '2000']],\n",
       " [[['The Garp Architecture and C Compiler.',\n",
       "    \"Various projects and products have been built using off-the-shelf field-programmable gate arrays (FPGAs) as computation accelerators for specific tasks. Such systems typically connect one or more FPGAs to the host computer via an I/O bus. Some have shown remarkable speedups, albeit limited to specific application domains.Many factors limit the general usefulness of such systems. Long reconfiguration times prevent acceleration of applications that spread their time over many different tasks. Low-bandwidth paths for data transfer limit the usefulness of such systems to tasks that have a high compute-to-memory-bandwidth ratio. In addition, standard FPGA tools require hardware design expertise beyond the knowledge of most programmers.To help investigate the viability of connected FPGA systems, the authors designed their own architecture called Garp and experimented with running applications on it. They are also investigating whether Garp's design enables automatic, fast, effective compilation across a broad range of applications. They present their results in this article.\"],\n",
       "   '2000']],\n",
       " [[['Component-Based Systems: A Classification of Issues.',\n",
       "    'Developing and using various component forms as building blocks can significantly enhance software-based system development and use. The authors describe software components as units of independent production, acquisition, and deployment that interact to form a functional system.Both the academic and commercial sectors have devoted considerable effort to defining and describing the terms and concepts involved in component-based software development.The component-based systems approach could potentially overcome difficulties associated with developing and maintaining monolithic software applications. The authors believe that this approach should result in better quality products, rapid development, and an in-creased capability to accommodate change.The authors identify a set of issues within an overall framework that software developers must address for component-based systems to achieve their full potential. They contend that using this framework leads to a more effective understanding of components because it helps clarify aspects of the component concept that are largely independent of architectural and implementation issues.'],\n",
       "   '2000']],\n",
       " [[['Advances in Network Simulation.',\n",
       "    \"The Internet's rapid growth has spurred development of new protocols and algorithms to meet changing operational requirements-- such as security, multicast transport, mobile networking, policy management, and quality-of-service support. Development and evaluation of these operational tools requires answering many design questions.Despite their value, custom simulators, wide-area test-beds, and small-scale lab evaluations all have drawbacks. Because they use real code, experiments run in testbeds or labs automatically capture important details that might be missed in a simulation. However, building testbeds and labs is expensive, reconfiguring and sharing them is difficult, and they are relatively inflexible.In the current paradigm, directly comparable data would be available only if designers implemented all competing mechanisms within every simulator.The Virtual InterNetwork Testbed (VINT) project, detailed and explained in this article, provides improved simulation tools for network researchers to use in the design and deployment of new wide-area Internet protocols.\"],\n",
       "   '2000']],\n",
       " [[['Cellular Processing Tools for High-Performance Simulation.',\n",
       "    'Recently, computational simulation has become a third approach along with theory and laboratory simulation to studying and solving scientific problems. In this approach, a computer equipped with problem solving software tools may represent a virtual laboratory in which researchers can build a model for a given problem and run it under varying conditions. These increasingly complex computational methodologies require sophisticated models and techniques, and vice versa. The authors explain how developing and validating complex models will increasingly depend on significant advances in experimental and testing techniques. High performance parallel computers gave researchers the ability to implement inherently parallel techniques such as cellular automata (CA), neural networks, and genetic algorithms significant new mathematical models for describing complex scientific phenomena. This article explains how cellular automata offer a powerful modeling approach for complex systems in which global behavior arises from the collective effect of many locally interacting, simple components.'],\n",
       "   '2000']],\n",
       " [[['Survivable Information Storage Systems.',\n",
       "    'As society increasingly relies on digitally stored and accessed information, supporting the availability, integrity, and confidentiality of this information is crucial. Systems need to help users securely store critical information, ensuring that it persists, is continuously accessible, cannot be destroyed, and is kept confidential. The authors show why they believe a survivable storage system would provide these guarantees, despite malicious compromises of storage node subsets.This article describes the PASIS architecture, which combines decentralized storage system technologies, data redundancy and encoding, and dynamic self-maintenance to create survivable information storage.The authors explain how the PASIS architecture flexibly and efficiently combines technologies for constructing in-formation storage systems whose availability, confidentiality, and integrity policies can survive component failures and malicious attacks.'],\n",
       "   '2000']],\n",
       " [[['Compression: A Key for Next-Generation Text Retrieval Systems.',\n",
       "    \"As online textual information explodes through the widespread use of digital libraries, office automation systems, document databases, and the Web, the need arises for an effective information retrieval (IR) system. The Web alone comprises approximately 800 million static pages, containing 6 trillion bytes of plain text--enough to store the text of a million books. Today's IR systems face the dynamic challenge of providing rapid and immediate access to this textual mass.Recent methods have demonstrated that directly searching compressed text is faster than searching original text and that flexible word searching improves the amount of compression obtained.Text compression focuses on finding ways to represent actual text in less space. This process involves replacing text symbols with equivalent symbols that use fewer bits or bytes. Text compression is attractive because it is cost efficient, requires less storage space, speeds up data transmittal, and reduces search time.The authors discuss the recent techniques that allow fast and direct searching of compressed text, and they explain how these techniques can improve the overall efficiency of IR systems.\"],\n",
       "   '2000']],\n",
       " [[['Generalized Definite Set Constraints.',\n",
       "    'Set constraints (SC) are logical formul\\\\ae in which atoms are inclusions between set expressions. Those set expressions are built over a signature \\\\Sigma, variables and various set operators. On a semantical point of view, the set constraints are interpreted over sets of trees built from \\\\Sigma and the inclusion symbol is interpreted as the subset relation over those sets. By restricting the syntax of those formul\\\\ae and/or the set of operators that can occur in set expressions, different classes of set constraints are obtained. Several classes have been proposed and studied for some problems such as satisfiability and entailment. Among those classes, we focus in this article on the class of definite SC&lsquo;s introduced by Heintze and Jaffar, and the class of co-definite SC&lsquo;s studied by Charatonik and Podelski. In spite of their name, those two classes are not dual from each other, neither through inclusion inversion nor through complementation. In this article, we propose an extension for each of those two classes by means of an intentional set construction, so called membership expression. A membership expression is an expression \\\\{x\\\\mid \\\\Phi(x)\\\\}. The formula \\\\Phi(x) is a positive first-order formula built from membership atoms t \\\\in S in which S is a set expression. We name those two classes respectively generalized definite and generalized co-definite set constraints. One of the main point concerning those so-extended classes is that the two generalized classes turn out to be dual through complementation. First, we prove in this article that generalized definite set constraints is a proper extension of the definite class, as it is more expressive in terms of sets of solutions. But we show also that those extensions preserve some main properties of the definite and co-definite class. Hence for instance, as definite set constraints, generalized definite SC&lsquo;s have a least solution whereas the generalized co-definite SC&lsquo;s have a greatest solution, just as co-definite ones. Furthermore, we devise an algorithm based on tree automata that solves the satisfiability problem for generalized definite set constraints. Due to the dualization, the algorithm solves the satisfiability problem for generalized co-definite set constraints as well. This algorithm proves first that for those generalized classes, the satisfiability problem remains DEXPTIME-complete. It provides also a proof for regularity of the least solution of generalized definite constraints and so, by dualization for the greatest solution for the generalized co-definite SC&lsquo;s.'],\n",
       "   '2000']],\n",
       " [[['Representation and Reasoning with Multi-Point Events.',\n",
       "    'Allen&lsquo;s Interval Algebra (IA) and Vilain & Kautz&lsquo;s Point Algebra (PA) consider an interval and a point as basic temporal entities (i.e., events) respectively. However, in many situations we need to deal with recurring events that include multiple points, multiple intervals or combinations of points and intervals. In this paper, we present a framework to model recurring events as multi-point events (MPEs) by extending point algebra. The reasoning tasks are formulated as binary constraint satisfaction problems. We propose a polynomial time algorithm (based on van Beek&lsquo;s algorithm) for finding all feasible relations. For the problem of finding a consistent scenario, we propose a backtracking method with a local search heuristic. We also describe an implementation and a detail empirical evaluation of the proposed algorithms. Our empirical results indicate that the MPE-based approach performs better than the existing approaches.'],\n",
       "   '2000']],\n",
       " [[['SLT-Resolution for the Well-Founded Semantics',\n",
       "    'Global SLS-resolution and SLG-resolution are two representative mechanisms for top-down evaluation of the well-founded semantics of general logic programs. Global SLS-resolution is linear for query evaluation but suffers from infinite loops and redundant computations. In contrast, SLG-resolution resolves infinite loops and redundant computations by means of tabling, but it is not linear. The principal disadvantage of a nonlinear approach is that it cannot be implemented by using a simple, efficient stack-based memory structure nor can it be easily extended to handle some strictly sequential operators such as cuts in Prolog. In this paper, we present a linear tabling method, called i>SLT-resolution, for top-down evaluation of the well-founded semantics. SLT-resolution is a substantial extension of SLDNF-resolution with tabling. Its main features are the following. First, it resolves infinite loops and redundant computations while preserving the linearity. Second, it is terminating and is sound and complete w.r.t. the well-founded semantics for programs with the bounded-term-size property with nonfloundering queries. Its time complexity is comparable with SLG-resolution and polynomial for function-free logic programs. Third, because of its linearity for query evaluation, SLT-resolution bridges the gap between the well-founded semantics and standard Prolog implementation techniques. It can be implemented by an extension to any existing Prolog abstract machines such as WAM or ATOAM.'],\n",
       "   '2000']],\n",
       " [[['Linear Tabulated Resolution Based on Prolog Control Strategy',\n",
       "    'Infinite loops and redundant computations are long recognized open problems in Prolog. Two methods have been explored to resolve these problems: loop checking and tabling. Loop checking can cut infinite loops, but it cannot be both sound and complete even for function-free logic programs. Tabling seems to be an effective way to resolve infinite loops and redundant computations. However, existing tabulated resolutions, such as OLDT-resolution, SLG-resolution and Tabulated SLS-resolution, are non-linear because they rely on the solution-lookup mode in formulating tabling. The principal disadvantage of non-linear resolutions is that they cannot be implemented using a simple stack-based memory structure like that in Prolog. Moreover, some strictly sequential operators such as cuts may not be handled as easily as in Prolog. In this paper, we propose a hybrid method to resolve infinite loops and redundant computations. We combine the ideas of loop checking and tabling to establish a linear tabulated resolution called TP-resolution. TP-resolution has two distinctive features: (1) it makes linear tabulated derivations in the same way as Prolog except that infinite loops are broken and redundant computations are reduced. It handles cuts as effectively as Prolog; and (2) it is sound and complete for positive logic programs with the bounded-term-size property. The underlying algorithm can be implemented by an extension to any existing Prolog abstract machines such as WAM or ATOAM.'],\n",
       "   '2000']],\n",
       " [[['Constraint Programming viewed as Rule-based Programming',\n",
       "    \"We study here a natural situation when constraint programming can be entirely reduced to rule-based programming. To this end we explain first how one can compute on constraint satisfaction problems using rules represented by simple first-order formulas. Then we consider constraint satisfaction problems that are based on predefined, explicitly given constraints. To solve them we first derive rules from these explicitly given constraints and limit the computation process to a repeated application of these rules, combined with labeling. We consider two types of rule here. The first type, that we call equality rules, leads to a new notion of local consistency, called rule consistency that turns out to be weaker than arc consistency for constraints of arbitrary arity (called hyper-arc consistency in Marriott & Stuckey (1998)). For Boolean constraints rule consistency coincides with the closure under the well-known propagation rules for Boolean constraints. The second type of rules, that we call membership rules, yields a rule-based characterization of arc consistency. To show feasibility of this rule-based approach to constraint programming, we show how both types of rules can be automatically generated, as CHR rules of Fr&uuml;hwirth (1995). This yields an implementation of this approach to programming by means of constraint logic programming. We illustrate the usefulness of this approach to constraint programming by discussing various examples, including Boolean constraints, two typical examples of many valued logics, constraints dealing with Waltz's language for describing polyhedral scenes, and Allen's qualitative approach to temporal logic.\"],\n",
       "   '2000']],\n",
       " [[['Interval Constraint Solving for Camera Control and Motion Planning',\n",
       "    'Many problems in robust control and motion planning can be reduced to either finding a sound approximation of the solution space determined by a set of nonlinear inequalities, or to the \"guaranteed tuning problem\" as defined by Jaulin and Walter, which amounts to finding a value for some tuning parameter such that a set of inequalities be verified for all the possible values of some perturbation vector. A classical approach to solving these problems, which satisfies the strong soundness requirement, involves some quantifier elimination procedure such as Collins\\' Cylindrical Algebraic Decomposition symbolic method. Sound numerical methods using interval arithmetic and local consistency enforcement to prune the search space are presented in this article as much faster alternatives for both soundly solving systems of nonlinear inequalities, and addressing the guaranteed tuning problem whenever the perturbation vector has dimension 1. The use of these methods in camera control is investigated, and experiments with the prototype of a declarative modeler to express camera motion using a cinematic language are reported and commented upon.'],\n",
       "   '2000']],\n",
       " [[['Towards a Theory of Cache-Efficient Algorithms',\n",
       "    \"We present a model that enables us to analyze the running time of an algorithm on a computer with a memory hierarchy with limited associativity, in terms of various cache parameters. Our cache model, an extension of Aggarwal and Vitter's I/O model, enables us to establish useful relationships between the cache complexity and the I/O complexity of computations. As a corollary, we obtain cache-efficient algorithms in the single-level cache model for fundamental problems like sorting, FFT, and an important subclass of permutations. We also analyze the average-case cache behavior of mergesort, show that ignoring associativity concerns could lead to inferior performance, and present supporting experimental evidence.We further extend our model to multiple levels of cache with limited associativity and present optimal algorithms for matrix transpose and sorting. Our techniques may be used for systematic exploitation of the memory hierarchy starting from the algorithm design stage, and for dealing with the hitherto unresolved problem of limited associativity.\"],\n",
       "   '2000']],\n",
       " [[['Advances in domain independent linear text segmentation',\n",
       "    'This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering.'],\n",
       "   '2000']],\n",
       " [[['An Experimental Comparison of Naive Bayesian and Keyword-Based Anti-Spam Filtering with Personal E-mail Messages',\n",
       "    'The growing problem of unsolicited bulk e-mail, also known as &ldquo;spam&rdquo;, has generated a need for reliable anti-spam e-mail filters. Filters of this type have so far been based mostly on manually constructed keyword patterns. An alternative approach has recently been proposed, whereby a Naive Bayesian classifier is trained automatically to detect spam messages. We test this approach on a large collection of personal e-mail messages, which we make publicly available in &ldquo;encrypted&rdquo; form contributing towards standard benchmarks. We introduce appropriate cost-sensitive measures, investigating at the same time the effect of attribute-set size, training-corpus size, lemmatization, and stop lists, issues that have not been explored in previous experiments. Finally, the Naive Bayesian filter is compared, in terms of performance, to a filter that uses keyword patterns, and which is part of a widely used e-mail reader.'],\n",
       "   '2000']],\n",
       " [[['Automatic Extraction of Subcategorization Frames for Czech',\n",
       "    'We present some novel machine learning techniques for the identification of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we are able to achieve 88% precision on unseen parsed text.'],\n",
       "   '2000']],\n",
       " [[['Parsing with the Shortest Derivation',\n",
       "    'Common wisdom has it that the bias of stochastic grammars in favor of shorter derivations of a sentence is harmful and should be redressed. We show that the common wisdom is wrong for stochastic grammars that use elementary trees instead of context-free rules, such as Stochastic Tree-Substitution Grammars used by Data-Oriented Parsing models. For such grammars a non-probabilistic metric based on the shortest derivation outperforms a probabilistic metric on the ATIS and OVIS corpora, while it obtains competitive results on the Wall Street Journal (WSJ) corpus. This paper also contains the first published experiments with DOP on the WSJ.'],\n",
       "   '2000']],\n",
       " [[['A Classification Approach to Word Prediction',\n",
       "    'The eventual goal of a language model is to curately predict the value of a missing word given its context. We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context. This approach raises a few new questions that we address. First, in order to learn good word representations it is necessary to use an expressive representation of the context. We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction. Second, since the number of words \"competing\" for each prediction is large, there is a need to \"focus the attention\" on a smaller subset of these. We exhibit the contribution of a \"focus of attention\" mechanism to the performance of the word predictor. Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks.'],\n",
       "   '2000']],\n",
       " [[['Do All Fragments Count?',\n",
       "    'We aim at finding the minimal set of fragments that achieves maximal parse accuracy in Data Oriented Parsing (DOP). Experiments with the Penn Wall Street Journal (WSJ) treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank. We isolate a number of dependency relations which previous models neglect but which contribute to higher accuracy. We show that the history of statistical parsing models displays a tendency towards using more and larger fragments from training data.'],\n",
       "   '2000']],\n",
       " [[['The Random Oracle Methodology, Revisited',\n",
       "    'We take a critical look at the relationship between the security of cryptographic schemes in the Random Oracle Model, and the security of the schemes that result from implementing the random oracle by so called \"cryptographic hash functions\".The main result of this article is a negative one: There exist signature and encryption schemes that are secure in the Random Oracle Model, but for which any implementation of the random oracle results in insecure schemes. In the process of devising the above schemes, we consider possible definitions for the notion of a \"good implementation\" of a random oracle, pointing out limitations and challenges.'],\n",
       "   '2000']],\n",
       " [[['Managing Periodically Updated Data in Relational Databases: A Stochastic Modeling Approach',\n",
       "    'Recent trends in information management involve the periodic transcription of data onto secondary devices in a networked environment, and the proper scheduling of these transcriptions is critical for efficient data management. To assist in the scheduling process, we are interested in modeling data obsolescence, that is, the reduction of consistency over time between a relation and its replica. The modeling is based on techniques from the field of stochastic processes, and provides several stochastic models for content evolution in the base relations of a database, taking referential integrity constraints into account. These models are general enough to accommodate most of the common scenarios in databases, including batch insertions and lifespans both with and without memory. As an initial \"proof of concept\" of the applicability of our approach, we validate the insertion portion of our model framework via experiments with real data feeds. We also discuss a set of transcription protocols that make use of the proposed stochastic model.'],\n",
       "   '2000']],\n",
       " [[['A note on knowledge-based programs and specifications',\n",
       "    'Knowledge-based program are programs with explicit tests for knowledge. They have been used successfully in a number of applications. Sanders has pointed out what seem to be a counterintuitive property of knowledge-based programs. Roughly speaking, they do not satisfy a certain monotonicity property, while standard programs (ones without tests for knowledge) do. It is shown that there are two ways of defining the monotonicity property, which agree for standard programs. Knowledge-based programs satisfy the first, but do not satisfy the second. It is further argued by example that the fact that they do not satisfy the second is actually a feature, not a problem. Moreover, once we allow the more general class of knowledge-based specifications, standard programs do not satisfy the monotonicity property either.'],\n",
       "   '2000']],\n",
       " [[['Phase Clocks for Transient Fault Repair',\n",
       "    'Phase clocks are synchronization tools that implement a form of logical time in distributed systems. For systems tolerating transient faults by self-repair of damaged data, phase clocks can enable reasoning about the progress of distributed repair procedures. This paper presents a phase clock algorithm suited to the model of transient memory faults in asynchronous systems with read/write registers. The algorithm is self-stabilizing and guarantees accuracy of phase clocks within $O(k)$ time following an initial state that is k-faulty. Composition theorems show how the algorithm can be used for the timing of distributed procedures that repair system outputs.'],\n",
       "   '2000']],\n",
       " [[['Deciding first-order properties of locally tree-decomposable structures',\n",
       "    'We introduce the concept of a class of graphs, or more generally, relational structures, being locally tree-decomposable. There are numerous examples of locally tree-decomposable classes, among them the class of planar graphs and all classes of bounded valence or of bounded tree-width. We also consider a slightly more general concept of a class of structures having bounded local tree-width.We show that for each property &phi; of structures that is definable in first-order logic and for each locally tree-decomposable class C of structures, there is a linear time algorithm deciding whether a given structure A &isin; C has property &phi;. For classes C of bounded local tree-width, we show that for every k &ge; 1 there is an algorithm solving the same problem in time O(n1+(1/k)) (where n is the cardinality of the input structure).'],\n",
       "   '2000']],\n",
       " [[['All Pairs Shortest Paths using Bridging Sets and Rectangular Matrix Multiplication',\n",
       "    'We present two new algorithms for solving the All Pairs Shortest Paths (APSP) problem for weighted directed graphs. Both algorithms use fast matrix multiplication algorithms.The first algorithm solves the APSP problem for weighted directed graphs in which the edge weights are integers of small absolute value in &Otilde;(n2+&mu;) time, where &mu; satisfies the equation &omega;(1, &mu;, 1) = 1 + 2&mu; and &omega;(1, &mu;, 1) is the exponent of the multiplication of an n &times; n&mu; matrix by an n&mu; &times; n matrix. Currently, the best available bounds on &omega;(1, &mu;, 1), obtained by Coppersmith, imply that &mu; < 0.575. The running time of our algorithm is therefore O(n2.575). Our algorithm improves on the &Otilede;(n(3c+&omega;)/2) time algorithm, where &omega; = &omega;(1, 1, 1) < 2.376 is the usual exponent of matrix multiplication, obtained by Alon et al., whose running time is only known to be O(n2.688).The second algorithm solves the APSP problem almost exactly for directed graphs with arbitrary nonnegative real weights. The algorithm runs in &Otilde;((n&omega;/&epsis;) log(W/&epsis;)) time, where &epsis; > 0 is an error parameter and W is the largest edge weight in the graph, after the edge weights are scaled so that the smallest non-zero edge weight in the graph is 1. It returns estimates of all the distances in the graph with a stretch of at most 1 + &epsis;. Corresponding paths can also be found efficiently.'],\n",
       "   '2000']],\n",
       " [[['Robust Classification for Imprecise Environments',\n",
       "    'In real-world environments it usually is difficult to specify target operating conditions precisely, for example, target misclassification costs. This uncertainty makes building robust classification systems problematic. We show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. In some cases, the performance of the hybrid actually can surpass that of the best known classifier. This robust performance extends across a wide variety of comparison frameworks, including the optimization of metrics such as accuracy, expected cost, lift, precision, recall, and workforce utilization. The hybrid also is efficient to build, to store, and to update. The hybrid is based on a method for the comparison of classifier performance that is robust to imprecise class distributions and misclassification costs. The ROC convex hull (ROCCH) method combines techniques from ROC analysis, decision analysis and computational geometry, and adapts them to the particulars of analyzing learned classifiers. The method is efficient and incremental, minimizes the management of classifier performance data, and allows for clear visual comparisons and sensitivity analyses. Finally, we point to empirical evidence that a robust hybrid classifier indeed is needed for many real-world problems.'],\n",
       "   '2000']],\n",
       " [[['Scaling Up Inductive Logic Programming by Learning from Interpretations',\n",
       "    'When comparing inductive logic programming (ILP) and attribute-value learning techniques, there is a trade-off between expressive power and efficiency. Inductive logic programming techniques are typically more expressive but also less efficient. Therefore, the data sets handled by current inductive logic programming systems are small according to general standards within the data mining community. The main source of inefficiency lies in the assumption that several examples may be related to each other, so they cannot be handled independently.Within the learning from interpretations framework for inductive logic programming this assumption is unnecessary, which allows to scale up existing ILP algorithms. In this paper we explain this learning setting in the context of relational databases. We relate the setting to propositional data mining and to the classical ILP setting, and show that learning from interpretations corresponds to learning from multiple relations and thus extends the expressiveness of propositional learning, while maintaining its efficiency to a large extent (which is not the case in the classical ILP setting).As a case study, we present two alternative implementations of the ILP system TILDE (Top-down Induction of Logical DEcision trees): TILDEclassic, which loads all data in main memory, and TILDELDS, which loads the examples one by one. We experimentally compare the implementations, showing TILDELDS can handle large data sets (in the order of 100,000 examples or 100 MB) and indeed scales up linearly in the number of examples.'],\n",
       "   '2000']],\n",
       " [[['Termination Proofs for Logic Programs with Tabling',\n",
       "    'Tabled evaluation is receiving increasing attention in the logic programming community. It avoids many of the shortcomings of SLD execution and provides a more flexible and often considerably more efficient execution mechanism for logic programs. In particular, tabled execution terminates more often than execution based on SLD-resolution. In this article, we introduce two notions of universal termination of logic programming with tabling: quasi-termination and (the stronger notion of) LG-termination. We present sufficient conditions for these two notions of termination, namely quasi-acceptability and LG-acceptability, and we show that these conditions are also necessary in case the selection of tabled predicates meets certain natural criteria. Starting from these conditions, we develop modular termination proofs, i.e., proofs capable of combining termination proofs of separate programs to obtain termination proofs of combined programs. Finally, in the presence of mode information, we state sufficient conditions which form the basis for automatically proving termination in a constraint-based way.'],\n",
       "   '2000']],\n",
       " [[['Reasoning with Higher-Order Abstract Syntax in a Logical Framework',\n",
       "    'Logical frameworks based on intuitionistic or linear logics with higher-type quantification have been successfully used to give high-level, modular, and formal specifications of many important judgments in the area of programming languages and inference systems. Given such specifications, it is natural to consider proving properties about the specified systems in the framework: for example, given the specification of evaluation for a functional programming language, prove that the language is deterministic or that evaluation preserves types. One challenge in developing a framework for such reasoning is that higher-order abstract syntax (HOAS), an elegant and declarative treatment of object-level abstraction and substitution, is difficult to treat in proofs involving induction. In this article, we present a meta-logic that can be used to reason about judgments coded using HOAS; this meta-logic is an extension of a simple intuitionistic logic that admits higher-order quantification over simply typed &lambda;-terms (key ingredients for HOAS) as well as induction and a notion of definition. The latter concept of definition is a proof-theoretic device that allows certain theories to be treated as \"closed\" or as defining fixed points. We explore the difficulties of formal meta-theoretic analysis of HOAS encodings by considering encodings of intuitionistic and linear logics, and formally derive the admissibility of cut for important subsets of these logics. We then propose an approach to avoid the apparent trade-off between the benefits of higher-order abstract syntax and the ability to analyze the resulting encodings. We illustrate this approach through examples involving the simple functional and imperative programming languages PCF and PCF:=. We formally derive such properties as unicity of typing, subject reduction, determinacy of evaluation, and the equivalence of transition semantics and natural semantics presentations of evaluation.'],\n",
       "   '2000']],\n",
       " [[['Verifying Termination and Error-Freedom of Logic Programs with block Declarations',\n",
       "    'We present verification methods for logic programs with delay declarations. The verified properties are termination and freedom from errors related to built-ins. Concerning termination, we present two approaches. The first approach tries to eliminate the well-known problem of speculative output bindings. The second approach is based on identifying the predicates for which the textual position of an atom using this predicate is irrelevant with respect to termination. Three features are distinctive of this work: it allows for predicates to be used in several modes; it shows that block declarations, which are a very simple delay construct, are sufficient to ensure the desired properties; it takes the selection rule into account, assuming it to be as in most Prolog implementations. The methods can be used to verify existing programs and assist in writing new programs.'],\n",
       "   '2000']],\n",
       " [[['Sequence-Based Abstract Interpretation of Prolog',\n",
       "    'Abstract interpretation is a general methodology for systematic development of program analyses. An abstract interpretation framework is centered around a parametrized non-standard semantics that can be instantiated by various domains to approximate different program properties. Many abstract interpretation frameworks and analyses for Prolog have been proposed, which seek to extract information useful for program optimization. Although motivated by practical considerations, notably making Prolog competitive with imperative languages, such frameworks fail to capture some of the control structures of existing implementations of the language. In this paper, we propose a novel framework for the abstract interpretation of Prolog which handles the depth-first search rule and the cut operator. It relies on the notion of substitution sequence to model the result of the execution of a goal. The framework consists of (i) a denotational concrete semantics, (ii) a safe abstraction of the concrete semantics defined in terms of a class of post-fixpoints, and (iii) a generic abstract interpretation algorithm. We show that traditional abstract domains of substitutions may easily be adapted to the new framework, and provide experimental evidence of the effectiveness of our approach. We also show that previous work on determinacy analysis, that was not expressible by existing abstract interpretation frameworks, can be seen as an instance of our framework. The ideas developed in this paper can be applied to other logic languages, notably to constraint logic languages, and the theoretical approach should be of general interest for the analysis of many non-deterministic programming languages.'],\n",
       "   '2000']],\n",
       " [[['Transformation-Based Bottom-Up Computation of the Well-Founded Model',\n",
       "    'We present a framework for expressing bottom-up algorithms to compute the well-founded model of non-disjunctive logic programs. Our method is based on the notion of conditional facts and elementary program transformations studied by BRASS and DIX (Brass and Dix, 1994; Brass and Dix, 1999) for disjunctive programs. However, even if we restrict their framework to nondisjunctive programs, their &lsquo;residual program&rsquo; can grow to exponential size, whereas for function-free programs our &lsquo;program remainder&rsquo; is always polynomial in the size of the extensional database (EDB). We show that particular orderings of our transformations (we call them strategies) correspond to well-known computational methods like the alternating fixpoint approach (Van Gelder, 1989; Van Gelder, 1993), the well-founded magic sets method (Kemp et al., 1995) and the magic alternating fixpoint procedure (Morishita, 1996). However, due to the confluence of our calculi (first noted in Brass and Dix, 1998), we come up with computations of the well-founded model that are provably better than these methods. In contrast to other approaches, our transformation method treats magic set transformed programs correctly, i.e. it always computes a relevant part of the well-founded model of the original program. These results show that our approach is a valuable tool to analyze, compare, and optimize existing evaluation methods or to create new strategies that are automatically proven to be correct if they can be described by a sequence of transformations in our framework. We have also developed a prototypical implementation. Experiments illustrate that the theoretical results carry over to the implemented prototype and may be used to optimize real life systems.'],\n",
       "   '2000']],\n",
       " [[['Resource-distribution via Boolean constraints',\n",
       "    'We consider the problem of searching for proofs in sequential presentations of logics with multiplicative (or intensional) connectives. Specifically, we start with the multiplicative fragment of linear logic and extend, on the one hand, to linear logic with its additives and, on the other, to the additives of the logic of bunched implications (BI). We give an algebraic method for calculating the distribution of the side-formul&aelig; in multiplicative rules which allows the occurrence or non-occurrence of a formula on a branch of a proof to be determined once sufficient information is available. Each formula in the conclusion of such a rule is assigned a Boolean expression. As a search proceeds, a set of Boolean constraint equations is generated. We show that a solution to such a set of equations determines a proof corresponding to the given search. We explain a range of strategies, from the lazy to the eager, for solving sets of constraint equations. We indicate how to apply our methods systematically to large family of relevant systems.'],\n",
       "   '2000']],\n",
       " [[['On the theory of system administration',\n",
       "    'This paper describes a mean field approach to defining and implementing policy-based system administration. The concepts of regulation and optimization are used to define the notion of maintenance. These are then used to evaluate stable equilibria of system configuration, that are associated with sustainable policies for system management. Stable policies are thus associated with fixed points of a mapping that describes the evolution of the system. In general, such fixed points are the solutions of strategic games. A consistent system policy is not sufficient to guarantee compliance; the policy must also be implementable and maintainable. The paper proposes two types of model to understand policy driven management of Human-Computer systems: (i) average dynamical descriptions of computer system variables which provide a quantitative basis for decision, and (ii) competitive game theoretical descriptions that select optimal courses of action by generalizing the notion of configuration equilibria. It is shown how models can be formulated and simple examples are given.'],\n",
       "   '2000']],\n",
       " [[['The Role of Commutativity in Constraint Propagation Algorithms',\n",
       "    'Constraing propagation algorithms form an important part of most of the constraint programming systems. We provide here a simple, yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way. In this framework we proceed in two steps. First, we introduce a generic iteration algorithm on partial orderings and prove its correctness in an abstract setting. Then we instantiate this algorithm with specific partial orderings and functions to obtain specific constraint propagation algorithms. In particular, using the notions commutativity and semi-commutativity, we show that the AC-3, PC-2, DAC, and DPC algorithms for achieving (directional) arc consistency and (directional) path consistency are instances of a single generic algorithm. The work reported here extends and simplifies that of Apt [1999a].'],\n",
       "   '2000']],\n",
       " [[['Compiling Language Definitions: The ASF+SDF Compiler',\n",
       "    \"The ASF+SDF Meta-Environment is an interactive language development environment whose main application areas are definition and implementation of domain-specific languages, generation of program analysis and transformation tools, and production of software renovation tools. It uses conditional rewrite rules to define the dynamic semantics and other tool-oriented aspects of languages, so the effectiveness of the generated tools is critically dependent on the quality of the rewrite rule implementation. The ASF+SDF rewrite rule compiler generates C code, thus taking advantage of C's portability and the sophisticated optimization capabilities of current C compilers as well as avoiding potential abstract machine interface bottlenecks. It can handle large (10,000+ rule) language definitions and uses an efficient run-time storage scheme capable of handling large (1,000,000+ node) terms. Term storage uses maximal subterm sharing (hash-consing), which turns out to be more effective in the case of ASF+SDF than in Lisp or SML. Extensive benchmarking has shown the time and space performance of the generated code to be as good as or better than that of the best current rewrite rule and functional language compilers.\"],\n",
       "   '2000']],\n",
       " [[[\"One Complexity Theorist's View of Quantum Computing\",\n",
       "    'The complexity of quantum computation remains poorly understood. While physicists attempt to find ways to create quantum computers, we still do not have much evidence one way or the other as to how useful these machines will be. The tools of computational complexity theory should come to bear on these important questions.Quantum computing often scares away many potential researchers in computer science because of the apparent background need in quantum mechanics and the alien looking notation used in papers on the topic.This paper will give an overview of quantum computation from the point of view of a complexity theorist. We will see that one can think of BQP as yet another complexity class and study its power without focusing on the physical aspects behind it.'],\n",
       "   '2000']],\n",
       " [[['Convergence Theorems For Some Layout Measures On Random Lattice And Random Geometric Graphs.',\n",
       "    'This work deals with convergence theorems and bounds on the cost of several layout measures for lattice graphs, random lattice graphs and sparse random geometric graphs. Specifically, we consider the following problems: Minimum Linear Arrangement, Cutwidth, Sum Cut, Vertex Separation, Edge Bisection and Vertex Bisection. For full square lattices, we give optimal layouts for the problems still open. For arbitrary lattice graphs, we present best possible bounds disregarding a constant factor. We apply percolation theory to the study of lattice graphs in a probabilistic setting. In particular, we deal with the subcritical regime that this class of graphs exhibits and characterize the behaviour of several layout measures in this space of probability. We extend the results on random lattice graphs to random geometric graphs, which are graphs whose nodes are spread at random in the unit square and whose edges connect pairs of points which are within a given distance. We also characterize the behaviour of several layout measures on random geometric graphs in their subcritical regime. Our main results are convergence theorems that can be viewed as an analogue of the Beardwood, Halton and Hammersley theorem for the Euclidean TSP on random points in the unit square.'],\n",
       "   '2000']],\n",
       " [[['Workflow Systems: Occasions for Success and Failure.',\n",
       "    'Workflow technologies have created considerable discussion within the computer supported cooperative work community. Although a number of theoretical and empirical warnings about the difficulties of workflow systems have appeared, the technologies continue to be built and sold. This paper examines the use of one workflow-like system and outlines three cases when the technology supported the work of its users. Comparing these successful occasions with some reports of difficulties, this paper draws conclusions about the circumstances that led to tool usage.'],\n",
       "   '2000']],\n",
       " [[['Design of Extensible Component-Based Groupware.',\n",
       "    'Tailoring is identified as a key requirement for CSCW applications. One major tailoring mechanism is the extension of an application at run-time to change its behavior.This article shows how synchronous CSCW component-based applications can be designed to be extensible at run-time. We propose to split the act of tailoring into two steps: the design-time customization of new components in visual builder tools and their insertion into the running application. Thus the customization tool is not required to be part of the application.This article presents a new design pattern for extensibility and gives several examples based on that pattern. With the help of the pattern extensible application frameworks can be systematically created from a non-extensible application design. The different possibilities to place insertion points into the application design are discussed with respect to flexibility and ease of deployment. Finally, we present the advantages and limitations of this approach.'],\n",
       "   '2000']],\n",
       " [[['Techniques for Supporting Dynamic and Adaptive Workflow.',\n",
       "    'The unpredictability of business processes requires that workflow systems support exception handling with the ability to dynamically adapt to the changing environment. Traditional approaches to handling this problem have fallen short, providing little support for change, particularly once the process has begun execution. Further, exceptions vary widely in their character and significance, challenging the application of any single approach to handling them. We briefly discuss the classification of exceptions, highlighting differing impacts on the workflow model. Based on this discussion, we suggest principal goals to address in the development of adaptive workflow support, including strategies for avoiding exceptions, detecting them when they occur, and handling them at various levels of impact. We then identify a number of specific approaches to supporting these goals within the design of a workflow system infrastructure. Finally, we describe the implementation of many of these approaches in the Endeavors workflow support system.'],\n",
       "   '2000']],\n",
       " [[['ML-DEWS: Modeling Language to Support Dynamic Evolution within Workflow Systems.',\n",
       "    \"Organizations that are geared for success within today's business environments must be capable of rapid and continuous change. Dynamic change is a large and pervasive problem which surfaces within organizational workflows as well as within soft ware engineering, manufacturing, and numerous other domains. Procedural changes, performed in an ad hoc manner, can cause inefficiencies, inconsistencies, and catastrophic breakdowns within organizations. This document is concerned with change, especially dynamic change, to organizational procedures. We explain a taxonomy of change modalities, and present a modeling language for the unambiguous specification of procedural change. This language, call i>ML-DEWS, complements the formal model of dynamic change previously presented by the authors. Issues of exception handling, temporal specification, and participatory change are conveniently handled within the framework presented in this document.\"],\n",
       "   '2000']],\n",
       " [[['Users as Composers: Parts and Features as a Basis for Tailorability in CSCW Systems.',\n",
       "    \"Tailoring CSCW systems by composing components has received some attention in the last few years. The underlying ideas and techniques usually come from the domain of software engineering. We take a closer look at the specific situation and requirements when applying composition for tailoring CSCW systems and relate it to other kinds of tailoring mechanisms. We then describe the different approach of feature composition, originating from the domain of telecommunication networks. Roughly, the difference can be characterized as composing either i>parts or i>properties (``features''). We argue, that in many cases feature composition is better suited for tailoring CSCW systems than the normal approach.Hence, feature composition should be applied, in addition to the normal approach, in tailorable CSCW systems. We show how both approaches can be combined in a single system and demonstrate their use with an example from the workflow management domain.\"],\n",
       "   '2000']],\n",
       " ...]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e057f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/dblp_paper/doc_input.pkl', 'wb') as f:\n",
    "    pickle.dump(doc_input, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37534742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
